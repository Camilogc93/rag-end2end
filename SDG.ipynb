{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCk2Rx4cjlYF"
      },
      "source": [
        "# Synthetic Data Generation Using RAGAS\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vSRr2MXk0P_"
      },
      "source": [
        "We'll need to provide our LangSmith API key, and set tracing to \"true\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEBDoTM4UPG7",
        "outputId": "6942a051-a15e-48f4-ddef-6d047fb352fe"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangChain API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYr-ZmMOk6ZP"
      },
      "source": [
        "We'll also want to set a project name to make things easier for ourselves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7hZx_pKnZYt7"
      },
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIM - SDG - {uuid4().hex[0:8]}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ixdGJFelARW"
      },
      "source": [
        "OpenAI's API Key!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LoleVkxZUoo1",
        "outputId": "7bd79536-ebae-4ca0-9209-5762a55cfec7"
      },
      "outputs": [],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XOut20Olcdq"
      },
      "source": [
        "**Loading Source Documents**\n",
        "\n",
        "In order to create a synthetic dataset, we must first load our source documents!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xn5E3vJ6VGYX"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "\n",
        "# List of file paths for your PDFs\n",
        "file_paths = [\"dataset/data1.pdf\", \"dataset/data2.pdf\"]  # Add more files here\n",
        "\n",
        "# List to store all loaded documents\n",
        "documents = []\n",
        "\n",
        "# Loop through the file paths and load each PDF\n",
        "for file_path in file_paths:\n",
        "    loader = PyMuPDFLoader(file_path=file_path)\n",
        "    docs = loader.load()  # Load documents from the current PDF\n",
        "    documents.extend(docs)  # Add them to the overall documents list\n",
        "\n",
        "# Now 'documents' contains the contents of all loaded PDFs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 0, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content=' \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nBLUEPRINT FOR AN \\nAI BILL OF \\nRIGHTS \\nMAKING AUTOMATED \\nSYSTEMS WORK FOR \\nTHE AMERICAN PEOPLE \\nOCTOBER 2022 \\n'),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 1, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content=' \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nAbout this Document \\nThe Blueprint for an AI Bill of Rights: Making Automated Systems Work for the American People was \\npublished by the White House Office of Science and Technology Policy in October 2022. This framework was \\nreleased one year after OSTP announced the launch of a process to develop “a bill of rights for an AI-powered \\nworld.” Its release follows a year of public engagement to inform this initiative. The framework is available \\nonline at: https://www.whitehouse.gov/ostp/ai-bill-of-rights \\nAbout the Office of Science and Technology Policy \\nThe Office of Science and Technology Policy (OSTP) was established by the National Science and Technology \\nPolicy, Organization, and Priorities Act of 1976 to provide the President and others within the Executive Office \\nof the President with advice on the scientific, engineering, and technological aspects of the economy, national \\nsecurity, health, foreign relations, the environment, and the technological recovery and use of resources, among \\nother topics. OSTP leads interagency science and technology policy coordination efforts, assists the Office of \\nManagement and Budget (OMB) with an annual review and analysis of Federal research and development in \\nbudgets, and serves as a source of scientific and technological analysis and judgment for the President with \\nrespect to major policies, plans, and programs of the Federal Government. \\nLegal Disclaimer \\nThe Blueprint for an AI Bill of Rights: Making Automated Systems Work for the American People is a white paper \\npublished by the White House Office of Science and Technology Policy. It is intended to support the \\ndevelopment of policies and practices that protect civil rights and promote democratic values in the building, \\ndeployment, and governance of automated systems. \\nThe Blueprint for an AI Bill of Rights is non-binding and does not constitute U.S. government policy. It \\ndoes not supersede, modify, or direct an interpretation of any existing statute, regulation, policy, or \\ninternational instrument. It does not constitute binding guidance for the public or Federal agencies and \\ntherefore does not require compliance with the principles described herein. It also is not determinative of what \\nthe U.S. government’s position will be in any international negotiation. Adoption of these principles may not \\nmeet the requirements of existing statutes, regulations, policies, or international instruments, or the \\nrequirements of the Federal agencies that enforce them. These principles are not intended to, and do not, \\nprohibit or limit any lawful activity of a government agency, including law enforcement, national security, or \\nintelligence activities. \\nThe appropriate application of the principles set forth in this white paper depends significantly on the \\ncontext in which automated systems are being utilized. In some circumstances, application of these principles \\nin whole or in part may not be appropriate given the intended use of automated systems to achieve government \\nagency missions. Future sector-specific guidance will likely be necessary and important for guiding the use of \\nautomated systems in certain settings such as AI systems used as part of school building security or automated \\nhealth diagnostic systems. \\nThe Blueprint for an AI Bill of Rights recognizes that law enforcement activities require a balancing of \\nequities, for example, between the protection of sensitive law enforcement information and the principle of \\nnotice; as such, notice may not be appropriate, or may need to be adjusted to protect sources, methods, and \\nother law enforcement equities. Even in contexts where these principles may not apply in whole or in part, \\nfederal departments and agencies remain subject to judicial, privacy, and civil liberties oversight as well as \\nexisting policies and safeguards that govern automated systems, including, for example, Executive Order 13960, \\nPromoting the Use of Trustworthy Artificial Intelligence in the Federal Government (December 2020). \\nThis white paper recognizes that national security (which includes certain law enforcement and \\nhomeland security activities) and defense activities are of increased sensitivity and interest to our nation’s \\nadversaries and are often subject to special requirements, such as those governing classified information and \\nother protected data. Such activities require alternative, compatible safeguards through existing policies that \\ngovern automated systems and AI, such as the Department of Defense (DOD) AI Ethical Principles and \\nResponsible AI Implementation Pathway and the Intelligence Community (IC) AI Ethics Principles and \\nFramework. The implementation of these policies to national security and defense activities can be informed by \\nthe Blueprint for an AI Bill of Rights where feasible. \\nThe Blueprint for an AI Bill of Rights is not intended to, and does not, create any legal right, benefit, or \\ndefense, substantive or procedural, enforceable at law or in equity by any party against the United States, its \\ndepartments, agencies, or entities, its officers, employees, or agents, or any other person, nor does it constitute a \\nwaiver of sovereign immunity. \\nCopyright Information \\nThis document is a work of the United States Government and is in the public domain (see 17 U.S.C. §105). \\n2\\n'),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 2, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content=' \\nSECTION TITLE\\xad\\nFOREWORD\\nAmong the great challenges posed to democracy today is the use of technology, data, and automated systems in \\nways that threaten the rights of the American public. Too often, these tools are used to limit our opportunities and \\nprevent our access to critical resources or services. These problems are well documented. In America and around \\nthe world, systems supposed to help with patient care have proven unsafe, ineffective, or biased. Algorithms used \\nin hiring and credit decisions have been found to reflect and reproduce existing unwanted inequities or embed \\nnew harmful bias and discrimination. Unchecked social media data collection has been used to threaten people’s \\nopportunities, undermine their privacy, or pervasively track their activity—often without their knowledge or \\nconsent. \\nThese outcomes are deeply harmful—but they are not inevitable. Automated systems have brought about extraor-\\ndinary benefits, from technology that helps farmers grow food more efficiently and computers that predict storm \\npaths, to algorithms that can identify diseases in patients. These tools now drive important decisions across \\nsectors, while data is helping to revolutionize global industries. Fueled by the power of American innovation, \\nthese tools hold the potential to redefine every part of our society and make life better for everyone. \\nThis important progress must not come at the price of civil rights or democratic values, foundational American \\nprinciples that President Biden has affirmed as a cornerstone of his Administration. On his first day in office, the \\nPresident ordered the full Federal government to work to root out inequity, embed fairness in decision-\\nmaking processes, and affirmatively advance civil rights, equal opportunity, and racial justice in America.1 The \\nPresident has spoken forcefully about the urgent challenges posed to democracy today and has regularly called \\non people of conscience to act to preserve civil rights—including the right to privacy, which he has called “the \\nbasis for so many more rights that we have come to take for granted that are ingrained in the fabric of this \\ncountry.”2\\nTo advance President Biden’s vision, the White House Office of Science and Technology Policy has identified \\nfive principles that should guide the design, use, and deployment of automated systems to protect the American \\npublic in the age of artificial intelligence. The Blueprint for an AI Bill of Rights is a guide for a society that \\nprotects all people from these threats—and uses technologies in ways that reinforce our highest values. \\nResponding to the experiences of the American public, and informed by insights from researchers, \\ntechnologists, advocates, journalists, and policymakers, this framework is accompanied by a technical \\ncompanion—a handbook for anyone seeking to incorporate these protections into policy and practice, including \\ndetailed steps toward actualizing these principles in the technological design process. These principles help \\nprovide guidance whenever automated systems can meaningfully impact the public’s rights, opportunities, \\nor access to critical needs. \\n3\\n'),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 3, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content=' \\n \\n \\nABOUT THIS FRAMEWORK\\xad\\xad\\xad\\xad\\xad\\nThe Blueprint for an AI Bill of Rights is a set of five principles and associated practices to help guide the \\ndesign, use, and deployment of automated systems to protect the rights of the American public in the age of \\nartificial intel-ligence. Developed through extensive consultation with the American public, these principles are \\na blueprint for building and deploying automated systems that are aligned with democratic values and protect \\ncivil rights, civil liberties, and privacy. The Blueprint for an AI Bill of Rights includes this Foreword, the five \\nprinciples, notes on Applying the The Blueprint for an AI Bill of Rights, and a Technical Companion that gives \\nconcrete steps that can be taken by many kinds of organizations—from governments at all levels to companies of \\nall sizes—to uphold these values. Experts from across the private sector, governments, and international \\nconsortia have published principles and frameworks to guide the responsible use of automated systems; this \\nframework provides a national values statement and toolkit that is sector-agnostic to inform building these \\nprotections into policy, practice, or the technological design process.  Where existing law or policy—such as \\nsector-specific privacy laws and oversight requirements—do not already provide guidance, the Blueprint for an \\nAI Bill of Rights should be used to inform policy decisions.\\nLISTENING TO THE AMERICAN PUBLIC\\nThe White House Office of Science and Technology Policy has led a year-long process to seek and distill input \\nfrom people across the country—from impacted communities and industry stakeholders to technology develop-\\ners and other experts across fields and sectors, as well as policymakers throughout the Federal government—on \\nthe issue of algorithmic and data-driven harms and potential remedies. Through panel discussions, public listen-\\ning sessions, meetings, a formal request for information, and input to a publicly accessible and widely-publicized \\nemail address, people throughout the United States, public servants across Federal agencies, and members of the \\ninternational community spoke up about both the promises and potential harms of these technologies, and \\nplayed a central role in shaping the Blueprint for an AI Bill of Rights. The core messages gleaned from these \\ndiscussions include that AI has transformative potential to improve Americans’ lives, and that preventing the \\nharms of these technologies is both necessary and achievable. The Appendix includes a full list of public engage-\\nments. \\n4\\n'),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 4, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content=' AI BILL OF RIGHTS\\nFFECTIVE SYSTEMS\\nineffective systems. Automated systems should be \\ncommunities, stakeholders, and domain experts to identify \\nSystems should undergo pre-deployment testing, risk \\nthat demonstrate they are safe and effective based on \\nincluding those beyond the intended use, and adherence to \\nprotective measures should include the possibility of not \\nAutomated systems should not be designed with an intent \\nreasonably foreseeable possibility of endangering your safety or the safety of your community. They should \\nstemming from unintended, yet foreseeable, uses or \\n \\n \\n \\n \\n  \\n \\n \\nSECTION TITLE\\nBLUEPRINT FOR AN\\nSAFE AND E \\nYou should be protected from unsafe or \\ndeveloped with consultation from diverse \\nconcerns, risks, and potential impacts of the system. \\nidentification and mitigation, and ongoing monitoring \\ntheir intended use, mitigation of unsafe outcomes \\ndomain-specific standards. Outcomes of these \\ndeploying the system or removing a system from use. \\nor \\nbe designed to proactively protect you from harms \\nimpacts of automated systems. You should be protected from inappropriate or irrelevant data use in the \\ndesign, development, and deployment of automated systems, and from the compounded harm of its reuse. \\nIndependent evaluation and reporting that confirms that the system is safe and effective, including reporting of \\nsteps taken to mitigate potential harms, should be performed and the results made public whenever possible. \\nALGORITHMIC DISCRIMINATION PROTECTIONS\\nYou should not face discrimination by algorithms and systems should be used and designed in \\nan equitable way. Algorithmic discrimination occurs when automated systems contribute to unjustified \\ndifferent treatment or impacts disfavoring people based on their race, color, ethnicity, sex (including \\npregnancy, childbirth, and related medical conditions, gender identity, intersex status, and sexual \\norientation), religion, age, national origin, disability, veteran status, genetic information, or any other \\nclassification protected by law. Depending on the specific circumstances, such algorithmic discrimination \\nmay violate legal protections. Designers, developers, and deployers of automated systems should take \\nproactive \\nand \\ncontinuous \\nmeasures \\nto \\nprotect \\nindividuals \\nand \\ncommunities \\nfrom algorithmic \\ndiscrimination and to use and design systems in an equitable way. This protection should include proactive \\nequity assessments as part of the system design, use of representative data and protection against proxies \\nfor demographic features, ensuring accessibility for people with disabilities in design and development, \\npre-deployment and ongoing disparity testing and mitigation, and clear organizational oversight. Independent \\nevaluation and plain language reporting in the form of an algorithmic impact assessment, including \\ndisparity testing results and mitigation information, should be performed and made public whenever \\npossible to confirm these protections. \\n5\\n'),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 5, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content=' \\n \\n \\n \\n \\nSECTION TITLE\\nDATA PRIVACY\\nYou should be protected from abusive data practices via built-in protections and you \\nshould have agency over how data about you is used. You should be protected from violations of \\nprivacy through design choices that ensure such protections are included by default, including ensuring that \\ndata collection conforms to reasonable expectations and that only data strictly necessary for the specific \\ncontext is collected. Designers, developers, and deployers of automated systems should seek your permission \\nand respect your decisions regarding collection, use, access, transfer, and deletion of your data in appropriate \\nways and to the greatest extent possible; where not possible, alternative privacy by design safeguards should be \\nused. Systems should not employ user experience and design decisions that obfuscate user choice or burden \\nusers with defaults that are privacy invasive. Consent should only be used to justify collection of data in cases \\nwhere it can be appropriately and meaningfully given. Any consent requests should be brief, be understandable \\nin plain language, and give you agency over data collection and the specific context of use; current hard-to\\xad\\nunderstand notice-and-choice practices for broad uses of data should be changed. Enhanced protections and \\nrestrictions for data and inferences related to sensitive domains, including health, work, education, criminal \\njustice, and finance, and for data pertaining to youth should put you first. In sensitive domains, your data and \\nrelated inferences should only be used for necessary functions, and you should be protected by ethical review \\nand use prohibitions. You and your communities should be free from unchecked surveillance; surveillance \\ntechnologies should be subject to heightened oversight that includes at least pre-deployment assessment of their \\npotential harms and scope limits to protect privacy and civil liberties. Continuous surveillance and monitoring \\nshould not be used in education, work, housing, or in other contexts where the use of such surveillance \\ntechnologies is likely to limit rights, opportunities, or access. Whenever possible, you should have access to \\nreporting that confirms your data decisions have been respected and provides an assessment of the \\npotential impact of surveillance technologies on your rights, opportunities, or access. \\nNOTICE AND EXPLANATION\\nYou should know that an automated system is being used and understand how and why it \\ncontributes to outcomes that impact you. Designers, developers, and deployers of automated systems \\nshould provide generally accessible plain language documentation including clear descriptions of the overall \\nsystem functioning and the role automation plays, notice that such systems are in use, the individual or organiza\\xad\\ntion responsible for the system, and explanations of outcomes that are clear, timely, and accessible. Such notice \\nshould be kept up-to-date and people impacted by the system should be notified of significant use case or key \\nfunctionality changes. You should know how and why an outcome impacting you was determined by an \\nautomated system, including when the automated system is not the sole input determining the outcome. \\nAutomated systems should provide explanations that are technically valid, meaningful and useful to you and to \\nany operators or others who need to understand the system, and calibrated to the level of risk based on the \\ncontext. Reporting that includes summary information about these automated systems in plain language and \\nassessments of the clarity and quality of the notice and explanations should be made public whenever possible. \\n6\\n'),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 6, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content=' \\nSECTION TITLE\\nHUMAN ALTERNATIVES, CONSIDERATION, AND FALLBACK\\nYou should be able to opt out, where appropriate, and have access to a person who can quickly \\nconsider and remedy problems you encounter. You should be able to opt out from automated systems in \\nfavor of a human alternative, where appropriate. Appropriateness should be determined based on reasonable \\nexpectations in a given context and with a focus on ensuring broad accessibility and protecting the public from \\nespecially harmful impacts. In some cases, a human or other alternative may be required by law. You should have \\naccess to timely human consideration and remedy by a fallback and escalation process if an automated system \\nfails, it produces an error, or you would like to appeal or contest its impacts on you. Human consideration and \\nfallback should be accessible, equitable, effective, maintained, accompanied by appropriate operator training, and \\nshould not impose an unreasonable burden on the public. Automated systems with an intended use within sensi\\xad\\ntive domains, including, but not limited to, criminal justice, employment, education, and health, should additional\\xad\\nly be tailored to the purpose, provide meaningful access for oversight, include training for any people interacting \\nwith the system, and incorporate human consideration for adverse or high-risk decisions. Reporting that includes \\na description of these human governance processes and assessment of their timeliness, accessibility, outcomes, \\nand effectiveness should be made public whenever possible. \\nDefinitions for key terms in The Blueprint for an AI Bill of Rights can be found in Applying the Blueprint for an AI Bill of Rights. \\nAccompanying analysis and tools for actualizing each principle can be found in the Technical Companion. \\n7\\n'),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 7, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content=\" \\n \\n \\nSECTION TITLE\\nApplying The Blueprint for an AI Bill of Rights \\nWhile many of the concerns addressed in this framework derive from the use of AI, the technical \\ncapabilities and specific definitions of such systems change with the speed of innovation, and the potential \\nharms of their use occur even with less technologically sophisticated tools. Thus, this framework uses a two-\\npart test to determine what systems are in scope. This framework applies to (1) automated systems that (2) \\nhave the potential to meaningfully impact the American public’s rights, opportunities, or access to \\ncritical resources or services. These rights, opportunities, and access to critical resources of services should \\nbe enjoyed equally and be fully protected, regardless of the changing role that automated systems may play in \\nour lives. \\nThis framework describes protections that should be applied with respect to all automated systems that \\nhave the potential to meaningfully impact individuals' or communities' exercise of: \\nRIGHTS, OPPORTUNITIES, OR ACCESS\\nCivil rights, civil liberties, and privacy, including freedom of speech, voting, and protections from discrimi\\xad\\nnation, excessive punishment, unlawful surveillance, and violations of privacy and other freedoms in both \\npublic and private sector contexts; \\nEqual opportunities, including equitable access to education, housing, credit, employment, and other \\nprograms; or, \\nAccess to critical resources or services, such as healthcare, financial services, safety, social services, \\nnon-deceptive information about goods and services, and government benefits. \\nA list of examples of automated systems for which these principles should be considered is provided in the \\nAppendix. The Technical Companion, which follows, offers supportive guidance for any person or entity that \\ncreates, deploys, or oversees automated systems. \\nConsidered together, the five principles and associated practices of the Blueprint for an AI Bill of \\nRights form an overlapping set of backstops against potential harms. This purposefully overlapping \\nframework, when taken as a whole, forms a blueprint to help protect the public from harm. \\nThe measures taken to realize the vision set forward in this framework should be proportionate \\nwith the extent and nature of the harm, or risk of harm, to people's rights, opportunities, and \\naccess. \\nRELATIONSHIP TO EXISTING LAW AND POLICY\\nThe Blueprint for an AI Bill of Rights is an exercise in envisioning a future where the American public is \\nprotected from the potential harms, and can fully enjoy the benefits, of automated systems. It describes princi\\xad\\nples that can help ensure these protections. Some of these protections are already required by the U.S. Constitu\\xad\\ntion or implemented under existing U.S. laws. For example, government surveillance, and data search and \\nseizure are subject to legal requirements and judicial oversight. There are Constitutional requirements for \\nhuman review of criminal investigative matters and statutory requirements for judicial review. Civil rights laws \\nprotect the American people against discrimination. \\n8\\n\"),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 8, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content='SECTION TITLE\\n \\n \\n \\n \\n \\n \\nApplying The Blueprint for an AI Bill of Rights \\nRELATIONSHIP TO EXISTING LAW AND POLICY\\nThere are regulatory safety requirements for medical devices, as well as sector-, population-, or technology-spe\\xad\\ncific privacy and security protections. Ensuring some of the additional protections proposed in this framework \\nwould require new laws to be enacted or new policies and practices to be adopted. In some cases, exceptions to \\nthe principles described in the Blueprint for an AI Bill of Rights may be necessary to comply with existing law, \\nconform to the practicalities of a specific use case, or balance competing public interests. In particular, law \\nenforcement, and other regulatory contexts may require government actors to protect civil rights, civil liberties, \\nand privacy in a manner consistent with, but using alternate mechanisms to, the specific principles discussed in \\nthis framework. The Blueprint for an AI Bill of Rights is meant to assist governments and the private sector in \\nmoving principles into practice. \\nThe expectations given in the Technical Companion are meant to serve as a blueprint for the development of \\nadditional technical standards and practices that should be tailored for particular sectors and contexts. While \\nexisting laws informed the development of the Blueprint for an AI Bill of Rights, this framework does not detail \\nthose laws beyond providing them as examples, where appropriate, of existing protective measures. This \\nframework instead shares a broad, forward-leaning vision of recommended principles for automated system \\ndevelopment and use to inform private and public involvement with these systems where they have the poten\\xad\\ntial to meaningfully impact rights, opportunities, or access. Additionally, this framework does not analyze or \\ntake a position on legislative and regulatory proposals in municipal, state, and federal government, or those in \\nother countries. \\nWe have seen modest progress in recent years, with some state and local governments responding to these prob\\xad\\nlems with legislation, and some courts extending longstanding statutory protections to new and emerging tech\\xad\\nnologies. There are companies working to incorporate additional protections in their design and use of auto\\xad\\nmated systems, and researchers developing innovative guardrails. Advocates, researchers, and government \\norganizations have proposed principles for the ethical use of AI and other automated systems. These include \\nthe Organization for Economic Co-operation and Development’s (OECD’s) 2019 Recommendation on Artificial \\nIntelligence, which includes principles for responsible stewardship of trustworthy AI and which the United \\nStates adopted, and Executive Order 13960 on Promoting the Use of Trustworthy Artificial Intelligence in the \\nFederal Government, which sets out principles that govern the federal government’s use of AI. The Blueprint \\nfor an AI Bill of Rights is fully consistent with these principles and with the direction in Executive Order 13985 \\non Advancing Racial Equity and Support for Underserved Communities Through the Federal Government. \\nThese principles find kinship in the Fair Information Practice Principles (FIPPs), derived from the 1973 report \\nof an advisory committee to the U.S. Department of Health, Education, and Welfare, Records, Computers, \\nand the Rights of Citizens.4 While there is no single, universal articulation of the FIPPs, these core \\nprinciples for managing information about individuals have been incorporated into data privacy laws and \\npolicies across the globe.5 The Blueprint for an AI Bill of Rights embraces elements of the FIPPs that are \\nparticularly relevant to automated systems, without articulating a specific set of FIPPs or scoping \\napplicability or the interests served to a single particular domain, like privacy, civil rights and civil liberties, \\nethics, or risk management. The Technical Companion builds on this prior work to provide practical next \\nsteps to move these principles into practice and promote common approaches that allow technological \\ninnovation to flourish while protecting people from harm. \\n9\\n'),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 9, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content='Applying The Blueprint for an AI Bill of Rights \\nDEFINITIONS\\nALGORITHMIC DISCRIMINATION: “Algorithmic discrimination” occurs when automated systems \\ncontribute to unjustified different treatment or impacts disfavoring people based on their race, color, ethnicity, \\nsex (including pregnancy, childbirth, and related medical conditions, gender identity, intersex status, and sexual \\norientation), religion, age, national origin, disability, veteran status, genetic information, or any other classifica-\\ntion protected by law. Depending on the specific circumstances, such algorithmic discrimination may violate \\nlegal protections. Throughout this framework the term “algorithmic discrimination” takes this meaning (and \\nnot a technical understanding of discrimination as distinguishing between items). \\nAUTOMATED SYSTEM: An \"automated system\" is any system, software, or process that uses computation as \\nwhole or part of a system to determine outcomes, make or aid decisions, inform policy implementation, collect \\ndata or observations, or otherwise interact with individuals and/or communities. Automated systems \\ninclude, but are not limited to, systems derived from machine learning, statistics, or other data processing \\nor artificial intelligence techniques, and exclude passive computing infrastructure. “Passive computing \\ninfrastructure” is any intermediary technology that does not influence or determine the outcome of decision, \\nmake or aid in decisions, inform policy implementation, or collect data or observations, including web \\nhosting, domain registration, networking, caching, data storage, or cybersecurity. Throughout this \\nframework, automated systems that are considered in scope are only those that have the potential to \\nmeaningfully impact individuals’ or communi-ties’ rights, opportunities, or access. \\nCOMMUNITIES: “Communities” include: neighborhoods; social network connections (both online and \\noffline); families (construed broadly); people connected by affinity, identity, or shared traits; and formal organi-\\nzational ties. This includes Tribes, Clans, Bands, Rancherias, Villages, and other Indigenous communities. AI \\nand other data-driven automated systems most directly collect data on, make inferences about, and may cause \\nharm to individuals. But the overall magnitude of their impacts may be most readily visible at the level of com-\\nmunities. Accordingly, the concept of community is integral to the scope of the Blueprint for an AI Bill of Rights. \\nUnited States law and policy have long employed approaches for protecting the rights of individuals, but exist-\\ning frameworks have sometimes struggled to provide protections when effects manifest most clearly at a com-\\nmunity level. For these reasons, the Blueprint for an AI Bill of Rights asserts that the harms of automated \\nsystems should be evaluated, protected against, and redressed at both the individual and community levels. \\nEQUITY: “Equity” means the consistent and systematic fair, just, and impartial treatment of all individuals. \\nSystemic, fair, and just treatment must take into account the status of individuals who belong to underserved \\ncommunities that have been denied such treatment, such as Black, Latino, and Indigenous and Native American \\npersons, Asian Americans and Pacific Islanders and other persons of color; members of religious minorities; \\nwomen, girls, and non-binary people; lesbian, gay, bisexual, transgender, queer, and intersex (LGBTQI+) \\npersons; older adults; persons with disabilities; persons who live in rural areas; and persons otherwise adversely \\naffected by persistent poverty or inequality. \\nRIGHTS, OPPORTUNITIES, OR ACCESS: “Rights, opportunities, or access” is used to indicate the scoping \\nof this framework. It describes the set of: civil rights, civil liberties, and privacy, including freedom of speech, \\nvoting, and protections from discrimination, excessive punishment, unlawful surveillance, and violations of \\nprivacy and other freedoms in both public and private sector contexts; equal opportunities, including equitable \\naccess to education, housing, credit, employment, and other programs; or, access to critical resources or \\nservices, such as healthcare, financial services, safety, social services, non-deceptive information about goods \\nand services, and government benefits. \\n10\\n'),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 10, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content=' \\n \\n \\nApplying The Blueprint for an AI Bill of Rights \\nSENSITIVE DATA: Data and metadata are sensitive if they pertain to an individual in a sensitive domain \\n(defined below); are generated by technologies used in a sensitive domain; can be used to infer data from a \\nsensitive domain or sensitive data about an individual (such as disability-related data, genomic data, biometric \\ndata, behavioral data, geolocation data, data related to interaction with the criminal justice system, relationship \\nhistory and legal status such as custody and divorce information, and home, work, or school environmental \\ndata); or have the reasonable potential to be used in ways that are likely to expose individuals to meaningful \\nharm, such as a loss of privacy or financial harm due to identity theft. Data and metadata generated by or about \\nthose who are not yet legal adults is also sensitive, even if not related to a sensitive domain. Such data includes, \\nbut is not limited to, numerical, text, image, audio, or video data. \\nSENSITIVE DOMAINS: “Sensitive domains” are those in which activities being conducted can cause material \\nharms, including significant adverse effects on human rights such as autonomy and dignity, as well as civil liber\\xad\\nties and civil rights. Domains that have historically been singled out as deserving of enhanced data protections \\nor where such enhanced protections are reasonably expected by the public include, but are not limited to, \\nhealth, family planning and care, employment, education, criminal justice, and personal finance. In the context \\nof this framework, such domains are considered sensitive whether or not the specifics of a system context \\nwould necessitate coverage under existing law, and domains and data that are considered sensitive are under\\xad\\nstood to change over time based on societal norms and context. \\nSURVEILLANCE TECHNOLOGY: “Surveillance technology” refers to products or services marketed for \\nor that can be lawfully used to detect, monitor, intercept, collect, exploit, preserve, protect, transmit, and/or \\nretain data, identifying information, or communications concerning individuals or groups. This framework \\nlimits its focus to both government and commercial use of surveillance technologies when juxtaposed with \\nreal-time or subsequent automated analysis and when such systems have a potential for meaningful impact \\non individuals’ or communities’ rights, opportunities, or access. \\nUNDERSERVED COMMUNITIES: The term “underserved communities” refers to communities that have \\nbeen systematically denied a full opportunity to participate in aspects of economic, social, and civic life, as \\nexemplified by the list in the preceding definition of “equity.” \\n11\\n'),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 11, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content=' \\n \\n \\nFROM \\nPRINCIPLES \\nTO PRACTICE \\nA TECHINCAL COMPANION TO\\nTHE Blueprint for an \\nAI BILL OF RIGHTS\\n12\\n'),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 12, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content='TABLE OF CONTENTS\\nFROM PRINCIPLES TO PRACTICE: A TECHNICAL COMPANION TO THE BLUEPRINT \\nFOR AN AI BILL OF RIGHTS \\n \\nUSING THIS TECHNICAL COMPANION\\n \\nSAFE AND EFFECTIVE SYSTEMS\\n \\nALGORITHMIC DISCRIMINATION PROTECTIONS\\n \\nDATA PRIVACY\\n \\nNOTICE AND EXPLANATION\\n \\nHUMAN ALTERNATIVES, CONSIDERATION, AND FALLBACK\\nAPPENDIX\\n \\nEXAMPLES OF AUTOMATED SYSTEMS\\n \\nLISTENING TO THE AMERICAN PEOPLE\\nENDNOTES \\n12\\n14\\n15\\n23\\n30\\n40\\n46\\n53\\n53\\n55\\n63\\n13\\n'),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 13, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content=' \\n \\n \\n \\n \\n  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n-    \\nUSING THIS TECHNICAL COMPANION\\nThe Blueprint for an AI Bill of Rights is a set of five principles and associated practices to help guide the design, \\nuse, and deployment of automated systems to protect the rights of the American public in the age of artificial \\nintelligence. This technical companion considers each principle in the Blueprint for an AI Bill of Rights and \\nprovides examples and concrete steps for communities, industry, governments, and others to take in order to \\nbuild these protections into policy, practice, or the technological design process. \\nTaken together, the technical protections and practices laid out in the Blueprint for an AI Bill of Rights can help \\nguard the American public against many of the potential and actual harms identified by researchers, technolo\\xad\\ngists, advocates, journalists, policymakers, and communities in the United States and around the world. This \\ntechnical companion is intended to be used as a reference by people across many circumstances – anyone \\nimpacted by automated systems, and anyone developing, designing, deploying, evaluating, or making policy to \\ngovern the use of an automated system. \\nEach principle is accompanied by three supplemental sections: \\n1\\n2\\nWHY THIS PRINCIPLE IS IMPORTANT: \\nThis section provides a brief summary of the problems that the principle seeks to address and protect against, including \\nillustrative examples. \\nWHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS: \\n• The expectations for automated systems are meant to serve as a blueprint for the development of additional technical\\nstandards and practices that should be tailored for particular sectors and contexts.\\n• This section outlines practical steps that can be implemented to realize the vision of the Blueprint for an AI Bill of Rights. The \\nexpectations laid out often mirror existing practices for technology development, including pre-deployment testing, ongoing \\nmonitoring, and governance structures for automated systems, but also go further to address unmet needs for change and offer \\nconcrete directions for how those changes can be made. \\n• Expectations about reporting are intended for the entity developing or using the automated system. The resulting reports can \\nbe provided to the public, regulators, auditors, industry standards groups, or others engaged in independent review, and should \\nbe made public as much as possible consistent with law, regulation, and policy, and noting that intellectual property, law \\nenforcement, or national security considerations may prevent public release. Where public reports are not possible, the \\ninformation should be provided to oversight bodies and privacy, civil liberties, or other ethics officers charged with safeguard \\ning individuals’ rights. These reporting expectations are important for transparency, so the American people can have\\nconfidence that their rights, opportunities, and access as well as their expectations about technologies are respected. \\n3\\nHOW THESE PRINCIPLES CAN MOVE INTO PRACTICE: \\nThis section provides real-life examples of how these guiding principles can become reality, through laws, policies, and practices. \\nIt describes practical technical and sociotechnical approaches to protecting rights, opportunities, and access. \\nThe examples provided are not critiques or endorsements, but rather are offered as illustrative cases to help \\nprovide a concrete vision for actualizing the Blueprint for an AI Bill of Rights. Effectively implementing these \\nprocesses require the cooperation of and collaboration among industry, civil society, researchers, policymakers, \\ntechnologists, and the public. \\n14\\n'),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 14, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content=' \\n \\n \\nSAFE AND EFFECTIVE SYSTEMS \\nYou should be protected from unsafe or ineffective sys\\xad\\ntems. Automated systems should be developed with consultation \\nfrom diverse communities, stakeholders, and domain experts to iden\\xad\\ntify concerns, risks, and potential impacts of the system. Systems \\nshould undergo pre-deployment testing, risk identification and miti\\xad\\ngation, and ongoing monitoring that demonstrate they are safe and \\neffective based on their intended use, mitigation of unsafe outcomes \\nincluding those beyond the intended use, and adherence to do\\xad\\nmain-specific standards. Outcomes of these protective measures \\nshould include the possibility of not deploying the system or remov\\xad\\ning a system from use. Automated systems should not be designed \\nwith an intent or reasonably foreseeable possibility of endangering \\nyour safety or the safety of your community. They should be designed \\nto proactively protect you from harms stemming from unintended, \\nyet foreseeable, uses or impacts of automated systems. You should be \\nprotected from inappropriate or irrelevant data use in the design, de\\xad\\nvelopment, and deployment of automated systems, and from the \\ncompounded harm of its reuse. Independent evaluation and report\\xad\\ning that confirms that the system is safe and effective, including re\\xad\\nporting of steps taken to mitigate potential harms, should be per\\xad\\nformed and the results made public whenever possible. \\n15\\n'),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 15, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content=' \\n \\n  \\n \\nSAFE AND EFFECTIVE \\nSYSTEMS \\nWHY THIS PRINCIPLE IS IMPORTANT\\nThis section provides a brief summary of the problems which the principle seeks to address and protect \\nagainst, including illustrative examples. \\nWhile technologies are being deployed to solve problems across a wide array of issues, our reliance on technology can \\nalso lead to its use in situations where it has not yet been proven to work—either at all or within an acceptable range \\nof error. In other cases, technologies do not work as intended or as promised, causing substantial and unjustified harm. \\nAutomated systems sometimes rely on data from other systems, including historical data, allowing irrelevant informa\\xad\\ntion from past decisions to infect decision-making in unrelated situations.  In some cases, technologies are purposeful\\xad\\nly designed to violate the safety of others, such as technologies designed to facilitate stalking; in other cases, intended \\nor unintended uses lead to unintended harms. \\nMany of the harms resulting from these technologies are preventable, and actions are already being taken to protect \\nthe public. Some companies have put in place safeguards that have prevented harm from occurring by ensuring that \\nkey development decisions are vetted by an ethics review; others have identified and mitigated harms found through \\npre-deployment testing and ongoing monitoring processes. Governments at all levels have existing public consulta\\xad\\ntion processes that may be applied when considering the use of new automated systems, and existing product develop\\xad\\nment and testing practices already protect the American public from many potential harms. \\nStill, these kinds of practices are deployed too rarely and unevenly. Expanded, proactive protections could build on \\nthese existing practices, increase confidence in the use of automated systems, and protect the American public. Inno\\xad\\nvators deserve clear rules of the road that allow new ideas to flourish, and the American public deserves protections \\nfrom unsafe outcomes. All can benefit from assurances that automated systems will be designed, tested, and consis\\xad\\ntently confirmed to work as intended, and that they will be proactively protected from foreseeable unintended harm\\xad\\nful outcomes. \\n•\\nA proprietary model was developed to predict the likelihood of sepsis in hospitalized patients and was imple\\xad\\nmented at hundreds of hospitals around the country. An independent study showed that the model predictions\\nunderperformed relative to the designer’s claims while also causing ‘alert fatigue’ by falsely alerting\\nlikelihood of sepsis.6\\n•\\nOn social media, Black people who quote and criticize racist messages have had their own speech silenced when\\na platform’s automated moderation system failed to distinguish this “counter speech” (or other critique\\nand journalism) from the original hateful messages to which such speech responded.7\\n•\\nA device originally developed to help people track and find lost items has been used as a tool by stalkers to track\\nvictims’ locations in violation of their privacy and safety. The device manufacturer took steps after release to\\nprotect people from unwanted tracking by alerting people on their phones when a device is found to be moving\\nwith them over time and also by having the device make an occasional noise, but not all phones are able\\nto receive the notification and the devices remain a safety concern due to their misuse.8 \\n•\\nAn algorithm used to deploy police was found to repeatedly send police to neighborhoods they regularly visit,\\neven if those neighborhoods were not the ones with the highest crime rates. These incorrect crime predictions\\nwere the result of a feedback loop generated from the reuse of data from previous arrests and algorithm\\npredictions.9\\n16\\n'),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 16, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content=' \\n \\n  \\n \\nSAFE AND EFFECTIVE \\nSYSTEMS \\nWHY THIS PRINCIPLE IS IMPORTANT\\nThis section provides a brief summary of the problems which the principle seeks to address and protect \\nagainst, including illustrative examples. \\n•\\nAI-enabled “nudification” technology that creates images where people appear to be nude—including apps that\\nenable non-technical users to create or alter images of individuals without their consent—has proliferated at an\\nalarming rate. Such technology is becoming a common form of image-based abuse that disproportionately\\nimpacts women. As these tools become more sophisticated, they are producing altered images that are increasing\\xad\\nly realistic and are difficult for both humans and AI to detect as inauthentic. Regardless of authenticity, the expe\\xad\\nrience of harm to victims of non-consensual intimate images can be devastatingly real—affecting their personal\\nand professional lives, and impacting their mental and physical health.10\\n•\\nA company installed AI-powered cameras in its delivery vans in order to evaluate the road safety habits of its driv\\xad\\ners, but the system incorrectly penalized drivers when other cars cut them off or when other events beyond\\ntheir control took place on the road. As a result, drivers were incorrectly ineligible to receive a bonus.11\\n17\\n'),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 17, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content=' \\n \\n \\n \\n \\n \\n \\nSAFE AND EFFECTIVE \\nSYSTEMS \\nWHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS\\nThe expectations for automated systems are meant to serve as a blueprint for the development of additional \\ntechnical standards and practices that are tailored for particular sectors and contexts. \\nIn order to ensure that an automated system is safe and effective, it should include safeguards to protect the \\npublic from harm in a proactive and ongoing manner; avoid use of data inappropriate for or irrelevant to the task \\nat hand, including reuse that could cause compounded harm; and demonstrate the safety and effectiveness of \\nthe system. These expectations are explained below. \\nProtect the public from harm in a proactive and ongoing manner \\nConsultation. The public should be consulted in the design, implementation, deployment, acquisition, and \\nmaintenance phases of automated system development, with emphasis on early-stage consultation before a \\nsystem is introduced or a large change implemented. This consultation should directly engage diverse impact\\xad\\ned communities to consider concerns and risks that may be unique to those communities, or disproportionate\\xad\\nly prevalent or severe for them. The extent of this engagement and the form of outreach to relevant stakehold\\xad\\ners may differ depending on the specific automated system and development phase, but should include \\nsubject matter, sector-specific, and context-specific experts as well as experts on potential impacts such as \\ncivil rights, civil liberties, and privacy experts. For private sector applications, consultations before product \\nlaunch may need to be confidential. Government applications, particularly law enforcement applications or \\napplications that raise national security considerations, may require confidential or limited engagement based \\non system sensitivities and preexisting oversight laws and structures. Concerns raised in this consultation \\nshould be documented, and the automated system developers were proposing to create, use, or deploy should \\nbe reconsidered based on this feedback. \\nTesting. Systems should undergo extensive testing before deployment. This testing should follow \\ndomain-specific best practices, when available, for ensuring the technology will work in its real-world \\ncontext. Such testing should take into account both the specific technology used and the roles of any human \\noperators or reviewers who impact system outcomes or effectiveness; testing should include both automated \\nsystems testing and human-led (manual) testing. Testing conditions should mirror as closely as possible the \\nconditions in which the system will be deployed, and new testing may be required for each deployment to \\naccount for material differences in conditions from one deployment to another. Following testing, system \\nperformance should be compared with the in-place, potentially human-driven, status quo procedures, with \\nexisting human performance considered as a performance baseline for the algorithm to meet pre-deployment, \\nand as a lifecycle minimum performance standard. Decision possibilities resulting from performance testing \\nshould include the possibility of not deploying the system. \\nRisk identification and mitigation. Before deployment, and in a proactive and ongoing manner, poten\\xad\\ntial risks of the automated system should be identified and mitigated. Identified risks should focus on the \\npotential for meaningful impact on people’s rights, opportunities, or access and include those to impacted \\ncommunities that may not be direct users of the automated system, risks resulting from purposeful misuse of \\nthe system, and other concerns identified via the consultation process. Assessment and, where possible, mea\\xad\\nsurement of the impact of risks should be included and balanced such that high impact risks receive attention \\nand mitigation proportionate with those impacts. Automated systems with the intended purpose of violating \\nthe safety of others should not be developed or used; systems with such safety violations as identified unin\\xad\\ntended consequences should not be used until the risk can be mitigated. Ongoing risk mitigation may necessi\\xad\\ntate rollback or significant modification to a launched automated system. \\n18\\n'),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 18, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content=' \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nSAFE AND EFFECTIVE \\nSYSTEMS \\nWHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS\\nThe expectations for automated systems are meant to serve as a blueprint for the development of additional \\ntechnical standards and practices that are tailored for particular sectors and contexts. \\nOngoing monitoring. Automated systems should have ongoing monitoring procedures, including recalibra\\xad\\ntion procedures, in place to ensure that their performance does not fall below an acceptable level over time, \\nbased on changing real-world conditions or deployment contexts, post-deployment modification, or unexpect\\xad\\ned conditions. This ongoing monitoring should include continuous evaluation of performance metrics and \\nharm assessments, updates of any systems, and retraining of any machine learning models as necessary, as well \\nas ensuring that fallback mechanisms are in place to allow reversion to a previously working system. Monitor\\xad\\ning should take into account the performance of both technical system components (the algorithm as well as \\nany hardware components, data inputs, etc.) and human operators. It should include mechanisms for testing \\nthe actual accuracy of any predictions or recommendations generated by a system, not just a human operator’s \\ndetermination of their accuracy. Ongoing monitoring procedures should include manual, human-led monitor\\xad\\ning as a check in the event there are shortcomings in automated monitoring systems. These monitoring proce\\xad\\ndures should be in place for the lifespan of the deployed automated system. \\nClear organizational oversight. Entities responsible for the development or use of automated systems \\nshould lay out clear governance structures and procedures.  This includes clearly-stated governance proce\\xad\\ndures before deploying the system, as well as responsibility of specific individuals or entities to oversee ongoing \\nassessment and mitigation. Organizational stakeholders including those with oversight of the business process \\nor operation being automated, as well as other organizational divisions that may be affected due to the use of \\nthe system, should be involved in establishing governance procedures. Responsibility should rest high enough \\nin the organization that decisions about resources, mitigation, incident response, and potential rollback can be \\nmade promptly, with sufficient weight given to risk mitigation objectives against competing concerns. Those \\nholding this responsibility should be made aware of any use cases with the potential for meaningful impact on \\npeople’s rights, opportunities, or access as determined based on risk identification procedures.  In some cases, \\nit may be appropriate for an independent ethics review to be conducted before deployment. \\nAvoid inappropriate, low-quality, or irrelevant data use and the compounded harm of its \\nreuse \\nRelevant and high-quality data. Data used as part of any automated system’s creation, evaluation, or \\ndeployment should be relevant, of high quality, and tailored to the task at hand. Relevancy should be \\nestablished based on research-backed demonstration of the causal influence of the data to the specific use case \\nor justified more generally based on a reasonable expectation of usefulness in the domain and/or for the \\nsystem design or ongoing development. Relevance of data should not be established solely by appealing to \\nits historical connection to the outcome. High quality and tailored data should be representative of the task at \\nhand and errors from data entry or other sources should be measured and limited. Any data used as the target \\nof a prediction process should receive particular attention to the quality and validity of the predicted outcome \\nor label to ensure the goal of the automated system is appropriately identified and measured. Additionally, \\njustification should be documented for each data attribute and source to explain why it is appropriate to use \\nthat data to inform the results of the automated system and why such use will not violate any applicable laws. \\nIn cases of high-dimensional and/or derived attributes, such justifications can be provided as overall \\ndescriptions of the attribute generation process and appropriateness. \\n19\\n'),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 19, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content=' \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nSAFE AND EFFECTIVE \\nSYSTEMS \\nWHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS\\nThe expectations for automated systems are meant to serve as a blueprint for the development of additional \\ntechnical standards and practices that are tailored for particular sectors and contexts. \\nDerived data sources tracked and reviewed carefully. Data that is derived from other data through \\nthe use of algorithms, such as data derived or inferred from prior model outputs, should be identified and \\ntracked, e.g., via a specialized type in a data schema. Derived data should be viewed as potentially high-risk \\ninputs that may lead to feedback loops, compounded harm, or inaccurate results. Such sources should be care\\xad\\nfully validated against the risk of collateral consequences. \\nData reuse limits in sensitive domains. Data reuse, and especially data reuse in a new context, can result \\nin the spreading and scaling of harms. Data from some domains, including criminal justice data and data indi\\xad\\ncating adverse outcomes in domains such as finance, employment, and housing, is especially sensitive, and in \\nsome cases its reuse is limited by law. Accordingly, such data should be subject to extra oversight to ensure \\nsafety and efficacy. Data reuse of sensitive domain data in other contexts (e.g., criminal data reuse for civil legal \\nmatters or private sector use) should only occur where use of such data is legally authorized and, after examina\\xad\\ntion, has benefits for those impacted by the system that outweigh identified risks and, as appropriate, reason\\xad\\nable measures have been implemented to mitigate the identified risks. Such data should be clearly labeled to \\nidentify contexts for limited reuse based on sensitivity. Where possible, aggregated datasets may be useful for \\nreplacing individual-level sensitive data. \\nDemonstrate the safety and effectiveness of the system \\nIndependent evaluation. Automated systems should be designed to allow for independent evaluation (e.g., \\nvia application programming interfaces). Independent evaluators, such as researchers, journalists, ethics \\nreview boards, inspectors general, and third-party auditors, should be given access to the system and samples \\nof associated data, in a manner consistent with privacy, security, law, or regulation (including, e.g., intellectual \\nproperty law), in order to perform such evaluations. Mechanisms should be included to ensure that system \\naccess for evaluation is: provided in a timely manner to the deployment-ready version of the system; trusted to \\nprovide genuine, unfiltered access to the full system; and truly independent such that evaluator access cannot \\nbe revoked without reasonable and verified justification. \\nReporting.12 Entities responsible for the development or use of automated systems should provide \\nregularly-updated reports that include: an overview of the system, including how it is embedded in the \\norganization’s business processes or other activities, system goals, any human-run procedures that form a \\npart of the system, and specific performance expectations; a description of any data used to train machine \\nlearning models or for other purposes, including how data sources were processed and interpreted, a \\nsummary of what data might be missing, incomplete, or erroneous, and data relevancy justifications; the \\nresults of public consultation such as concerns raised and any decisions made due to these concerns; risk \\nidentification and management assessments and any steps taken to mitigate potential harms; the results of \\nperformance testing including, but not limited to, accuracy, differential demographic impact, resulting \\nerror rates (overall and per demographic group), and comparisons to previously deployed systems; \\nongoing monitoring procedures and regular performance testing reports, including monitoring frequency, \\nresults, and actions taken; and the procedures for and results from independent evaluations. Reporting \\nshould be provided in a plain language and machine-readable manner. \\n20\\n'),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 20, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content=' \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nSAFE AND EFFECTIVE \\nSYSTEMS \\nHOW THESE PRINCIPLES CAN MOVE INTO PRACTICE\\nReal-life examples of how these principles can become reality, through laws, policies, and practical \\ntechnical and sociotechnical approaches to protecting rights, opportunities, and access. \\xad\\xad\\nExecutive Order 13960 on Promoting the Use of Trustworthy Artificial Intelligence in the \\nFederal Government requires that certain federal agencies adhere to nine principles when \\ndesigning, developing, acquiring, or using AI for purposes other than national security or \\ndefense. These principles—while taking into account the sensitive law enforcement and other contexts in which \\nthe federal government may use AI, as opposed to private sector use of AI—require that AI is: (a) lawful and \\nrespectful of our Nation’s values; (b) purposeful and performance-driven; (c) accurate, reliable, and effective; (d) \\nsafe, secure, and resilient; (e) understandable; (f ) responsible and traceable; (g) regularly monitored; (h) transpar-\\nent; and, (i) accountable. The Blueprint for an AI Bill of Rights is consistent with the Executive Order. \\nAffected agencies across the federal government have released AI use case inventories13 and are implementing \\nplans to bring those AI systems into compliance with the Executive Order or retire them. \\nThe law and policy landscape for motor vehicles shows that strong safety regulations—and \\nmeasures to address harms when they occur—can enhance innovation in the context of com-\\nplex technologies. Cars, like automated digital systems, comprise a complex collection of components. \\nThe National Highway Traffic Safety Administration,14 through its rigorous standards and independent \\nevaluation, helps make sure vehicles on our roads are safe without limiting manufacturers’ ability to \\ninnovate.15 At the same time, rules of the road are implemented locally to impose contextually appropriate \\nrequirements on drivers, such as slowing down near schools or playgrounds.16\\nFrom large companies to start-ups, industry is providing innovative solutions that allow \\norganizations to mitigate risks to the safety and efficacy of AI systems, both before \\ndeployment and through monitoring over time.17 These innovative solutions include risk \\nassessments, auditing mechanisms, assessment of organizational procedures, dashboards to allow for ongoing \\nmonitoring, documentation procedures specific to model assessments, and many other strategies that aim to \\nmitigate risks posed by the use of AI to companies’ reputation, legal responsibilities, and other product safety \\nand effectiveness concerns. \\nThe Office of Management and Budget (OMB) has called for an expansion of opportunities \\nfor meaningful stakeholder engagement in the design of programs and services. OMB also \\npoints to numerous examples of effective and proactive stakeholder engagement, including the Community-\\nBased Participatory Research Program developed by the National Institutes of Health and the participatory \\ntechnology assessments developed by the National Oceanic and Atmospheric Administration.18\\nThe National Institute of Standards and Technology (NIST) is developing a risk \\nmanagement framework to better manage risks posed to individuals, organizations, and \\nsociety by AI.19 The NIST AI Risk Management Framework, as mandated by Congress, is intended for \\nvoluntary use to help incorporate trustworthiness considerations into the design, development, use, and \\nevaluation of AI products, services, and systems. The NIST framework is being developed through a consensus-\\ndriven, open, transparent, and collaborative process that includes workshops and other opportunities to provide \\ninput. The NIST framework aims to foster the development of innovative approaches to address \\ncharacteristics of trustworthiness including accuracy, explainability and interpretability, reliability, privacy, \\nrobustness, safety, security (resilience), and mitigation of unintended and/or harmful bias, as well as of \\nharmful \\nuses. \\nThe \\nNIST \\nframework \\nwill \\nconsider \\nand \\nencompass \\nprinciples \\nsuch \\nas \\ntransparency, accountability, and fairness during pre-design, design and development, deployment, use, \\nand testing and evaluation of AI technologies and systems. It is expected to be released in the winter of 2022-23. \\n21\\n'),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 21, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content=' \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nSAFE AND EFFECTIVE \\nSYSTEMS \\nHOW THESE PRINCIPLES CAN MOVE INTO PRACTICE\\nReal-life examples of how these principles can become reality, through laws, policies, and practical \\ntechnical and sociotechnical approaches to protecting rights, opportunities, and access. \\xad\\nSome U.S government agencies have developed specific frameworks for ethical use of AI \\nsystems. The Department of Energy (DOE) has activated the AI Advancement Council that oversees coordina-\\ntion and advises on implementation of the DOE AI Strategy and addresses issues and/or escalations on the \\nethical use and development of AI systems.20 The Department of Defense has adopted Artificial Intelligence \\nEthical Principles, and tenets for Responsible Artificial Intelligence specifically tailored to its national \\nsecurity and defense activities.21 Similarly, the U.S. Intelligence Community (IC) has developed the Principles \\nof Artificial Intelligence Ethics for the Intelligence Community to guide personnel on whether and how to \\ndevelop and use AI in furtherance of the IC\\'s mission, as well as an AI Ethics Framework to help implement \\nthese principles.22\\nThe National Science Foundation (NSF) funds extensive research to help foster the \\ndevelopment of automated systems that adhere to and advance their safety, security and \\neffectiveness. Multiple NSF programs support research that directly addresses many of these principles: \\nthe National AI Research Institutes23 support research on all aspects of safe, trustworthy, fair, and explainable \\nAI algorithms and systems; the Cyber Physical Systems24 program supports research on developing safe \\nautonomous and cyber physical systems with AI components; the Secure and Trustworthy Cyberspace25 \\nprogram supports research on cybersecurity and privacy enhancing technologies in automated systems; the \\nFormal Methods in the Field26 program supports research on rigorous formal verification and analysis of \\nautomated systems and machine learning, and the Designing Accountable Software Systems27 program supports \\nresearch on rigorous and reproducible methodologies for developing software systems with legal and regulatory \\ncompliance in mind. \\nSome state legislatures have placed strong transparency and validity requirements on \\nthe use of pretrial risk assessments. The use of algorithmic pretrial risk assessments has been a \\ncause of concern for civil rights groups.28 Idaho Code Section 19-1910, enacted in 2019,29 requires that any \\npretrial risk assessment, before use in the state, first be \"shown to be free of bias against any class of \\nindividuals protected from discrimination by state or federal law\", that any locality using a pretrial risk \\nassessment must first formally validate the claim of its being free of bias, that \"all documents, records, and \\ninformation used to build or validate the risk assessment shall be open to public inspection,\" and that assertions \\nof trade secrets cannot be used \"to quash discovery in a criminal matter by a party to a criminal case.\" \\n22\\n'),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 22, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content=' \\xad\\xad\\xad\\xad\\xad\\xad\\xad\\nALGORITHMIC DISCRIMINATION Protections\\nYou should not face discrimination by algorithms \\nand systems should be used and designed in an \\nequitable \\nway. \\nAlgorithmic \\ndiscrimination \\noccurs when \\nautomated systems contribute to unjustified different treatment or \\nimpacts disfavoring people based on their race, color, ethnicity, \\nsex \\n(including \\npregnancy, \\nchildbirth, \\nand \\nrelated \\nmedical \\nconditions, \\ngender \\nidentity, \\nintersex \\nstatus, \\nand \\nsexual \\norientation), religion, age, national origin, disability, veteran status, \\ngenetic infor-mation, or any other classification protected by law. \\nDepending on the specific circumstances, such algorithmic \\ndiscrimination may violate legal protections. Designers, developers, \\nand deployers of automated systems should take proactive and \\ncontinuous measures to protect individuals and communities \\nfrom algorithmic discrimination and to use and design systems in \\nan equitable way.  This protection should include proactive equity \\nassessments as part of the system design, use of representative data \\nand protection against proxies for demographic features, ensuring \\naccessibility for people with disabilities in design and development, \\npre-deployment and ongoing disparity testing and mitigation, and \\nclear organizational oversight. Independent evaluation and plain \\nlanguage reporting in the form of an algorithmic impact assessment, \\nincluding disparity testing results and mitigation information, \\nshould be performed and made public whenever possible to confirm \\nthese protections.\\n23\\n'),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 23, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content=\" \\n \\n  \\n \\n \\n \\n \\n \\nAlgorithmic \\nDiscrimination \\nProtections \\nWHY THIS PRINCIPLE IS IMPORTANT\\nThis section provides a brief summary of the problems which the principle seeks to address and protect \\nagainst, including illustrative examples. \\nThere is extensive evidence showing that automated systems can produce inequitable outcomes and amplify \\nexisting inequity.30 Data that fails to account for existing systemic biases in American society can result in a range of \\nconsequences. For example, facial recognition technology that can contribute to wrongful and discriminatory \\narrests,31 hiring algorithms that inform discriminatory decisions, and healthcare algorithms that discount \\nthe severity of certain diseases in Black Americans. Instances of discriminatory practices built into and \\nresulting from AI and other automated systems exist across many industries, areas, and contexts. While automated \\nsystems have the capacity to drive extraordinary advances and innovations, algorithmic discrimination \\nprotections should be built into their design, deployment, and ongoing use. \\nMany companies, non-profits, and federal government agencies are already taking steps to ensure the public \\nis protected from algorithmic discrimination. Some companies have instituted bias testing as part of their product \\nquality assessment and launch procedures, and in some cases this testing has led products to be changed or not \\nlaunched, preventing harm to the public. Federal government agencies have been developing standards and guidance \\nfor the use of automated systems in order to help prevent bias. Non-profits and companies have developed best \\npractices for audits and impact assessments to help identify potential algorithmic discrimination and provide \\ntransparency to the public in the mitigation of such biases. \\nBut there is much more work to do to protect the public from algorithmic discrimination to use and design \\nautomated systems in an equitable way. The guardrails protecting the public from discrimination in their daily \\nlives should include their digital lives and impacts—basic safeguards against abuse, bias, and discrimination to \\nensure that all people are treated fairly when automated systems are used. This includes all dimensions of their \\nlives, from hiring to loan approvals, from medical treatment and payment to encounters with the criminal \\njustice system. Ensuring equity should also go beyond existing guardrails to consider the holistic impact that \\nautomated systems make on underserved communities and to institute proactive protections that support these \\ncommunities. \\n•\\nAn automated system using nontraditional factors such as educational attainment and employment history as\\npart of its loan underwriting and pricing model was found to be much more likely to charge an applicant who\\nattended a Historically Black College or University (HBCU) higher loan prices for refinancing a student loan\\nthan an applicant who did not attend an HBCU. This was found to be true even when controlling for\\nother credit-related factors.32\\n•\\nA hiring tool that learned the features of a company's employees (predominantly men) rejected women appli\\xad\\ncants for spurious and discriminatory reasons; resumes with the word “women’s,” such as “women’s\\nchess club captain,” were penalized in the candidate ranking.33\\n•\\nA predictive model marketed as being able to predict whether students are likely to drop out of school was\\nused by more than 500 universities across the country. The model was found to use race directly as a predictor,\\nand also shown to have large disparities by race; Black students were as many as four times as likely as their\\notherwise similar white peers to be deemed at high risk of dropping out. These risk scores are used by advisors \\nto guide students towards or away from majors, and some worry that they are being used to guide\\nBlack students away from math and science subjects.34\\n•\\nA risk assessment tool designed to predict the risk of recidivism for individuals in federal custody showed\\nevidence of disparity in prediction. The tool overpredicts the risk of recidivism for some groups of color on the\\ngeneral recidivism tools, and underpredicts the risk of recidivism for some groups of color on some of the\\nviolent recidivism tools. The Department of Justice is working to reduce these disparities and has\\npublicly released a report detailing its review of the tool.35 \\n24\\n\"),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 24, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content=' \\n \\n  \\nWHY THIS PRINCIPLE IS IMPORTANT\\nThis section provides a brief summary of the problems which the principle seeks to address and protect \\nagainst, including illustrative examples. \\xad\\xad\\xad\\n•\\nAn automated sentiment analyzer, a tool often used by technology platforms to determine whether a state-\\nment posted online expresses a positive or negative sentiment, was found to be biased against Jews and gay\\npeople. For example, the analyzer marked the statement “I’m a Jew” as representing a negative sentiment,\\nwhile “I’m a Christian” was identified as expressing a positive sentiment.36 This could lead to the\\npreemptive blocking of social media comments such as: “I’m gay.” A related company with this bias concern\\nhas made their data public to encourage researchers to help address the issue37 and has released reports\\nidentifying and measuring this problem as well as detailing attempts to address it.38\\n•\\nSearches for “Black girls,” “Asian girls,” or “Latina girls” return predominantly39 sexualized content, rather\\nthan role models, toys, or activities.40 Some search engines have been working to reduce the prevalence of\\nthese results, but the problem remains.41\\n•\\nAdvertisement delivery systems that predict who is most likely to click on a job advertisement end up deliv-\\nering ads in ways that reinforce racial and gender stereotypes, such as overwhelmingly directing supermar-\\nket cashier ads to women and jobs with taxi companies to primarily Black people.42\\xad\\n•\\nBody scanners, used by TSA at airport checkpoints, require the operator to select a “male” or “female”\\nscanning setting based on the passenger’s sex, but the setting is chosen based on the operator’s perception of\\nthe passenger’s gender identity. These scanners are more likely to flag transgender travelers as requiring\\nextra screening done by a person. Transgender travelers have described degrading experiences associated\\nwith these extra screenings.43 TSA has recently announced plans to implement a gender-neutral algorithm44 \\nwhile simultaneously enhancing the security effectiveness capabilities of the existing technology. \\n•\\nThe National Disabled Law Students Association expressed concerns that individuals with disabilities were\\nmore likely to be flagged as potentially suspicious by remote proctoring AI systems because of their disabili-\\nty-specific access needs such as needing longer breaks or using screen readers or dictation software.45 \\n•\\nAn algorithm designed to identify patients with high needs for healthcare systematically assigned lower\\nscores (indicating that they were not as high need) to Black patients than to those of white patients, even\\nwhen those patients had similar numbers of chronic conditions and other markers of health.46 In addition,\\nhealthcare clinical algorithms that are used by physicians to guide clinical decisions may include\\nsociodemographic variables that adjust or “correct” the algorithm’s output on the basis of a patient’s race or\\nethnicity, which can lead to race-based health inequities.47\\n25\\nAlgorithmic \\nDiscrimination \\nProtections \\n'),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 25, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content=' \\n \\n \\n \\n \\n \\n \\nWHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS\\nThe expectations for automated systems are meant to serve as a blueprint for the development of additional \\ntechnical standards and practices that are tailored for particular sectors and contexts. \\nAny automated system should be tested to help ensure it is free from algorithmic discrimination before it can be \\nsold or used. Protection against algorithmic discrimination should include designing to ensure equity, broadly \\nconstrued.  Some algorithmic discrimination is already prohibited under existing anti-discrimination law. The \\nexpectations set out below describe proactive technical and policy steps that can be taken to not only \\nreinforce those legal protections but extend beyond them to ensure equity for underserved communities48 \\neven in circumstances where a specific legal protection may not be clearly established. These protections \\nshould be instituted throughout the design, development, and deployment process and are described below \\nroughly in the order in which they would be instituted. \\nProtect the public from algorithmic discrimination in a proactive and ongoing manner \\nProactive assessment of equity in design. Those responsible for the development, use, or oversight of \\nautomated systems should conduct proactive equity assessments in the design phase of the technology \\nresearch and development or during its acquisition to review potential input data, associated historical \\ncontext, accessibility for people with disabilities, and societal goals to identify potential discrimination and \\neffects on equity resulting from the introduction of the technology. The assessed groups should be as inclusive \\nas possible of the underserved communities mentioned in the equity definition:  Black, Latino, and Indigenous \\nand Native American persons, Asian Americans and Pacific Islanders and other persons of color; members of \\nreligious minorities; women, girls, and non-binary people; lesbian, gay, bisexual, transgender, queer, and inter-\\nsex (LGBTQI+) persons; older adults; persons with disabilities; persons who live in rural areas; and persons \\notherwise adversely affected by persistent poverty or inequality. Assessment could include both qualitative \\nand quantitative evaluations of the system. This equity assessment should also be considered a core part of the \\ngoals of the consultation conducted as part of the safety and efficacy review. \\nRepresentative and robust data. Any data used as part of system development or assessment should be \\nrepresentative of local communities based on the planned deployment setting and should be reviewed for bias \\nbased on the historical and societal context of the data. Such data should be sufficiently robust to identify and \\nhelp to mitigate biases and potential harms. \\nGuarding against proxies.  Directly using demographic information in the design, development, or \\ndeployment of an automated system (for purposes other than evaluating a system for discrimination or using \\na system to counter discrimination) runs a high risk of leading to algorithmic discrimination and should be \\navoided. In many cases, attributes that are highly correlated with demographic features, known as proxies, can \\ncontribute to algorithmic discrimination. In cases where use of the demographic features themselves would \\nlead to illegal algorithmic discrimination, reliance on such proxies in decision-making (such as that facilitated \\nby an algorithm) may also be prohibited by law. Proactive testing should be performed to identify proxies by \\ntesting for correlation between demographic information and attributes in any data used as part of system \\ndesign, development, or use. If a proxy is identified, designers, developers, and deployers should remove the \\nproxy; if needed, it may be possible to identify alternative attributes that can be used instead. At a minimum, \\norganizations should ensure a proxy feature is not given undue weight and should monitor the system closely \\nfor any resulting algorithmic discrimination.   \\n26\\nAlgorithmic \\nDiscrimination \\nProtections \\n'),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 26, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content=\" \\n \\n \\n \\n \\n \\n \\n \\nWHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS\\nThe expectations for automated systems are meant to serve as a blueprint for the development of additional \\ntechnical standards and practices that are tailored for particular sectors and contexts. \\nEnsuring accessibility during design, development, and deployment. Systems should be \\ndesigned, developed, and deployed by organizations in ways that ensure accessibility to people with disabili\\xad\\nties. This should include consideration of a wide variety of disabilities, adherence to relevant accessibility \\nstandards, and user experience research both before and after deployment to identify and address any accessi\\xad\\nbility barriers to the use or effectiveness of the automated system. \\nDisparity assessment. Automated systems should be tested using a broad set of measures to assess wheth\\xad\\ner the system components, both in pre-deployment testing and in-context deployment, produce disparities. \\nThe demographics of the assessed groups should be as inclusive as possible of race, color, ethnicity, sex \\n(including pregnancy, childbirth, and related medical conditions, gender identity, intersex status, and sexual \\norientation), religion, age, national origin, disability, veteran status, genetic information, or any other classifi\\xad\\ncation protected by law. The broad set of measures assessed should include demographic performance mea\\xad\\nsures, overall and subgroup parity assessment, and calibration. Demographic data collected for disparity \\nassessment should be separated from data used for the automated system and privacy protections should be \\ninstituted; in some cases it may make sense to perform such assessment using a data sample. For every \\ninstance where the deployed automated system leads to different treatment or impacts disfavoring the identi\\xad\\nfied groups, the entity governing, implementing, or using the system should document the disparity and a \\njustification for any continued use of the system. \\nDisparity mitigation. When a disparity assessment identifies a disparity against an assessed group, it may \\nbe appropriate to take steps to mitigate or eliminate the disparity. In some cases, mitigation or elimination of \\nthe disparity may be required by law. \\nDisparities that have the potential to lead to algorithmic \\ndiscrimination, cause meaningful harm, or violate equity49 goals should be mitigated. When designing and \\nevaluating an automated system, steps should be taken to evaluate multiple models and select the one that \\nhas the least adverse impact, modify data input choices, or otherwise identify a system with fewer \\ndisparities. If adequate mitigation of the disparity is not possible, then the use of the automated system \\nshould be reconsidered. One of the considerations in whether to use the system should be the validity of any \\ntarget measure; unobservable targets may result in the inappropriate use of proxies. Meeting these \\nstandards may require instituting mitigation procedures and other protective measures to address \\nalgorithmic discrimination, avoid meaningful harm, and achieve equity goals. \\nOngoing monitoring and mitigation. Automated systems should be regularly monitored to assess algo\\xad\\nrithmic discrimination that might arise from unforeseen interactions of the system with inequities not \\naccounted for during the pre-deployment testing, changes to the system after deployment, or changes to the \\ncontext of use or associated data. Monitoring and disparity assessment should be performed by the entity \\ndeploying or using the automated system to examine whether the system has led to algorithmic discrimina\\xad\\ntion when deployed. This assessment should be performed regularly and whenever a pattern of unusual \\nresults is occurring. It can be performed using a variety of approaches, taking into account whether and how \\ndemographic information of impacted people is available, for example via testing with a sample of users or via \\nqualitative user experience research. Riskier and higher-impact systems should be monitored and assessed \\nmore frequently. Outcomes of this assessment should include additional disparity mitigation, if needed, or \\nfallback to earlier procedures in the case that equity standards are no longer met and can't be mitigated, and \\nprior mechanisms provide better adherence to equity standards. \\n27\\nAlgorithmic \\nDiscrimination \\nProtections \\n\"),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 27, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content=' \\n \\n \\n \\n \\n \\n \\nWHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS\\nThe expectations for automated systems are meant to serve as a blueprint for the development of additional \\ntechnical standards and practices that are tailored for particular sectors and contexts. \\nDemonstrate that the system protects against algorithmic discrimination \\nIndependent evaluation. As described in the section on Safe and Effective Systems, entities should allow \\nindependent evaluation of potential algorithmic discrimination caused by automated systems they use or \\noversee. In the case of public sector uses, these independent evaluations should be made public unless law \\nenforcement or national security restrictions prevent doing so. Care should be taken to balance individual \\nprivacy with evaluation data access needs; in many cases, policy-based and/or technological innovations and \\ncontrols allow access to such data without compromising privacy. \\nReporting. Entities responsible for the development or use of automated systems should provide \\nreporting of an appropriately designed algorithmic impact assessment,50 with clear specification of who \\nperforms the assessment, who evaluates the system, and how corrective actions are taken (if necessary) in \\nresponse to the assessment. This algorithmic impact assessment should include at least: the results of any \\nconsultation, design stage equity assessments (potentially including qualitative analysis), accessibility \\ndesigns and testing, disparity testing, document any remaining disparities, and detail any mitigation \\nimplementation and assessments. This algorithmic impact assessment should be made public whenever \\npossible. Reporting should be provided in a clear and machine-readable manner using plain language to \\nallow for more straightforward public accountability. \\n28\\nAlgorithmic \\nDiscrimination \\nProtections \\n'),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 28, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content=\" \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\n \\n \\n \\n \\nHOW THESE PRINCIPLES CAN MOVE INTO PRACTICE\\nReal-life examples of how these principles can become reality, through laws, policies, and practical \\ntechnical and sociotechnical approaches to protecting rights, opportunities, and access. \\nThe federal government is working to combat discrimination in mortgage lending. The Depart\\xad\\nment of Justice has launched a nationwide initiative to combat redlining, which includes reviewing how \\nlenders who may be avoiding serving communities of color are conducting targeted marketing and advertising.51 \\nThis initiative will draw upon strong partnerships across federal agencies, including the Consumer Financial \\nProtection Bureau and prudential regulators. The Action Plan to Advance Property Appraisal and Valuation \\nEquity includes a commitment from the agencies that oversee mortgage lending to include a \\nnondiscrimination standard in the proposed rules for Automated Valuation Models.52\\nThe Equal Employment Opportunity Commission and the Department of Justice have clearly \\nlaid out how employers’ use of AI and other automated systems can result in \\ndiscrimination against job applicants and employees with disabilities.53 The documents explain \\nhow employers’ use of software that relies on algorithmic decision-making may violate existing requirements \\nunder Title I of the Americans with Disabilities Act (“ADA”). This technical assistance also provides practical \\ntips to employers on how to comply with the ADA, and to job applicants and employees who think that their \\nrights may have been violated. \\nDisparity assessments identified harms to Black patients' healthcare access. A widely \\nused healthcare algorithm relied on the cost of each patient’s past medical care to predict future medical needs, \\nrecommending early interventions for the patients deemed most at risk. This process discriminated \\nagainst Black patients, who generally have less access to medical care and therefore have generated less cost \\nthan white patients with similar illness and need. A landmark study documented this pattern and proposed \\npractical ways that were shown to reduce this bias, such as focusing specifically on active chronic health \\nconditions or avoidable future costs related to emergency visits and hospitalization.54 \\nLarge employers have developed best practices to scrutinize the data and models used \\nfor hiring. An industry initiative has developed Algorithmic Bias Safeguards for the Workforce, a structured \\nquestionnaire that businesses can use proactively when procuring software to evaluate workers. It covers \\nspecific technical questions such as the training data used, model training process, biases identified, and \\nmitigation steps employed.55 \\nStandards organizations have developed guidelines to incorporate accessibility criteria \\ninto technology design processes. The most prevalent in the United States is the Access Board’s Section \\n508 regulations,56 which are the technical standards for federal information communication technology (software, \\nhardware, and web). Other standards include those issued by the International Organization for \\nStandardization,57 and the World Wide Web Consortium Web Content Accessibility Guidelines,58 a globally \\nrecognized voluntary consensus standard for web content and other information and communications \\ntechnology. \\nNIST has released Special Publication 1270, Towards a Standard for Identifying and Managing Bias \\nin Artificial Intelligence.59 The special publication: describes the stakes and challenges of bias in artificial \\nintelligence and provides examples of how and why it can chip away at public trust; identifies three categories \\nof bias in AI – systemic, statistical, and human – and describes how and where they contribute to harms; and \\ndescribes three broad challenges for mitigating bias – datasets, testing and evaluation, and human factors – and \\nintroduces preliminary guidance for addressing them. Throughout, the special publication takes a socio-\\ntechnical perspective to identifying and managing AI bias. \\n29\\nAlgorithmic \\nDiscrimination \\nProtections \\n\"),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 29, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content='You should be protected from abusive data practices via built-in \\nprotections and you should have agency over how data about \\nyou is used. You should be protected from violations of privacy through \\ndesign choices that ensure such protections are included by default, including \\nensuring that data collection conforms to reasonable expectations and that \\nonly data strictly necessary for the specific context is collected. Designers, de\\xad\\nvelopers, and deployers of automated systems should seek your permission \\nand respect your decisions regarding collection, use, access, transfer, and de\\xad\\nletion of your data in appropriate ways and to the greatest extent possible; \\nwhere not possible, alternative privacy by design safeguards should be used. \\nSystems should not employ user experience and design decisions that obfus\\xad\\ncate user choice or burden users with defaults that are privacy invasive. Con\\xad\\nsent should only be used to justify collection of data in cases where it can be \\nappropriately and meaningfully given. Any consent requests should be brief, \\nbe understandable in plain language, and give you agency over data collection \\nand the specific context of use; current hard-to-understand no\\xad\\ntice-and-choice practices for broad uses of data should be changed. Enhanced \\nprotections and restrictions for data and inferences related to sensitive do\\xad\\nmains, including health, work, education, criminal justice, and finance, and \\nfor data pertaining to youth should put you first. In sensitive domains, your \\ndata and related inferences should only be used for necessary functions, and \\nyou should be protected by ethical review and use prohibitions. You and your \\ncommunities should be free from unchecked surveillance; surveillance tech\\xad\\nnologies should be subject to heightened oversight that includes at least \\npre-deployment assessment of their potential harms and scope limits to pro\\xad\\ntect privacy and civil liberties. Continuous surveillance and monitoring \\nshould not be used in education, work, housing, or in other contexts where the \\nuse of such surveillance technologies is likely to limit rights, opportunities, or \\naccess. Whenever possible, you should have access to reporting that confirms \\nyour data decisions have been respected and provides an assessment of the \\npotential impact of surveillance technologies on your rights, opportunities, or \\naccess. \\nDATA PRIVACY\\n30\\n'),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 30, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content=' \\n \\n  \\n \\n \\n \\nDATA PRIVACY \\nWHY THIS PRINCIPLE IS IMPORTANT\\nThis section provides a brief summary of the problems which the principle seeks to address and protect \\nagainst, including illustrative examples. \\nData privacy is a foundational and cross-cutting principle required for achieving all others in this framework. Surveil\\xad\\nlance and data collection, sharing, use, and reuse now sit at the foundation of business models across many industries, \\nwith more and more companies tracking the behavior of the American public, building individual profiles based on \\nthis data, and using this granular-level information as input into automated systems that further track, profile, and \\nimpact the American public. Government agencies, particularly law enforcement agencies, also use and help develop \\na variety of technologies that enhance and expand surveillance capabilities, which similarly collect data used as input \\ninto other automated systems that directly impact people’s lives. Federal law has not grown to address the expanding \\nscale of private data collection, or of the ability of governments at all levels to access that data and leverage the means \\nof private collection.  \\nMeanwhile, members of the American public are often unable to access their personal data or make critical decisions \\nabout its collection and use. Data brokers frequently collect consumer data from numerous sources without \\nconsumers’ permission or knowledge.60 Moreover, there is a risk that inaccurate and faulty data can be used to \\nmake decisions about their lives, such as whether they will qualify for a loan or get a job. Use of surveillance \\ntechnologies has increased in schools and workplaces, and, when coupled with consequential management and \\nevaluation decisions, it is leading to mental health harms such as lowered self-confidence, anxiety, depression, and \\na reduced ability to use analytical reasoning.61 Documented patterns show that personal data is being aggregated by \\ndata brokers to profile communities in harmful ways.62 The impact of all this data harvesting is corrosive, \\nbreeding distrust, anxiety, and other mental health problems; chilling speech, protest, and worker organizing; and \\nthreatening our democratic process.63 The American public should be protected from these growing risks. \\nIncreasingly, some companies are taking these concerns seriously and integrating mechanisms to protect consumer \\nprivacy into their products by design and by default, including by minimizing the data they collect, communicating \\ncollection and use clearly, and improving security practices. Federal government surveillance and other collection and \\nuse of data is governed by legal protections that help to protect civil liberties and provide for limits on data retention \\nin some cases. Many states have also enacted consumer data privacy protection regimes to address some of these \\nharms. \\nHowever, these are not yet standard practices, and the United States lacks a comprehensive statutory or regulatory \\nframework governing the rights of the public when it comes to personal data. While a patchwork of laws exists to \\nguide the collection and use of personal data in specific contexts, including health, employment, education, and credit, \\nit can be unclear how these laws apply in other contexts and in an increasingly automated society. Additional protec\\xad\\ntions would assure the American public that the automated systems they use are not monitoring their activities, \\ncollecting information on their lives, or otherwise surveilling them without context-specific consent or legal authori\\xad\\nty. \\n31\\n'),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 31, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content=\" \\n \\n  \\n \\nDATA PRIVACY \\nWHY THIS PRINCIPLE IS IMPORTANT\\nThis section provides a brief summary of the problems which the principle seeks to address and protect \\nagainst, including illustrative examples. \\n•\\nAn insurer might collect data from a person's social media presence as part of deciding what life\\ninsurance rates they should be offered.64\\n•\\nA data broker harvested large amounts of personal data and then suffered a breach, exposing hundreds of\\nthousands of people to potential identity theft. 65\\n•\\nA local public housing authority installed a facial recognition system at the entrance to housing complexes to\\nassist law enforcement with identifying individuals viewed via camera when police reports are filed, leading\\nthe community, both those living in the housing complex and not, to have videos of them sent to the local\\npolice department and made available for scanning by its facial recognition software.66\\n•\\nCompanies use surveillance software to track employee discussions about union activity and use the\\nresulting data to surveil individual employees and surreptitiously intervene in discussions.67\\n32\\n\"),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 32, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content=' \\n \\n \\n \\n \\n \\nDATA PRIVACY \\nWHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS\\nThe expectations for automated systems are meant to serve as a blueprint for the development of additional \\ntechnical standards and practices that are tailored for particular sectors and contexts. \\nTraditional terms of service—the block of text that the public is accustomed to clicking through when using a web\\xad\\nsite or digital app—are not an adequate mechanism for protecting privacy. The American public should be protect\\xad\\ned via built-in privacy protections, data minimization, use and collection limitations, and transparency, in addition \\nto being entitled to clear mechanisms to control access to and use of their data—including their metadata—in a \\nproactive, informed, and ongoing way. Any automated system collecting, using, sharing, or storing personal data \\nshould meet these expectations. \\nProtect privacy by design and by default \\nPrivacy by design and by default. Automated systems should be designed and built with privacy protect\\xad\\ned by default. Privacy risks should be assessed throughout the development life cycle, including privacy risks \\nfrom reidentification, and appropriate technical and policy mitigation measures should be implemented. This \\nincludes potential harms to those who are not users of the automated system, but who may be harmed by \\ninferred data, purposeful privacy violations, or community surveillance or other community harms. Data \\ncollection should be minimized and clearly communicated to the people whose data is collected. Data should \\nonly be collected or used for the purposes of training or testing machine learning models if such collection and \\nuse is legal and consistent with the expectations of the people whose data is collected. User experience \\nresearch should be conducted to confirm that people understand what data is being collected about them and \\nhow it will be used, and that this collection matches their expectations and desires. \\nData collection and use-case scope limits. Data collection should be limited in scope, with specific, \\nnarrow identified goals, to avoid \"mission creep.\"  Anticipated data collection should be determined to be \\nstrictly necessary to the identified goals and should be minimized as much as possible. Data collected based on \\nthese identified goals and for a specific context should not be used in a different context without assessing for \\nnew privacy risks and implementing appropriate mitigation measures, which may include express consent. \\nClear timelines for data retention should be established, with data deleted as soon as possible in accordance \\nwith legal or policy-based limitations. Determined data retention timelines should be documented and justi\\xad\\nfied. \\nRisk identification and mitigation. Entities that collect, use, share, or store sensitive data should \\nattempt to proactively identify harms and seek to manage them so as to avoid, mitigate, and respond appropri\\xad\\nately to identified risks. Appropriate responses include determining not to process data when the privacy risks \\noutweigh the benefits or implementing measures to mitigate acceptable risks. Appropriate responses do not \\ninclude sharing or transferring the privacy risks to users via notice or consent requests where users could not \\nreasonably be expected to understand the risks without further support. \\nPrivacy-preserving security. Entities creating, using, or governing automated systems should follow \\nprivacy and security best practices designed to ensure data and metadata do not leak beyond the specific \\nconsented use case. Best practices could include using privacy-enhancing cryptography or other types of \\nprivacy-enhancing technologies or fine-grained permissions and access control mechanisms, along with \\nconventional system security protocols. \\n33\\n'),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 33, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content=' \\n \\n \\n \\n \\n \\nDATA PRIVACY \\nWHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS\\nThe expectations for automated systems are meant to serve as a blueprint for the development of additional \\ntechnical standards and practices that are tailored for particular sectors and contexts. \\nProtect the public from unchecked surveillance \\nHeightened oversight of surveillance. Surveillance or monitoring systems should be subject to \\nheightened oversight that includes at a minimum assessment of potential harms during design (before deploy\\xad\\nment) and in an ongoing manner, to ensure that the American public’s rights, opportunities, and access are \\nprotected. This assessment should be done before deployment and should give special attention to ensure \\nthere is not algorithmic discrimination, especially based on community membership, when deployed in a \\nspecific real-world context. Such assessment should then be reaffirmed in an ongoing manner as long as the \\nsystem is in use. \\nLimited and proportionate surveillance. Surveillance should be avoided unless it is strictly necessary \\nto achieve a legitimate purpose and it is proportionate to the need. Designers, developers, and deployers of \\nsurveillance systems should use the least invasive means of monitoring available and restrict monitoring to the \\nminimum number of subjects possible. To the greatest extent possible consistent with law enforcement and \\nnational security needs, individuals subject to monitoring should be provided with clear and specific notice \\nbefore it occurs and be informed about how the data gathered through surveillance will be used. \\nScope limits on surveillance to protect rights and democratic values. Civil liberties and civil \\nrights must not be limited by the threat of surveillance or harassment facilitated or aided by an automated \\nsystem. Surveillance systems should not be used to monitor the exercise of democratic rights, such as voting, \\nprivacy, peaceful assembly, speech, or association, in a way that limits the exercise of civil rights or civil liber\\xad\\nties. Information about or algorithmically-determined assumptions related to identity should be carefully \\nlimited if used to target or guide surveillance systems in order to avoid algorithmic discrimination; such iden\\xad\\ntity-related information includes group characteristics or affiliations, geographic designations, location-based \\nand association-based inferences, social networks, and biometrics. Continuous surveillance and monitoring \\nsystems should not be used in physical or digital workplaces (regardless of employment status), public educa\\xad\\ntional institutions, and public accommodations. Continuous surveillance and monitoring systems should not \\nbe used in a way that has the effect of limiting access to critical resources or services or suppressing the exer\\xad\\ncise of rights, even where the organization is not under a particular duty to protect those rights. \\nProvide the public with mechanisms for appropriate and meaningful consent, access, and \\ncontrol over their data \\nUse-specific consent. Consent practices should not allow for abusive surveillance practices. Where data \\ncollectors or automated systems seek consent, they should seek it for specific, narrow use contexts, for specif\\xad\\nic time durations, and for use by specific entities. Consent should not extend if any of these conditions change; \\nconsent should be re-acquired before using data if the use case changes, a time limit elapses, or data is trans\\xad\\nferred to another entity (including being shared or sold). Consent requested should be limited in scope and \\nshould not request consent beyond what is required. Refusal to provide consent should be allowed, without \\nadverse effects, to the greatest extent possible based on the needs of the use case. \\nBrief and direct consent requests. When seeking consent from users short, plain language consent \\nrequests should be used so that users understand for what use contexts, time span, and entities they are \\nproviding data and metadata consent. User experience research should be performed to ensure these consent \\nrequests meet performance standards for readability and comprehension. This includes ensuring that consent \\nrequests are accessible to users with disabilities and are available in the language(s) and reading level appro\\xad\\npriate for the audience.  User experience design choices that intentionally obfuscate or manipulate user \\nchoice (i.e., “dark patterns”) should be not be used. \\n34\\n'),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 34, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content=' \\n \\n \\n \\n \\n \\nDATA PRIVACY \\nWHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS\\nThe expectations for automated systems are meant to serve as a blueprint for the development of additional \\ntechnical standards and practices that are tailored for particular sectors and contexts. \\nData access and correction. People whose data is collected, used, shared, or stored by automated \\nsystems should be able to access data and metadata about themselves, know who has access to this data, and \\nbe able to correct it if necessary. Entities should receive consent before sharing data with other entities and \\nshould keep records of what data is shared and with whom. \\nConsent withdrawal and data deletion. Entities should allow (to the extent legally permissible) with\\xad\\ndrawal of data access consent, resulting in the deletion of user data, metadata, and the timely removal of \\ntheir data from any systems (e.g., machine learning models) derived from that data.68\\nAutomated system support. Entities designing, developing, and deploying automated systems should \\nestablish and maintain the capabilities that will allow individuals to use their own automated systems to help \\nthem make consent, access, and control decisions in a complex data ecosystem. Capabilities include machine \\nreadable data, standardized data formats, metadata or tags for expressing data processing permissions and \\npreferences and data provenance and lineage, context of use and access-specific tags, and training models for \\nassessing privacy risk. \\nDemonstrate that data privacy and user control are protected \\nIndependent evaluation. As described in the section on Safe and Effective Systems, entities should allow \\nindependent evaluation of the claims made regarding data policies. These independent evaluations should be \\nmade public whenever possible. Care will need to be taken to balance individual privacy with evaluation data \\naccess needs. \\nReporting. When members of the public wish to know what data about them is being used in a system, the \\nentity responsible for the development of the system should respond quickly with a report on the data it has \\ncollected or stored about them. Such a report should be machine-readable, understandable by most users, and \\ninclude, to the greatest extent allowable under law, any data and metadata about them or collected from them, \\nwhen and how their data and metadata were collected, the specific ways that data or metadata are being used, \\nwho has access to their data and metadata, and what time limitations apply to these data. In cases where a user \\nlogin is not available, identity verification may need to be performed before providing such a report to ensure \\nuser privacy. Additionally, summary reporting should be proactively made public with general information \\nabout how peoples’ data and metadata is used, accessed, and stored. Summary reporting should include the \\nresults of any surveillance pre-deployment assessment, including disparity assessment in the real-world \\ndeployment context, the specific identified goals of any data collection, and the assessment done to ensure \\nonly the minimum required data is collected. It should also include documentation about the scope limit \\nassessments, including data retention timelines and associated justification, and an assessment of the \\nimpact of surveillance or data collection on rights, opportunities, and access. Where possible, this \\nassessment of the impact of surveillance should be done by an independent party. Reporting should be \\nprovided in a clear and machine-readable manner.  \\n35\\n'),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 35, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content=' \\n \\n \\n \\nDATA PRIVACY \\nEXTRA PROTECTIONS FOR DATA RELATED TO SENSITIVE\\nDOMAINS\\nSome domains, including health, employment, education, criminal justice, and personal finance, have long been \\nsingled out as sensitive domains deserving of enhanced data protections. This is due to the intimate nature of these \\ndomains as well as the inability of individuals to opt out of these domains in any meaningful way, and the \\nhistorical discrimination that has often accompanied data knowledge.69 Domains understood by the public to be \\nsensitive also change over time, including because of technological developments. Tracking and monitoring \\ntechnologies, personal tracking devices, and our extensive data footprints are used and misused more than ever \\nbefore; as such, the protections afforded by current legal guidelines may be inadequate. The American public \\ndeserves assurances that data related to such sensitive domains is protected and used appropriately and only in \\nnarrowly defined contexts with clear benefits to the individual and/or society. \\nTo this end, automated systems that collect, use, share, or store data related to these sensitive domains should meet \\nadditional expectations. Data and metadata are sensitive if they pertain to an individual in a sensitive domain (defined \\nbelow); are generated by technologies used in a sensitive domain; can be used to infer data from a sensitive domain or \\nsensitive data about an individual (such as disability-related data, genomic data, biometric data, behavioral data, \\ngeolocation data, data related to interaction with the criminal justice system, relationship history and legal status such \\nas custody and divorce information, and home, work, or school environmental data); or have the reasonable potential \\nto be used in ways that are likely to expose individuals to meaningful harm, such as a loss of privacy or financial harm \\ndue to identity theft. Data and metadata generated by or about those who are not yet legal adults is also sensitive, even \\nif not related to a sensitive domain. Such data includes, but is not limited to, numerical, text, image, audio, or video \\ndata. “Sensitive domains” are those in which activities being conducted can cause material harms, including signifi\\xad\\ncant adverse effects on human rights such as autonomy and dignity, as well as civil liberties and civil rights. Domains \\nthat have historically been singled out as deserving of enhanced data protections or where such enhanced protections \\nare reasonably expected by the public include, but are not limited to, health, family planning and care, employment, \\neducation, criminal justice, and personal finance. In the context of this framework, such domains are considered \\nsensitive whether or not the specifics of a system context would necessitate coverage under existing law, and domains \\nand data that are considered sensitive are understood to change over time based on societal norms and context. \\n36\\n'),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 36, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content=' \\n \\n \\n \\n \\n \\n \\nDATA PRIVACY \\nEXTRA PROTECTIONS FOR DATA RELATED TO SENSITIVE\\nDOMAINS\\n•\\nContinuous positive airway pressure machines gather data for medical purposes, such as diagnosing sleep\\napnea, and send usage data to a patient’s insurance company, which may subsequently deny coverage for the\\ndevice based on usage data. Patients were not aware that the data would be used in this way or monitored\\nby anyone other than their doctor.70 \\n•\\nA department store company used predictive analytics applied to collected consumer data to determine that a\\nteenage girl was pregnant, and sent maternity clothing ads and other baby-related advertisements to her\\nhouse, revealing to her father that she was pregnant.71\\n•\\nSchool audio surveillance systems monitor student conversations to detect potential \"stress indicators\" as\\na warning of potential violence.72 Online proctoring systems claim to detect if a student is cheating on an\\nexam using biometric markers.73 These systems have the potential to limit student freedom to express a range\\nof emotions at school and may inappropriately flag students with disabilities who need accommodations or\\nuse screen readers or dictation software as cheating.74\\n•\\nLocation data, acquired from a data broker, can be used to identify people who visit abortion clinics.75\\n•\\nCompanies collect student data such as demographic information, free or reduced lunch status, whether\\nthey\\'ve used drugs, or whether they\\'ve expressed interest in LGBTQI+ groups, and then use that data to \\nforecast student success.76 Parents and education experts have expressed concern about collection of such\\nsensitive data without express parental consent, the lack of transparency in how such data is being used, and\\nthe potential for resulting discriminatory impacts.\\n• Many employers transfer employee data to third party job verification services. This information is then used\\nby potential future employers, banks, or landlords. In one case, a former employee alleged that a\\ncompany supplied false data about her job title which resulted in a job offer being revoked.77\\n37\\n'),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 37, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content=' \\n \\n \\n \\n \\n \\n \\nDATA PRIVACY \\nWHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS\\nThe expectations for automated systems are meant to serve as a blueprint for the development of additional \\ntechnical standards and practices that are tailored for particular sectors and contexts. \\xad\\xad\\xad\\xad\\xad\\xad\\nIn addition to the privacy expectations above for general non-sensitive data, any system collecting, using, shar-\\ning, or storing sensitive data should meet the expectations below. Depending on the technological use case and \\nbased on an ethical assessment, consent for sensitive data may need to be acquired from a guardian and/or child. \\nProvide enhanced protections for data related to sensitive domains \\nNecessary functions only. Sensitive data should only be used for functions strictly necessary for that \\ndomain or for functions that are required for administrative reasons (e.g., school attendance records), unless \\nconsent is acquired, if appropriate, and the additional expectations in this section are met. Consent for non-\\nnecessary functions should be optional, i.e., should not be required, incentivized, or coerced in order to \\nreceive opportunities or access to services. In cases where data is provided to an entity (e.g., health insurance \\ncompany) in order to facilitate payment for such a need, that data should only be used for that purpose. \\nEthical review and use prohibitions. Any use of sensitive data or decision process based in part on sensi-\\ntive data that might limit rights, opportunities, or access, whether the decision is automated or not, should go \\nthrough a thorough ethical review and monitoring, both in advance and by periodic review (e.g., via an indepen-\\ndent ethics committee or similarly robust process). In some cases, this ethical review may determine that data \\nshould not be used or shared for specific uses even with consent. Some novel uses of automated systems in this \\ncontext, where the algorithm is dynamically developing and where the science behind the use case is not well \\nestablished, may also count as human subject experimentation, and require special review under organizational \\ncompliance bodies applying medical, scientific, and academic human subject experimentation ethics rules and \\ngovernance procedures. \\nData quality. In sensitive domains, entities should be especially careful to maintain the quality of data to \\navoid adverse consequences arising from decision-making based on flawed or inaccurate data. Such care is \\nnecessary in a fragmented, complex data ecosystem and for datasets that have limited access such as for fraud \\nprevention and law enforcement. It should be not left solely to individuals to carry the burden of reviewing and \\ncorrecting data. Entities should conduct regular, independent audits and take prompt corrective measures to \\nmaintain accurate, timely, and complete data. \\nLimit access to sensitive data and derived data. Sensitive data and derived data should not be sold, \\nshared, or made public as part of data brokerage or other agreements. Sensitive data includes data that can be \\nused to infer sensitive information; even systems that are not directly marketed as sensitive domain technologies \\nare expected to keep sensitive data private. Access to such data should be limited based on necessity and based \\non a principle of local control, such that those individuals closest to the data subject have more access while \\nthose who are less proximate do not (e.g., a teacher has access to their students’ daily progress data while a \\nsuperintendent does not). \\nReporting. In addition to the reporting on data privacy (as listed above for non-sensitive data), entities devel-\\noping technologies related to a sensitive domain and those collecting, using, storing, or sharing sensitive data \\nshould, whenever appropriate, regularly provide public reports describing: any data security lapses or breaches \\nthat resulted in sensitive data leaks; the number, type, and outcomes of ethical pre-reviews undertaken; a \\ndescription of any data sold, shared, or made public, and how that data was assessed to determine it did not pres-\\nent a sensitive data risk; and ongoing risk identification and management procedures, and any mitigation added \\nbased on these procedures. Reporting should be provided in a clear and machine-readable manner. \\n38\\n'),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 38, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content=' \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nDATA PRIVACY \\nHOW THESE PRINCIPLES CAN MOVE INTO PRACTICE\\nReal-life examples of how these principles can become reality, through laws, policies, and practical \\ntechnical and sociotechnical approaches to protecting rights, opportunities, and access. \\nThe Privacy Act of 1974 requires privacy protections for personal information in federal \\nrecords systems, including limits on data retention, and also provides individuals a general \\nright to access and correct their data. Among other things, the Privacy Act limits the storage of individual \\ninformation in federal systems of records, illustrating the principle of limiting the scope of data retention. Under \\nthe Privacy Act, federal agencies may only retain data about an individual that is “relevant and necessary” to \\naccomplish an agency’s statutory purpose or to comply with an Executive Order of the President. The law allows \\nfor individuals to be able to access any of their individual information stored in a federal system of records, if not \\nincluded under one of the systems of records exempted pursuant to the Privacy Act. In these cases, federal agen\\xad\\ncies must provide a method for an individual to determine if their personal information is stored in a particular \\nsystem of records, and must provide procedures for an individual to contest the contents of a record about them. \\nFurther, the Privacy Act allows for a cause of action for an individual to seek legal relief if a federal agency does not \\ncomply with the Privacy Act’s requirements. Among other things, a court may order a federal agency to amend or \\ncorrect an individual’s information in its records or award monetary damages if an inaccurate, irrelevant, untimely, \\nor incomplete record results in an adverse determination about an individual’s “qualifications, character, rights, … \\nopportunities…, or benefits.” \\nNIST’s Privacy Framework provides a comprehensive, detailed and actionable approach for \\norganizations to manage privacy risks. The NIST Framework gives organizations ways to identify and \\ncommunicate their privacy risks and goals to support ethical decision-making in system, product, and service \\ndesign or deployment, as well as the measures they are taking to demonstrate compliance with applicable laws \\nor regulations. It has been voluntarily adopted by organizations across many different sectors around the world.78\\nA school board’s attempt to surveil public school students—undertaken without \\nadequate community input—sparked a state-wide biometrics moratorium.79 Reacting to a plan in \\nthe city of Lockport, New York, the state’s legislature banned the use of facial recognition systems and other \\n“biometric identifying technology” in schools until July 1, 2022.80 The law additionally requires that a report on \\nthe privacy, civil rights, and civil liberties implications of the use of such technologies be issued before \\nbiometric identification technologies can be used in New York schools. \\nFederal law requires employers, and any consultants they may retain, to report the costs \\nof surveilling employees in the context of a labor dispute, providing a transparency \\nmechanism to help protect worker organizing. Employers engaging in workplace surveillance \"where \\nan object there-of, directly or indirectly, is […] to obtain information concerning the activities of employees or a \\nlabor organization in connection with a labor dispute\" must report expenditures relating to this surveillance to \\nthe Department of Labor Office of Labor-Management Standards, and consultants who employers retain for \\nthese purposes must also file reports regarding their activities.81\\nPrivacy choices on smartphones show that when technologies are well designed, privacy \\nand data agency can be meaningful and not overwhelming. These choices—such as contextual, timely \\nalerts about location tracking—are brief, direct, and use-specific. Many of the expectations listed here for \\nprivacy by design and use-specific consent mirror those distributed to developers as best practices when \\ndeveloping for smart phone devices,82 such as being transparent about how user data will be used, asking for app \\npermissions during their use so that the use-context will be clear to users, and ensuring that the app will still \\nwork if users deny (or later revoke) some permissions. \\n39\\n'),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 39, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content=' \\nYou should know that an automated system is being used, \\nand understand how and why it contributes to outcomes \\nthat impact you. Designers, developers, and deployers of automat\\xad\\ned systems should provide generally accessible plain language docu\\xad\\nmentation including clear descriptions of the overall system func\\xad\\ntioning and the role automation plays, notice that such systems are in \\nuse, the individual or organization responsible for the system, and ex\\xad\\nplanations of outcomes that are clear, timely, and accessible. Such \\nnotice should be kept up-to-date and people impacted by the system \\nshould be notified of significant use case or key functionality chang\\xad\\nes. You should know how and why an outcome impacting you was de\\xad\\ntermined by an automated system, including when the automated \\nsystem is not the sole input determining the outcome. Automated \\nsystems should provide explanations that are technically valid, \\nmeaningful and useful to you and to any operators or others who \\nneed to understand the system, and calibrated to the level of risk \\nbased on the context. Reporting that includes summary information \\nabout these automated systems in plain language and assessments of \\nthe clarity and quality of the notice and explanations should be made \\npublic whenever possible.   \\nNOTICE AND EXPLANATION\\n40\\n'),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 40, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content=\" \\n \\n  \\n \\n \\nNOTICE & \\nEXPLANATION \\nWHY THIS PRINCIPLE IS IMPORTANT\\nThis section provides a brief summary of the problems which the principle seeks to address and protect \\nagainst, including illustrative examples. \\nAutomated systems now determine opportunities, from employment to credit, and directly shape the American \\npublic’s experiences, from the courtroom to online classrooms, in ways that profoundly impact people’s lives. But this \\nexpansive impact is not always visible. An applicant might not know whether a person rejected their resume or a \\nhiring algorithm moved them to the bottom of the list. A defendant in the courtroom might not know if a judge deny\\xad\\ning their bail is informed by an automated system that labeled them “high risk.” From correcting errors to contesting \\ndecisions, people are often denied the knowledge they need to address the impact of automated systems on their lives. \\nNotice and explanations also serve an important safety and efficacy purpose, allowing experts to verify the reasonable\\xad\\nness of a recommendation before enacting it. \\nIn order to guard against potential harms, the American public needs to know if an automated system is being used. \\nClear, brief, and understandable notice is a prerequisite for achieving the other protections in this framework. Like\\xad\\nwise, the public is often unable to ascertain how or why an automated system has made a decision or contributed to a \\nparticular outcome. The decision-making processes of automated systems tend to be opaque, complex, and, therefore, \\nunaccountable, whether by design or by omission. These factors can make explanations both more challenging and \\nmore important, and should not be used as a pretext to avoid explaining important decisions to the people impacted \\nby those choices. In the context of automated systems, clear and valid explanations should be recognized as a baseline \\nrequirement. \\nProviding notice has long been a standard practice, and in many cases is a legal requirement, when, for example, \\nmaking a video recording of someone (outside of a law enforcement or national security context). In some cases, such \\nas credit, lenders are required to provide notice and explanation to consumers. Techniques used to automate the \\nprocess of explaining such systems are under active research and improvement and such explanations can take many \\nforms. Innovative companies and researchers are rising to the challenge and creating and deploying explanatory \\nsystems that can help the public better understand decisions that impact them. \\nWhile notice and explanation requirements are already in place in some sectors or situations, the American public \\ndeserve to know consistently and across sectors if an automated system is being used in a way that impacts their rights, \\nopportunities, or access. This knowledge should provide confidence in how the public is being treated, and trust in the \\nvalidity and reasonable use of automated systems. \\n•\\nA lawyer representing an older client with disabilities who had been cut off from Medicaid-funded home\\nhealth-care assistance couldn't determine why, especially since the decision went against historical access\\npractices. In a court hearing, the lawyer learned from a witness that the state in which the older client\\nlived had recently adopted a new algorithm to determine eligibility.83 The lack of a timely explanation made it\\nharder to understand and contest the decision.\\n•\\nA formal child welfare investigation is opened against a parent based on an algorithm and without the parent\\never being notified that data was being collected and used as part of an algorithmic child maltreatment\\nrisk assessment.84 The lack of notice or an explanation makes it harder for those performing child\\nmaltreatment assessments to validate the risk assessment and denies parents knowledge that could help them\\ncontest a decision.\\n41\\n\"),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 41, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content=' \\n \\n  \\n \\n \\nNOTICE & \\nEXPLANATION \\nWHY THIS PRINCIPLE IS IMPORTANT\\nThis section provides a brief summary of the problems which the principle seeks to address and protect \\nagainst, including illustrative examples. \\n•\\nA predictive policing system claimed to identify individuals at greatest risk to commit or become the victim of\\ngun violence (based on automated analysis of social ties to gang members, criminal histories, previous experi\\xad\\nences of gun violence, and other factors) and led to individuals being placed on a watch list with no\\nexplanation or public transparency regarding how the system came to its conclusions.85 Both police and\\nthe public deserve to understand why and how such a system is making these determinations.\\n•\\nA system awarding benefits changed its criteria invisibly. Individuals were denied benefits due to data entry\\nerrors and other system flaws. These flaws were only revealed when an explanation of the system\\nwas demanded and produced.86 The lack of an explanation made it harder for errors to be corrected in a\\ntimely manner.\\n42\\n'),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 42, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content=\" \\n \\n \\n \\n \\n \\nNOTICE & \\nEXPLANATION \\nWHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS\\nThe expectations for automated systems are meant to serve as a blueprint for the development of additional \\ntechnical standards and practices that are tailored for particular sectors and contexts. \\nAn automated system should provide demonstrably clear, timely, understandable, and accessible notice of use, and \\nexplanations as to how and why a decision was made or an action was taken by the system. These expectations are \\nexplained below. \\nProvide clear, timely, understandable, and accessible notice of use and explanations \\xad\\nGenerally accessible plain language documentation. The entity responsible for using the automated \\nsystem should ensure that documentation describing the overall system (including any human components) is \\npublic and easy to find. The documentation should describe, in plain language, how the system works and how \\nany automated component is used to determine an action or decision. It should also include expectations about \\nreporting described throughout this framework, such as the algorithmic impact assessments described as \\npart of Algorithmic Discrimination Protections. \\nAccountable. Notices should clearly identify the entity responsible for designing each component of the \\nsystem and the entity using it. \\nTimely and up-to-date. Users should receive notice of the use of automated systems in advance of using or \\nwhile being impacted by the technology. An explanation should be available with the decision itself, or soon \\nthereafter. Notice should be kept up-to-date and people impacted by the system should be notified of use case \\nor key functionality changes. \\nBrief and clear. Notices and explanations should be assessed, such as by research on users’ experiences, \\nincluding user testing, to ensure that the people using or impacted by the automated system are able to easily \\nfind notices and explanations, read them quickly, and understand and act on them. This includes ensuring that \\nnotices and explanations are accessible to users with disabilities and are available in the language(s) and read-\\ning level appropriate for the audience. Notices and explanations may need to be available in multiple forms, \\n(e.g., on paper, on a physical sign, or online), in order to meet these expectations and to be accessible to the \\nAmerican public. \\nProvide explanations as to how and why a decision was made or an action was taken by an \\nautomated system \\nTailored to the purpose. Explanations should be tailored to the specific purpose for which the user is \\nexpected to use the explanation, and should clearly state that purpose. An informational explanation might \\ndiffer from an explanation provided to allow for the possibility of recourse, an appeal, or one provided in the \\ncontext of a dispute or contestation process. For the purposes of this framework, 'explanation' should be \\nconstrued broadly. An explanation need not be a plain-language statement about causality but could consist of \\nany mechanism that allows the recipient to build the necessary understanding and intuitions to achieve the \\nstated purpose. Tailoring should be assessed (e.g., via user experience research). \\nTailored to the target of the explanation. Explanations should be targeted to specific audiences and \\nclearly state that audience. An explanation provided to the subject of a decision might differ from one provided \\nto an advocate, or to a domain expert or decision maker. Tailoring should be assessed (e.g., via user experience \\nresearch). \\n43\\n\"),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 43, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content=\" \\n \\n \\n \\n \\n \\nNOTICE & \\nEXPLANATION \\nWHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS\\nThe expectations for automated systems are meant to serve as a blueprint for the development of additional \\ntechnical standards and practices that are tailored for particular sectors and contexts. \\nTailored to the level of risk. An assessment should be done to determine the level of risk of the auto\\xad\\nmated system. In settings where the consequences are high as determined by a risk assessment, or extensive \\noversight is expected (e.g., in criminal justice or some public sector settings), explanatory mechanisms should \\nbe built into the system design so that the system’s full behavior can be explained in advance (i.e., only fully \\ntransparent models should be used), rather than as an after-the-decision interpretation. In other settings, the \\nextent of explanation provided should be tailored to the risk level. \\nValid. The explanation provided by a system should accurately reflect the factors and the influences that led \\nto a particular decision, and should be meaningful for the particular customization based on purpose, target, \\nand level of risk. While approximation and simplification may be necessary for the system to succeed based on \\nthe explanatory purpose and target of the explanation, or to account for the risk of fraud or other concerns \\nrelated to revealing decision-making information, such simplifications should be done in a scientifically \\nsupportable way. Where appropriate based on the explanatory system, error ranges for the explanation should \\nbe calculated and included in the explanation, with the choice of presentation of such information balanced \\nwith usability and overall interface complexity concerns. \\nDemonstrate protections for notice and explanation \\nReporting. Summary reporting should document the determinations made based on the above consider\\xad\\nations, including: the responsible entities for accountability purposes; the goal and use cases for the system, \\nidentified users, and impacted populations; the assessment of notice clarity and timeliness; the assessment of \\nthe explanation's validity and accessibility; the assessment of the level of risk; and the account and assessment \\nof how explanations are tailored, including to the purpose, the recipient of the explanation, and the level of \\nrisk. Individualized profile information should be made readily available to the greatest extent possible that \\nincludes explanations for any system impacts or inferences. Reporting should be provided in a clear plain \\nlanguage and machine-readable manner. \\n44\\n\"),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 44, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content=' \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nNOTICE & \\nEXPLANATION \\nHOW THESE PRINCIPLES CAN MOVE INTO PRACTICE\\nReal-life examples of how these principles can become reality, through laws, policies, and practical \\ntechnical and sociotechnical approaches to protecting rights, opportunities, and access. \\xad\\xad\\xad\\xad\\xad\\nPeople in Illinois are given written notice by the private sector if their biometric informa-\\ntion is used. The Biometric Information Privacy Act enacted by the state contains a number of provisions \\nconcerning the use of individual biometric data and identifiers. Included among them is a provision that no private \\nentity may \"collect, capture, purchase, receive through trade, or otherwise obtain\" such information about an \\nindividual, unless written notice is provided to that individual or their legally appointed representative. 87\\nMajor technology companies are piloting new ways to communicate with the public about \\ntheir automated technologies. For example, a collection of non-profit organizations and companies have \\nworked together to develop a framework that defines operational approaches to transparency for machine \\nlearning systems.88 This framework, and others like it,89 inform the public about the use of these tools, going \\nbeyond simple notice to include reporting elements such as safety evaluations, disparity assessments, and \\nexplanations of how the systems work. \\nLenders are required by federal law to notify consumers about certain decisions made about \\nthem. Both the Fair Credit Reporting Act and the Equal Credit Opportunity Act require in certain circumstances \\nthat consumers who are denied credit receive \"adverse action\" notices. Anyone who relies on the information in a \\ncredit report to deny a consumer credit must, under the Fair Credit Reporting Act, provide an \"adverse action\" \\nnotice to the consumer, which includes \"notice of the reasons a creditor took adverse action on the application \\nor on an existing credit account.\"90 In addition, under the risk-based pricing rule,91 lenders must either inform \\nborrowers of their credit score, or else tell consumers when \"they are getting worse terms because of \\ninformation in their credit report.\" The CFPB has also asserted that \"[t]he law gives every applicant the right to \\na specific explanation if their application for credit was denied, and that right is not diminished simply because \\na company uses a complex algorithm that it doesn\\'t understand.\"92 Such explanations illustrate a shared value \\nthat certain decisions need to be explained. \\nA California law requires that warehouse employees are provided with notice and explana-\\ntion about quotas, potentially facilitated by automated systems, that apply to them. Warehous-\\ning employers in California that use quota systems (often facilitated by algorithmic monitoring systems) are \\nrequired to provide employees with a written description of each quota that applies to the employee, including \\n“quantified number of tasks to be performed or materials to be produced or handled, within the defined \\ntime period, and any potential adverse employment action that could result from failure to meet the quota.”93\\nAcross the federal government, agencies are conducting and supporting research on explain-\\nable AI systems. The NIST is conducting fundamental research on the explainability of AI systems. A multidis-\\nciplinary team of researchers aims to develop measurement methods and best practices to support the \\nimplementation of core tenets of explainable AI.94 The Defense Advanced Research Projects Agency has a \\nprogram on Explainable Artificial Intelligence that aims to create a suite of machine learning techniques that \\nproduce more explainable models, while maintaining a high level of learning performance (prediction \\naccuracy), and enable human users to understand, appropriately trust, and effectively manage the emerging \\ngeneration of artificially intelligent partners.95 The National Science Foundation’s program on Fairness in \\nArtificial Intelligence also includes a specific interest in research foundations for explainable AI.96\\n45\\n'),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 45, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content='You should be able to opt out, where appropriate, and \\nhave access to a person who can quickly consider and \\nremedy problems you encounter. You should be able to opt \\nout from automated systems in favor of a human alternative, where \\nappropriate. Appropriateness should be determined based on rea\\xad\\nsonable expectations in a given context and with a focus on ensuring \\nbroad accessibility and protecting the public from especially harm\\xad\\nful impacts. In some cases, a human or other alternative may be re\\xad\\nquired by law. You should have access to timely human consider\\xad\\nation and remedy by a fallback and escalation process if an automat\\xad\\ned system fails, it produces an error, or you would like to appeal or \\ncontest its impacts on you. Human consideration and fallback \\nshould be accessible, equitable, effective, maintained, accompanied \\nby appropriate operator training, and should not impose an unrea\\xad\\nsonable burden on the public. Automated systems with an intended \\nuse within sensitive domains, including, but not limited to, criminal \\njustice, employment, education, and health, should additionally be \\ntailored to the purpose, provide meaningful access for oversight, \\ninclude training for any people interacting with the system, and in\\xad\\ncorporate human consideration for adverse or high-risk decisions. \\nReporting that includes a description of these human governance \\nprocesses and assessment of their timeliness, accessibility, out\\xad\\ncomes, and effectiveness should be made public whenever possible. \\nHUMAN ALTERNATIVES, CONSIDERATION\\nALLBACK\\nF\\nAND\\n, \\n46\\n'),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 46, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content=' \\n \\n  \\nHUMAN ALTERNATIVES, \\nCONSIDERATION, AND \\nFALLBACK \\nWHY THIS PRINCIPLE IS IMPORTANT\\nThis section provides a brief summary of the problems which the principle seeks to address and protect \\nagainst, including illustrative examples. \\nThere are many reasons people may prefer not to use an automated system: the system can be flawed and can lead to \\nunintended outcomes; it may reinforce bias or be inaccessible; it may simply be inconvenient or unavailable; or it may \\nreplace a paper or manual process to which people had grown accustomed. Yet members of the public are often \\npresented with no alternative, or are forced to endure a cumbersome process to reach a human decision-maker once \\nthey decide they no longer want to deal exclusively with the automated system or be impacted by its results. As a result \\nof this lack of human reconsideration, many receive delayed access, or lose access, to rights, opportunities, benefits, \\nand critical services. The American public deserves the assurance that, when rights, opportunities, or access are \\nmeaningfully at stake and there is a reasonable expectation of an alternative to an automated system, they can conve\\xad\\nniently opt out of an automated system and will not be disadvantaged for that choice. In some cases, such a human or \\nother alternative may be required by law, for example it could be required as “reasonable accommodations” for people \\nwith disabilities. \\nIn addition to being able to opt out and use a human alternative, the American public deserves a human fallback \\nsystem in the event that an automated system fails or causes harm. No matter how rigorously an automated system is \\ntested, there will always be situations for which the system fails. The American public deserves protection via human \\nreview against these outlying or unexpected scenarios. In the case of time-critical systems, the public should not have \\nto wait—immediate human consideration and fallback should be available. In many time-critical systems, such a \\nremedy is already immediately available, such as a building manager who can open a door in the case an automated \\ncard access system fails. \\nIn the criminal justice system, employment, education, healthcare, and other sensitive domains, automated systems \\nare used for many purposes, from pre-trial risk assessments and parole decisions to technologies that help doctors \\ndiagnose disease. Absent appropriate safeguards, these technologies can lead to unfair, inaccurate, or dangerous \\noutcomes. These sensitive domains require extra protections. It is critically important that there is extensive human \\noversight in such settings. \\nThese critical protections have been adopted in some scenarios. Where automated systems have been introduced to \\nprovide the public access to government benefits, existing human paper and phone-based processes are generally still \\nin place, providing an important alternative to ensure access. Companies that have introduced automated call centers \\noften retain the option of dialing zero to reach an operator. When automated identity controls are in place to board an \\nairplane or enter the country, there is a person supervising the systems who can be turned to for help or to appeal a \\nmisidentification. \\nThe American people deserve the reassurance that such procedures are in place to protect their rights, opportunities, \\nand access. People make mistakes, and a human alternative or fallback mechanism will not always have the right \\nanswer, but they serve as an important check on the power and validity of automated systems. \\n• An automated signature matching system is used as part of the voting process in many parts of the country to\\ndetermine whether the signature on a mail-in ballot matches the signature on file. These signature matching\\nsystems are less likely to work correctly for some voters, including voters with mental or physical\\ndisabilities, voters with shorter or hyphenated names, and voters who have changed their name.97 A human\\ncuring process,98 which helps voters to confirm their signatures and correct other voting mistakes, is\\nimportant to ensure all votes are counted,99 and it is already standard practice in much of the country for\\nboth an election official and the voter to have the opportunity to review and correct any such issues.100 \\n47\\n'),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 47, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content=' \\n \\n  \\n \\nHUMAN ALTERNATIVES, \\nCONSIDERATION, AND \\nFALLBACK \\nWHY THIS PRINCIPLE IS IMPORTANT\\nThis section provides a brief summary of the problems which the principle seeks to address and protect \\nagainst, including illustrative examples. \\n•\\nAn unemployment benefits system in Colorado required, as a condition of accessing benefits, that applicants\\nhave a smartphone in order to verify their identity. No alternative human option was readily available,\\nwhich denied many people access to benefits.101\\n•\\nA fraud detection system for unemployment insurance distribution incorrectly flagged entries as fraudulent,\\nleading to people with slight discrepancies or complexities in their files having their wages withheld and tax\\nreturns seized without any chance to explain themselves or receive a review by a person.102\\n• A patient was wrongly denied access to pain medication when the hospital’s software confused her medica\\xad\\ntion history with that of her dog’s. Even after she tracked down an explanation for the problem, doctors\\nwere afraid to override the system, and she was forced to go without pain relief due to the system’s error.103\\n• A large corporation automated performance evaluation and other HR functions, leading to workers being\\nfired by an automated system without the possibility of human review, appeal or other form of recourse.104 \\n48\\n'),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 48, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content=' \\n \\n \\n \\n \\n \\nHUMAN ALTERNATIVES, \\nCONSIDERATION, AND \\nFALLBACK \\nWHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS\\nThe expectations for automated systems are meant to serve as a blueprint for the development of additional \\ntechnical standards and practices that are tailored for particular sectors and contexts. \\nAn automated system should provide demonstrably effective mechanisms to opt out in favor of a human alterna\\xad\\ntive, where appropriate, as well as timely human consideration and remedy by a fallback system, with additional \\nhuman oversight and safeguards for systems used in sensitive domains, and with training and assessment for any \\nhuman-based portions of the system to ensure effectiveness. \\nProvide a mechanism to conveniently opt out from automated systems in favor of a human \\nalternative, where appropriate \\nBrief, clear, accessible notice and instructions. Those impacted by an automated system should be \\ngiven a brief, clear notice that they are entitled to opt-out, along with clear instructions for how to opt-out. \\nInstructions should be provided in an accessible form and should be easily findable by those impacted by the \\nautomated system. The brevity, clarity, and accessibility of the notice and instructions should be assessed (e.g., \\nvia user experience research). \\nHuman alternatives provided when appropriate. In many scenarios, there is a reasonable expectation \\nof human involvement in attaining rights, opportunities, or access. When automated systems make up part of \\nthe attainment process, alternative timely human-driven processes should be provided. The use of a human \\nalternative should be triggered by an opt-out process. \\nTimely and not burdensome human alternative. Opting out should be timely and not unreasonably \\nburdensome in both the process of requesting to opt-out and the human-driven alternative provided. \\nProvide timely human consideration and remedy by a fallback and escalation system in the \\nevent that an automated system fails, produces error, or you would like to appeal or con\\xad\\ntest its impacts on you \\nProportionate. The availability of human consideration and fallback, along with associated training and \\nsafeguards against human bias, should be proportionate to the potential of the automated system to meaning\\xad\\nfully impact rights, opportunities, or access. Automated systems that have greater control over outcomes, \\nprovide input to high-stakes decisions, relate to sensitive domains, or otherwise have a greater potential to \\nmeaningfully impact rights, opportunities, or access should have greater availability (e.g., staffing) and over\\xad\\nsight of human consideration and fallback mechanisms. \\nAccessible. Mechanisms for human consideration and fallback, whether in-person, on paper, by phone, or \\notherwise provided, should be easy to find and use. These mechanisms should be tested to ensure that users \\nwho have trouble with the automated system are able to use human consideration and fallback, with the under\\xad\\nstanding that it may be these users who are most likely to need the human assistance. Similarly, it should be \\ntested to ensure that users with disabilities are able to find and use human consideration and fallback and also \\nrequest reasonable accommodations or modifications. \\nConvenient. Mechanisms for human consideration and fallback should not be unreasonably burdensome as \\ncompared to the automated system’s equivalent. \\n49\\n'),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 49, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content=\" \\n \\n \\n \\n \\n \\n \\nHUMAN ALTERNATIVES, \\nCONSIDERATION, AND \\nFALLBACK \\nWHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS\\nThe expectations for automated systems are meant to serve as a blueprint for the development of additional \\ntechnical standards and practices that are tailored for particular sectors and contexts. \\nEquitable. Consideration should be given to ensuring outcomes of the fallback and escalation system are \\nequitable when compared to those of the automated system and such that the fallback and escalation \\nsystem provides equitable access to underserved communities.105 \\nTimely. Human consideration and fallback are only useful if they are conducted and concluded in a \\ntimely manner. The determination of what is timely should be made relative to the specific automated \\nsystem, and the review system should be staffed and regularly assessed to ensure it is providing timely \\nconsideration and fallback. In time-critical systems, this mechanism should be immediately available or, \\nwhere possible, available before the harm occurs. Time-critical systems include, but are not limited to, \\nvoting-related systems, automated building access and other access systems, systems that form a critical \\ncomponent of healthcare, and systems that have the ability to withhold wages or otherwise cause \\nimmediate financial penalties. \\nEffective. The organizational structure surrounding processes for consideration and fallback should \\nbe designed so that if the human decision-maker charged with reassessing a decision determines that it \\nshould be overruled, the new decision will be effectively enacted. This includes ensuring that the new \\ndecision is entered into the automated system throughout its components, any previous repercussions from \\nthe old decision are also overturned, and safeguards are put in place to help ensure that future decisions do \\nnot result in the same errors. \\nMaintained. The human consideration and fallback process and any associated automated processes \\nshould be maintained and supported as long as the relevant automated system continues to be in use. \\nInstitute training, assessment, and oversight to combat automation bias and ensure any \\nhuman-based components of a system are effective. \\nTraining and assessment. Anyone administering, interacting with, or interpreting the outputs of an auto\\xad\\nmated system should receive training in that system, including how to properly interpret outputs of a system \\nin light of its intended purpose and in how to mitigate the effects of automation bias. The training should reoc\\xad\\ncur regularly to ensure it is up to date with the system and to ensure the system is used appropriately. Assess\\xad\\nment should be ongoing to ensure that the use of the system with human involvement provides for appropri\\xad\\nate results, i.e., that the involvement of people does not invalidate the system's assessment as safe and effective \\nor lead to algorithmic discrimination. \\nOversight. Human-based systems have the potential for bias, including automation bias, as well as other \\nconcerns that may limit their effectiveness. The results of assessments of the efficacy and potential bias of \\nsuch human-based systems should be overseen by governance structures that have the potential to update the \\noperation of the human-based system in order to mitigate these effects. \\n50\\n\"),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 50, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content=' \\n \\n \\n \\n \\n \\n \\nHUMAN ALTERNATIVES, \\nCONSIDERATION, AND \\nFALLBACK \\nWHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS\\nThe expectations for automated systems are meant to serve as a blueprint for the development of additional \\ntechnical standards and practices that are tailored for particular sectors and contexts. \\nImplement additional human oversight and safeguards for automated systems related to \\nsensitive domains \\nAutomated systems used within sensitive domains, including criminal justice, employment, education, and \\nhealth, should meet the expectations laid out throughout this framework, especially avoiding capricious, \\ninappropriate, and discriminatory impacts of these technologies. Additionally, automated systems used within \\nsensitive domains should meet these expectations: \\nNarrowly scoped data and inferences. Human oversight should ensure that automated systems in \\nsensitive domains are narrowly scoped to address a defined goal, justifying each included data item or attri\\xad\\nbute as relevant to the specific use case. Data included should be carefully limited to avoid algorithmic \\ndiscrimination resulting from, e.g., use of community characteristics, social network analysis, or group-based \\ninferences. \\nTailored to the situation. Human oversight should ensure that automated systems in sensitive domains \\nare tailored to the specific use case and real-world deployment scenario, and evaluation testing should show \\nthat the system is safe and effective for that specific situation. Validation testing performed based on one loca\\xad\\ntion or use case should not be assumed to transfer to another. \\nHuman consideration before any high-risk decision. Automated systems, where they are used in \\nsensitive domains, may play a role in directly providing information or otherwise providing positive outcomes \\nto impacted people. However, automated systems should not be allowed to directly intervene in high-risk \\nsituations, such as sentencing decisions or medical care, without human consideration. \\nMeaningful access to examine the system. Designers, developers, and deployers of automated \\nsystems should consider limited waivers of confidentiality (including those related to trade secrets) where \\nnecessary in order to provide meaningful oversight of systems used in sensitive domains, incorporating mea\\xad\\nsures to protect intellectual property and trade secrets from unwarranted disclosure as appropriate. This \\nincludes (potentially private and protected) meaningful access to source code, documentation, and related \\ndata during any associated legal discovery, subject to effective confidentiality or court orders. Such meaning\\xad\\nful access should include (but is not limited to) adhering to the principle on Notice and Explanation using the \\nhighest level of risk so the system is designed with built-in explanations; such systems should use fully-trans\\xad\\nparent models where the model itself can be understood by people needing to directly examine it. \\nDemonstrate access to human alternatives, consideration, and fallback \\nReporting. Reporting should include an assessment of timeliness and the extent of additional burden for \\nhuman alternatives, aggregate statistics about who chooses the human alternative, along with the results of \\nthe assessment about brevity, clarity, and accessibility of notice and opt-out instructions. Reporting on the \\naccessibility, timeliness, and effectiveness of human consideration and fallback should be made public at regu\\xad\\nlar intervals for as long as the system is in use. This should include aggregated information about the number \\nand type of requests for consideration, fallback employed, and any repeated requests; the timeliness of the \\nhandling of these requests, including mean wait times for different types of requests as well as maximum wait \\ntimes; and information about the procedures used to address requests for consideration along with the results \\nof the evaluation of their accessibility. For systems used in sensitive domains, reporting should include infor\\xad\\nmation about training and governance procedures for these technologies. Reporting should also include docu\\xad\\nmentation of goals and assessment of meeting those goals, consideration of data included, and documentation \\nof the governance of reasonable access to the technology. Reporting should be provided in a clear and \\nmachine-readable manner. \\n51\\n'),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 51, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content=\" \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nHUMAN ALTERNATIVES, \\nCONSIDERATION, AND \\nFALLBACK \\nHOW THESE PRINCIPLES CAN MOVE INTO PRACTICE\\nReal-life examples of how these principles can become reality, through laws, policies, and practical \\ntechnical and sociotechnical approaches to protecting rights, opportunities, and access. \\nHealthcare “navigators” help people find their way through online signup forms to choose \\nand obtain healthcare. A Navigator is “an individual or organization that's trained and able to help \\nconsumers, small businesses, and their employees as they look for health coverage options through the \\nMarketplace (a government web site), including completing eligibility and enrollment forms.”106 For \\nthe 2022 plan year, the Biden-Harris Administration increased funding so that grantee organizations could \\n“train and certify more than 1,500 Navigators to help uninsured consumers find affordable and comprehensive \\nhealth coverage.”107\\nThe customer service industry has successfully integrated automated services such as \\nchat-bots and AI-driven call response systems with escalation to a human support \\nteam.108 Many businesses now use partially automated customer service platforms that help answer customer \\nquestions and compile common problems for human agents to review. These integrated human-AI \\nsystems allow companies to provide faster customer care while maintaining human agents to answer \\ncalls or otherwise respond to complicated requests. Using both AI and human agents is viewed as key to \\nsuccessful customer service.109\\nBallot curing laws in at least 24 states require a fallback system that allows voters to \\ncorrect their ballot and have it counted in the case that a voter signature matching \\nalgorithm incorrectly flags their ballot as invalid or there is another issue with their \\nballot, and review by an election official does not rectify the problem. Some federal \\ncourts have found that such cure procedures are constitutionally required.110 \\nBallot \\ncuring processes vary among states, and include direct phone calls, emails, or mail contact by election \\nofficials.111 Voters are asked to provide alternative information or a new signature to verify the validity of their \\nballot. \\n52\\n\"),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 52, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content=' \\n \\n \\n \\n \\n \\n \\n \\n \\nAPPENDIX\\nExamples of Automated Systems \\nThe below examples are meant to illustrate the breadth of automated systems that, insofar as they have the \\npotential to meaningfully impact rights, opportunities, or access to critical resources or services, should \\nbe covered by the Blueprint for an AI Bill of Rights. These examples should not be construed to limit that \\nscope, which includes automated systems that may not yet exist, but which fall under these criteria. \\nExamples of automated systems for which the Blueprint for an AI Bill of Rights should be considered include \\nthose that have the potential to meaningfully impact: \\n• Civil rights, civil liberties, or privacy, including but not limited to:\\nSpeech-related systems such as automated content moderation tools; \\nSurveillance and criminal justice system algorithms such as risk assessments, predictive  \\n    policing, automated license plate readers, real-time facial recognition systems (especially  \\n    those used in public places or during protected activities like peaceful protests), social media  \\n    monitoring, and ankle monitoring devices; \\nVoting-related systems such as signature matching tools; \\nSystems with a potential privacy impact such as smart home systems and associated data,  \\n    systems that use or collect health-related data, systems that use or collect education-related  \\n    data, criminal justice system data, ad-targeting systems, and systems that perform big data  \\n    analytics in order to build profiles or infer personal information about individuals; and \\nAny system that has the meaningful potential to lead to algorithmic discrimination. \\n• Equal opportunities, including but not limited to:\\nEducation-related systems such as algorithms that purport to detect student cheating or  \\n    plagiarism, admissions algorithms, online or virtual reality student monitoring systems,  \\nprojections of student progress or outcomes, algorithms that determine access to resources or  \\n    rograms, and surveillance of classes (whether online or in-person); \\nHousing-related systems such as tenant screening algorithms, automated valuation systems that  \\n    estimate the value of homes used in mortgage underwriting or home insurance, and automated  \\n    valuations from online aggregator websites; and \\nEmployment-related systems such as workplace algorithms that inform all aspects of the terms  \\n    and conditions of employment including, but not limited to, pay or promotion, hiring or termina- \\n   tion algorithms, virtual or augmented reality workplace training programs, and electronic work \\nplace surveillance and management systems. \\n• Access to critical resources and services, including but not limited to:\\nHealth  and health insurance technologies such as medical AI systems and devices, AI-assisted \\n    diagnostic tools, algorithms or predictive models used to support clinical decision making, medical  \\n    or insurance health risk assessments, drug addiction risk assessments and associated access alg \\n-orithms, wearable technologies, wellness apps, insurance care allocation algorithms, and health\\ninsurance cost and underwriting algorithms;\\nFinancial system algorithms such as loan allocation algorithms, financial system access determi-\\nnation algorithms, credit scoring systems, insurance algorithms including risk assessments, auto\\n-mated interest rate determinations, and financial algorithms that apply penalties (e.g., that can\\ngarnish wages or withhold tax returns);\\n53\\n'),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 53, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content='APPENDIX\\nSystems that impact the safety of communities such as automated traffic control systems, elec \\n-ctrical grid controls, smart city technologies, and industrial emissions and environmental\\nimpact control algorithms; and\\nSystems related to access to benefits or services or assignment of penalties such as systems that\\nsupport decision-makers who adjudicate benefits such as collating or analyzing information or\\nmatching records, systems which similarly assist in the adjudication of administrative or criminal\\npenalties, fraud detection algorithms, services or benefits access control algorithms, biometric\\nsystems used as access control, and systems which make benefits or services related decisions on a\\nfully or partially autonomous basis (such as a determination to revoke benefits).\\n54\\n'),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 54, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content=' \\n \\n \\n \\n \\nSECTION TITLE\\nAPPENDIX\\nListening to the American People \\nThe White House Office of Science and Technology Policy (OSTP) led a yearlong process to seek and distill \\ninput from people across the country – from impacted communities to industry stakeholders to \\ntechnology developers to other experts across fields and sectors, as well as policymakers across the Federal \\ngovernment – on the issue of algorithmic and data-driven harms and potential remedies. Through panel \\ndiscussions, public listening sessions, private meetings, a formal request for information, and input to a \\npublicly accessible and widely-publicized email address, people across the United States spoke up about \\nboth the promises and potential harms of these technologies, and played a central role in shaping the \\nBlueprint for an AI Bill of Rights. \\nPanel Discussions to Inform the Blueprint for An AI Bill of Rights \\nOSTP co-hosted a series of six panel discussions in collaboration with the Center for American Progress, \\nthe Joint Center for Political and Economic Studies, New America, the German Marshall Fund, the Electronic \\nPrivacy Information Center, and the Mozilla Foundation. The purpose of these convenings – recordings of \\nwhich are publicly available online112 – was to bring together a variety of experts, practitioners, advocates \\nand federal government officials to offer insights and analysis on the risks, harms, benefits, and \\npolicy opportunities of automated systems. Each panel discussion was organized around a wide-ranging \\ntheme, exploring current challenges and concerns and considering what an automated society that \\nrespects democratic values should look like. These discussions focused on the topics of consumer \\nrights and protections, the criminal justice system, equal opportunities and civil justice, artificial \\nintelligence and democratic values, social welfare and development, and the healthcare system. \\nSummaries of Panel Discussions: \\nPanel 1: Consumer Rights and Protections. This event explored the opportunities and challenges for \\nindividual consumers and communities in the context of a growing ecosystem of AI-enabled consumer \\nproducts, advanced platforms and services, “Internet of Things” (IoT) devices, and smart city products and \\nservices. \\nWelcome:\\n•\\nRashida Richardson, Senior Policy Advisor for Data and Democracy, White House Office of Science and\\nTechnology Policy\\n•\\nKaren Kornbluh, Senior Fellow and Director of the Digital Innovation and Democracy Initiative, German\\nMarshall Fund\\nModerator: \\nDevin E. Willis, Attorney, Division of Privacy and Identity Protection, Bureau of Consumer Protection, Federal \\nTrade Commission \\nPanelists: \\n•\\nTamika L. Butler, Principal, Tamika L. Butler Consulting\\n•\\nJennifer Clark, Professor and Head of City and Regional Planning, Knowlton School of Engineering, Ohio\\nState University\\n•\\nCarl Holshouser, Senior Vice President for Operations and Strategic Initiatives, TechNet\\n•\\nSurya Mattu, Senior Data Engineer and Investigative Data Journalist, The Markup\\n•\\nMariah Montgomery, National Campaign Director, Partnership for Working Families\\n55\\n'),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 55, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content=\" \\n \\n \\n \\nAPPENDIX\\nPanelists discussed the benefits of AI-enabled systems and their potential to build better and more \\ninnovative infrastructure. They individually noted that while AI technologies may be new, the process of \\ntechnological diffusion is not, and that it was critical to have thoughtful and responsible development and \\nintegration of technology within communities. Some panelists suggested that the integration of technology \\ncould benefit from examining how technological diffusion has worked in the realm of urban planning: \\nlessons learned from successes and failures there include the importance of balancing ownership rights, use \\nrights, and community health, safety and welfare, as well ensuring better representation of all voices, \\nespecially those traditionally marginalized by technological advances. Some panelists also raised the issue of \\npower structures – providing examples of how strong transparency requirements in smart city projects \\nhelped to reshape power and give more voice to those lacking the financial or political power to effect change. \\nIn discussion of technical and governance interventions that that are needed to protect against the harms \\nof these technologies, various panelists emphasized the need for transparency, data collection, and \\nflexible and reactive policy development, analogous to how software is continuously updated and deployed. \\nSome panelists pointed out that companies need clear guidelines to have a consistent environment for \\ninnovation, with principles and guardrails being the key to fostering responsible innovation. \\nPanel 2: The Criminal Justice System. This event explored current and emergent uses of technology in \\nthe criminal justice system and considered how they advance or undermine public safety, justice, and \\ndemocratic values. \\nWelcome: \\n•\\nSuresh Venkatasubramanian, Assistant Director for Science and Justice, White House Office of Science\\nand Technology Policy\\n•\\nBen Winters, Counsel, Electronic Privacy Information Center\\nModerator: Chiraag Bains, Deputy Assistant to the President on Racial Justice & Equity \\nPanelists: \\n•\\nSean Malinowski, Director of Policing Innovation and Reform, University of Chicago Crime Lab\\n•\\nKristian Lum, Researcher\\n•\\nJumana Musa, Director, Fourth Amendment Center, National Association of Criminal Defense Lawyers\\n•\\nStanley Andrisse, Executive Director, From Prison Cells to PHD; Assistant Professor, Howard University\\nCollege of Medicine\\n•\\nMyaisha Hayes, Campaign Strategies Director, MediaJustice\\nPanelists discussed uses of technology within the criminal justice system, including the use of predictive \\npolicing, pretrial risk assessments, automated license plate readers, and prison communication tools. The \\ndiscussion emphasized that communities deserve safety, and strategies need to be identified that lead to safety; \\nsuch strategies might include data-driven approaches, but the focus on safety should be primary, and \\ntechnology may or may not be part of an effective set of mechanisms to achieve safety. Various panelists raised \\nconcerns about the validity of these systems, the tendency of adverse or irrelevant data to lead to a replication of \\nunjust outcomes, and the confirmation bias and tendency of people to defer to potentially inaccurate automated \\nsystems. Throughout, many of the panelists individually emphasized that the impact of these systems on \\nindividuals and communities is potentially severe: the systems lack individualization and work against the \\nbelief that people can change for the better, system use can lead to the loss of jobs and custody of children, and \\nsurveillance can lead to chilling effects for communities and sends negative signals to community members \\nabout how they're viewed. \\nIn discussion of technical and governance interventions that that are needed to protect against the harms of \\nthese technologies, various panelists emphasized that transparency is important but is not enough to achieve \\naccountability. Some panelists discussed their individual views on additional system needs for validity, and \\nagreed upon the importance of advisory boards and compensated community input early in the design process \\n(before the technology is built and instituted). Various panelists also emphasized the importance of regulation \\nthat includes limits to the type and cost of such technologies. \\n56\\n\"),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 56, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content=\"  \\n  \\nAPPENDIX\\nPanel 3: Equal Opportunities and Civil Justice. This event explored current and emerging uses of \\ntechnology that impact equity of opportunity in employment, education, and housing. \\nWelcome: \\n•\\nRashida Richardson, Senior Policy Advisor for Data and Democracy, White House Office of Science and\\nTechnology Policy\\n•\\nDominique Harrison, Director for Technology Policy, The Joint Center for Political and Economic\\nStudies\\nModerator: Jenny Yang, Director, Office of Federal Contract Compliance Programs, Department of Labor \\nPanelists: \\n•\\nChristo Wilson, Associate Professor of Computer Science, Northeastern University\\n•\\nFrida Polli, CEO, Pymetrics\\n•\\nKaren Levy, Assistant Professor, Department of Information Science, Cornell University\\n•\\nNatasha Duarte, Project Director, Upturn\\n•\\nElana Zeide, Assistant Professor, University of Nebraska College of Law\\n•\\nFabian Rogers, Constituent Advocate, Office of NY State Senator Jabari Brisport and Community\\nAdvocate and Floor Captain, Atlantic Plaza Towers Tenants Association\\nThe individual panelists described the ways in which AI systems and other technologies are increasingly being \\nused to limit access to equal opportunities in education, housing, and employment. Education-related \\nconcerning uses included the increased use of remote proctoring systems, student location and facial \\nrecognition tracking, teacher evaluation systems, robot teachers, and more. Housing-related concerning uses \\nincluding automated tenant background screening and facial recognition-based controls to enter or exit \\nhousing complexes. Employment-related concerning uses included discrimination in automated hiring \\nscreening and workplace surveillance. Various panelists raised the limitations of existing privacy law as a key \\nconcern, pointing out that students should be able to reinvent themselves and require privacy of their student \\nrecords and education-related data in order to do so. The overarching concerns of surveillance in these \\ndomains included concerns about the chilling effects of surveillance on student expression, inappropriate \\ncontrol of tenants via surveillance, and the way that surveillance of workers blurs the boundary between work \\nand life and exerts extreme and potentially damaging control over workers' lives. Additionally, some panelists \\npointed out ways that data from one situation was misapplied in another in a way that limited people's \\nopportunities, for example data from criminal justice settings or previous evictions being used to block further \\naccess to housing. Throughout, various panelists emphasized that these technologies are being used to shift the \\nburden of oversight and efficiency from employers to workers, schools to students, and landlords to tenants, in \\nways that diminish and encroach on equality of opportunity; assessment of these technologies should include \\nwhether they are genuinely helpful in solving an identified problem. \\nIn discussion of technical and governance interventions that that are needed to protect against the harms of \\nthese technologies, panelists individually described the importance of: receiving community input into the \\ndesign and use of technologies, public reporting on crucial elements of these systems, better notice and consent \\nprocedures that ensure privacy based on context and use case, ability to opt-out of using these systems and \\nreceive a fallback to a human process, providing explanations of decisions and how these systems work, the \\nneed for governance including training in using these systems, ensuring the technological use cases are \\ngenuinely related to the goal task and are locally validated to work, and the need for institution and protection \\nof third party audits to ensure systems continue to be accountable and valid. \\n57\\n\"),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 57, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content=\" \\n \\n \\n \\n \\nAPPENDIX\\nPanel 4: Artificial Intelligence and Democratic Values. This event examined challenges and opportunities in \\nthe design of technology that can help support a democratic vision for AI. It included discussion of the \\ntechnical aspects \\nof \\ndesigning \\nnon-discriminatory \\ntechnology, \\nexplainable \\nAI, \\nhuman-computer \\ninteraction with an emphasis on community participation, and privacy-aware design. \\nWelcome:\\n•\\nSorelle Friedler, Assistant Director for Data and Democracy, White House Office of Science and\\nTechnology Policy\\n•\\nJ. Bob Alotta, Vice President for Global Programs, Mozilla Foundation\\n•\\nNavrina Singh, Board Member, Mozilla Foundation\\nModerator: Kathy Pham Evans, Deputy Chief Technology Officer for Product and Engineering, U.S \\nFederal Trade Commission. \\nPanelists: \\n•\\nLiz O’Sullivan, CEO, Parity AI\\n•\\nTimnit Gebru, Independent Scholar\\n•\\nJennifer Wortman Vaughan, Senior Principal Researcher, Microsoft Research, New York City\\n•\\nPamela Wisniewski, Associate Professor of Computer Science, University of Central Florida; Director,\\nSocio-technical Interaction Research (STIR) Lab\\n•\\nSeny Kamara, Associate Professor of Computer Science, Brown University\\nEach panelist individually emphasized the risks of using AI in high-stakes settings, including the potential for \\nbiased data and discriminatory outcomes, opaque decision-making processes, and lack of public trust and \\nunderstanding of the algorithmic systems. The interventions and key needs various panelists put forward as \\nnecessary to the future design of critical AI systems included ongoing transparency, value sensitive and \\nparticipatory design, explanations designed for relevant stakeholders, and public consultation. \\nVarious \\npanelists emphasized the importance of placing trust in people, not technologies, and in engaging with \\nimpacted communities to understand the potential harms of technologies and build protection by design into \\nfuture systems. \\nPanel 5: Social Welfare and Development. This event explored current and emerging uses of technology to \\nimplement or improve social welfare systems, social development programs, and other systems that can impact \\nlife chances. \\nWelcome:\\n•\\nSuresh Venkatasubramanian, Assistant Director for Science and Justice, White House Office of Science\\nand Technology Policy\\n•\\nAnne-Marie Slaughter, CEO, New America\\nModerator: Michele Evermore, Deputy Director for Policy, Office of Unemployment Insurance \\nModernization, Office of the Secretary, Department of Labor \\nPanelists:\\n•\\nBlake Hall, CEO and Founder, ID.Me\\n•\\nKarrie Karahalios, Professor of Computer Science, University of Illinois, Urbana-Champaign\\n•\\nChristiaan van Veen, Director of Digital Welfare State and Human Rights Project, NYU School of Law's\\nCenter for Human Rights and Global Justice\\n58\\n\"),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 58, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content=\" \\n \\n \\n \\n \\nAPPENDIX\\n•\\nJulia Simon-Mishel, Supervising Attorney, Philadelphia Legal Assistance\\n•\\nDr. Zachary Mahafza, Research & Data Analyst, Southern Poverty Law Center\\n•\\nJ. Khadijah Abdurahman, Tech Impact Network Research Fellow, AI Now Institute, UCLA C2I1, and\\nUWA Law School\\nPanelists separately described the increasing scope of technology use in providing for social welfare, including \\nin fraud detection, digital ID systems, and other methods focused on improving efficiency and reducing cost. \\nHowever, various panelists individually cautioned that these systems may reduce burden for government \\nagencies by increasing the burden and agency of people using and interacting with these technologies. \\nAdditionally, these systems can produce feedback loops and compounded harm, collecting data from \\ncommunities and using it to reinforce inequality. Various panelists suggested that these harms could be \\nmitigated by ensuring community input at the beginning of the design process, providing ways to opt out of \\nthese systems and use associated human-driven mechanisms instead, ensuring timeliness of benefit payments, \\nand providing clear notice about the use of these systems and clear explanations of how and what the \\ntechnologies are doing. Some panelists suggested that technology should be used to help people receive \\nbenefits, e.g., by pushing benefits to those in need and ensuring automated decision-making systems are only \\nused to provide a positive outcome; technology shouldn't be used to take supports away from people who need \\nthem. \\nPanel 6: The Healthcare System. This event explored current and emerging uses of technology in the \\nhealthcare system and consumer products related to health. \\nWelcome:\\n•\\nAlondra Nelson, Deputy Director for Science and Society, White House Office of Science and Technology\\nPolicy\\n•\\nPatrick Gaspard, President and CEO, Center for American Progress\\nModerator: Micky Tripathi, National Coordinator for Health Information Technology, U.S Department of \\nHealth and Human Services. \\nPanelists: \\n•\\nMark Schneider, Health Innovation Advisor, ChristianaCare\\n•\\nZiad Obermeyer, Blue Cross of California Distinguished Associate Professor of Policy and Management,\\nUniversity of California, Berkeley School of Public Health\\n•\\nDorothy Roberts, George A. Weiss University Professor of Law and Sociology and the Raymond Pace and\\nSadie Tanner Mossell Alexander Professor of Civil Rights, University of Pennsylvania\\n•\\nDavid Jones, A. Bernard Ackerman Professor of the Culture of Medicine, Harvard University\\n•\\nJamila Michener, Associate Professor of Government, Cornell University; Co-Director, Cornell Center for\\nHealth Equity\\xad\\nPanelists discussed the impact of new technologies on health disparities; healthcare access, delivery, and \\noutcomes; and areas ripe for research and policymaking. Panelists discussed the increasing importance of tech-\\nnology as both a vehicle to deliver healthcare and a tool to enhance the quality of care. On the issue of \\ndelivery, various panelists pointed to a number of concerns including access to and expense of broadband \\nservice, the privacy concerns associated with telehealth systems, the expense associated with health \\nmonitoring devices, and how this can exacerbate equity issues.  On the issue of technology enhanced care, \\nsome panelists spoke extensively about the way in which racial biases and the use of race in medicine \\nperpetuate harms and embed prior discrimination, and the importance of ensuring that the technologies used \\nin medical care were accountable to the relevant stakeholders. Various panelists emphasized the importance \\nof having the voices of those subjected to these technologies be heard.\\n59\\n\"),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 59, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content='APPENDIX\\nSummaries of Additional Engagements: \\n• OSTP created an email address (ai-equity@ostp.eop.gov) to solicit comments from the public on the use of\\nartificial intelligence and other data-driven technologies in their lives.\\n• OSTP issued a Request For Information (RFI) on the use and governance of biometric technologies.113 The\\npurpose of this RFI was to understand the extent and variety of biometric technologies in past, current, or\\nplanned use; the domains in which these technologies are being used; the entities making use of them; current\\nprinciples, practices, or policies governing their use; and the stakeholders that are, or may be, impacted by their\\nuse or regulation. The 130 responses to this RFI are available in full online114 and were submitted by the below\\nlisted organizations and individuals:\\nAccenture \\nAccess Now \\nACT | The App Association \\nAHIP \\nAIethicist.org \\nAirlines for America \\nAlliance for Automotive Innovation \\nAmelia Winger-Bearskin \\nAmerican Civil Liberties Union \\nAmerican Civil Liberties Union of \\nMassachusetts \\nAmerican Medical Association \\nARTICLE19 \\nAttorneys General of the District of \\nColumbia, Illinois, Maryland, \\nMichigan, Minnesota, New York, \\nNorth Carolina, Oregon, Vermont, \\nand Washington \\nAvanade \\nAware \\nBarbara Evans \\nBetter Identity Coalition \\nBipartisan Policy Center \\nBrandon L. Garrett and Cynthia \\nRudin \\nBrian Krupp \\nBrooklyn Defender Services \\nBSA | The Software Alliance \\nCarnegie Mellon University \\nCenter for Democracy & \\nTechnology \\nCenter for New Democratic \\nProcesses \\nCenter for Research and Education \\non Accessible Technology and \\nExperiences at University of \\nWashington, Devva Kasnitz, L Jean \\nCamp, Jonathan Lazar, Harry \\nHochheiser \\nCenter on Privacy & Technology at \\nGeorgetown Law \\nCisco Systems \\nCity of Portland Smart City PDX \\nProgram \\nCLEAR \\nClearview AI \\nCognoa \\nColor of Change \\nCommon Sense Media \\nComputing Community Consortium \\nat Computing Research Association \\nConnected Health Initiative \\nConsumer Technology Association \\nCourtney Radsch \\nCoworker \\nCyber Farm Labs \\nData & Society Research Institute \\nData for Black Lives \\nData to Actionable Knowledge Lab \\nat Harvard University \\nDeloitte \\nDev Technology Group \\nDigital Therapeutics Alliance \\nDigital Welfare State & Human \\nRights Project and Center for \\nHuman Rights and Global Justice at \\nNew York University School of \\nLaw, and Temple University \\nInstitute for Law, Innovation & \\nTechnology \\nDignari \\nDouglas Goddard \\nEdgar Dworsky \\nElectronic Frontier Foundation \\nElectronic Privacy Information \\nCenter, Center for Digital \\nDemocracy, and Consumer \\nFederation of America \\nFaceTec \\nFight for the Future \\nGanesh Mani \\nGeorgia Tech Research Institute \\nGoogle \\nHealth Information Technology \\nResearch and Development \\nInteragency Working Group \\nHireVue \\nHR Policy Association \\nID.me \\nIdentity and Data Sciences \\nLaboratory at Science Applications \\nInternational Corporation \\nInformation Technology and \\nInnovation Foundation \\nInformation Technology Industry \\nCouncil \\nInnocence Project \\nInstitute for Human-Centered \\nArtificial Intelligence at Stanford \\nUniversity \\nIntegrated Justice Information \\nSystems Institute \\nInternational Association of Chiefs \\nof Police \\nInternational Biometrics + Identity \\nAssociation \\nInternational Business Machines \\nCorporation \\nInternational Committee of the Red \\nCross \\nInventionphysics \\niProov \\nJacob Boudreau \\nJennifer K. Wagner, Dan Berger, \\nMargaret Hu, and Sara Katsanis \\nJonathan Barry-Blocker \\nJoseph Turow \\nJoy Buolamwini \\nJoy Mack \\nKaren Bureau \\nLamont Gholston \\nLawyers’ Committee for Civil \\nRights Under Law \\n60\\n'),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 60, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content='APPENDIX\\nLisa Feldman Barrett \\nMadeline Owens \\nMarsha Tudor \\nMicrosoft Corporation \\nMITRE Corporation \\nNational Association for the \\nAdvancement of Colored People \\nLegal Defense and Educational \\nFund \\nNational Association of Criminal \\nDefense Lawyers \\nNational Center for Missing & \\nExploited Children \\nNational Fair Housing Alliance \\nNational Immigration Law Center \\nNEC Corporation of America \\nNew America’s Open Technology \\nInstitute \\nNew York Civil Liberties Union \\nNo Name Provided \\nNotre Dame Technology Ethics \\nCenter \\nOffice of the Ohio Public Defender \\nOnfido \\nOosto \\nOrissa Rose \\nPalantir \\nPangiam \\nParity Technologies \\nPatrick A. Stewart, Jeffrey K. \\nMullins, and Thomas J. Greitens \\nPel Abbott \\nPhiladelphia Unemployment \\nProject \\nProject On Government Oversight \\nRecording Industry Association of \\nAmerica \\nRobert Wilkens \\nRon Hedges \\nScience, Technology, and Public \\nPolicy Program at University of \\nMichigan Ann Arbor \\nSecurity Industry Association \\nSheila Dean \\nSoftware & Information Industry \\nAssociation \\nStephanie Dinkins and the Future \\nHistories Studio at Stony Brook \\nUniversity \\nTechNet \\nThe Alliance for Media Arts and \\nCulture, MIT Open Documentary \\nLab and Co-Creation Studio, and \\nImmerse \\nThe International Brotherhood of \\nTeamsters \\nThe Leadership Conference on \\nCivil and Human Rights \\nThorn \\nU.S. Chamber of Commerce’s \\nTechnology Engagement Center \\nUber Technologies \\nUniversity of Pittsburgh \\nUndergraduate Student \\nCollaborative \\nUpturn \\nUS Technology Policy Committee \\nof the Association of Computing \\nMachinery \\nVirginia Puccio \\nVisar Berisha and Julie Liss \\nXR Association \\nXR Safety Initiative \\n• As an additional effort to reach out to stakeholders regarding the RFI, OSTP conducted two listening sessions\\nfor members of the public. The listening sessions together drew upwards of 300 participants. The Science and\\nTechnology Policy Institute produced a synopsis of both the RFI submissions and the feedback at the listening\\nsessions.115\\n61\\n'),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 61, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content=\"APPENDIX\\n• OSTP conducted meetings with a variety of stakeholders in the private sector and civil society. Some of these\\nmeetings were specifically focused on providing ideas related to the development of the Blueprint for an AI\\nBill of Rights while others provided useful general context on the positive use cases, potential harms, and/or\\noversight possibilities for these technologies. Participants in these conversations from the private sector and\\ncivil society included:\\nAdobe \\nAmerican Civil Liberties Union \\n(ACLU) \\nThe Aspen Commission on \\nInformation Disorder \\nThe Awood Center \\nThe Australian Human Rights \\nCommission \\nBiometrics Institute \\nThe Brookings Institute \\nBSA | The Software Alliance \\nCantellus Group \\nCenter for American Progress \\nCenter for Democracy and \\nTechnology \\nCenter on Privacy and Technology \\nat Georgetown Law \\nChristiana Care \\nColor of Change \\nCoworker \\nData Robot \\nData Trust Alliance \\nData and Society Research Institute \\nDeepmind \\nEdSAFE AI Alliance \\nElectronic Privacy Information \\nCenter (EPIC) \\nEncode Justice \\nEqual AI \\nGoogle \\nHitachi's AI Policy Committee \\nThe Innocence Project \\nInstitute of Electrical and \\nElectronics Engineers (IEEE) \\nIntuit \\nLawyers Committee for Civil Rights \\nUnder Law \\nLegal Aid Society \\nThe Leadership Conference on \\nCivil and Human Rights \\nMeta \\nMicrosoft \\nThe MIT AI Policy Forum \\nMovement Alliance Project \\nThe National Association of \\nCriminal Defense Lawyers \\nO’Neil Risk Consulting & \\nAlgorithmic Auditing \\nThe Partnership on AI \\nPinterest \\nThe Plaintext Group \\npymetrics \\nSAP \\nThe Security Industry Association \\nSoftware and Information Industry \\nAssociation (SIIA) \\nSpecial Competitive Studies Project \\nThorn \\nUnited for Respect \\nUniversity of California at Berkeley \\nCitris Policy Lab \\nUniversity of California at Berkeley \\nLabor Center \\nUnfinished/Project Liberty \\nUpturn \\nUS Chamber of Commerce \\nUS Chamber of Commerce \\nTechnology Engagement Center \\nA.I. Working Group\\nVibrent Health\\nWarehouse Worker Resource\\nCenter\\nWaymap\\n62\\n\"),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 62, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content=\" \\n \\n \\n \\nENDNOTES\\n1.The Executive Order On Advancing Racial Equity and Support for Underserved Communities Through the\\nFederal\\xa0Government. https://www.whitehouse.gov/briefing-room/presidential-actions/2021/01/20/executive\\norder-advancing-racial-equity-and-support-for-underserved-communities-through-the-federal-government/\\n2. The White House. Remarks by President Biden on the Supreme Court Decision to Overturn Roe v. Wade. Jun.\\n24, 2022. https://www.whitehouse.gov/briefing-room/speeches-remarks/2022/06/24/remarks-by-president\\xad\\nbiden-on-the-supreme-court-decision-to-overturn-roe-v-wade/\\n3. The White House. Join the Effort to Create A Bill of Rights for an Automated Society. Nov. 10, 2021. https://\\nwww.whitehouse.gov/ostp/news-updates/2021/11/10/join-the-effort-to-create-a-bill-of-rights-for-an\\xad\\nautomated-society/\\n4. U.S. Dept. of Health, Educ. & Welfare, Report of the Sec’y’s Advisory Comm. on Automated Pers. Data Sys.,\\nRecords, Computers, and the Rights of Citizens (July 1973). https://www.justice.gov/opcl/docs/rec-com\\xad\\nrights.pdf.\\n5. See, e.g., Office of Mgmt. & Budget, Exec. Office of the President, Circular A-130, Managing Information as a\\nStrategic Resource, app. II §\\xa03 (July 28, 2016); Org. of Econ. Co-Operation & Dev., Revision of the\\nRecommendation of the Council Concerning Guidelines Governing the Protection of Privacy and Transborder\\nFlows of Personal Data, Annex Part Two (June 20, 2013). https://one.oecd.org/document/C(2013)79/en/pdf.\\n6. Andrew Wong et al. External validation of a widely implemented proprietary sepsis prediction model in\\nhospitalized patients. JAMA Intern Med. 2021; 181(8):1065-1070. doi:10.1001/jamainternmed.2021.2626\\n7. Jessica Guynn. Facebook while black: Users call it getting 'Zucked,' say talking about racism is censored as hate\\nspeech. USA Today. Apr. 24, 2019. https://www.usatoday.com/story/news/2019/04/24/facebook-while-black\\xad\\nzucked-users-say-they-get-blocked-racism-discussion/2859593002/\\n8. See, e.g., Michael Levitt. AirTags are being used to track people and cars. Here's what is being done about it.\\nNPR. Feb. 18, 2022. https://www.npr.org/2022/02/18/1080944193/apple-airtags-theft-stalking-privacy-tech;\\nSamantha Cole. Police Records Show Women Are Being Stalked With Apple AirTags Across the Country.\\nMotherboard. Apr. 6, 2022. https://www.vice.com/en/article/y3vj3y/apple-airtags-police-reports-stalking\\xad\\nharassment\\n9. Kristian Lum and William Isaac. To Predict and Serve? Significance. Vol. 13, No. 5, p. 14-19. Oct. 7, 2016.\\nhttps://rss.onlinelibrary.wiley.com/doi/full/10.1111/j.1740-9713.2016.00960.x; Aaron Sankin, Dhruv Mehrotra,\\nSurya Mattu, and Annie Gilbertson. Crime Prediction Software Promised to Be Free of Biases. New Data Shows\\nIt Perpetuates Them. The Markup and Gizmodo. Dec. 2, 2021. https://themarkup.org/prediction\\xad\\nbias/2021/12/02/crime-prediction-software-promised-to-be-free-of-biases-new-data-shows-it-perpetuates\\xad\\nthem\\n10. Samantha Cole. This Horrifying App Undresses a Photo of Any Woman With a Single Click. Motherboard.\\nJune 26, 2019. https://www.vice.com/en/article/kzm59x/deepnude-app-creates-fake-nudes-of-any-woman\\n11. Lauren Kaori Gurley. Amazon’s AI Cameras Are Punishing Drivers for Mistakes They Didn’t Make.\\nMotherboard. Sep. 20, 2021. https://www.vice.com/en/article/88npjv/amazons-ai-cameras-are-punishing\\xad\\ndrivers-for-mistakes-they-didnt-make\\n63\\n\"),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 63, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content=' \\nENDNOTES\\n12. Expectations about reporting are intended for the entity developing or using the automated system. The\\nresulting reports can be provided to the public, regulators, auditors, industry standards groups, or others\\nengaged in independent review, and should be made public as much as possible consistent with law,\\nregulation, and policy, and noting that intellectual property or law enforcement considerations may prevent\\npublic release. These reporting expectations are important for transparency, so the American people can\\nhave confidence that their rights, opportunities, and access as well as their expectations around\\ntechnologies are respected.\\n13. National Artificial Intelligence Initiative Office. Agency Inventories of AI Use Cases. Accessed Sept. 8,\\n2022. https://www.ai.gov/ai-use-case-inventories/\\n14. National Highway Traffic Safety Administration. https://www.nhtsa.gov/\\n15. See, e.g., Charles Pruitt. People Doing What They Do Best: The Professional Engineers and NHTSA. Public\\nAdministration Review. Vol. 39, No. 4. Jul.-Aug., 1979. https://www.jstor.org/stable/976213?seq=1\\n16. The US Department of Transportation has publicly described the health and other benefits of these\\n“traffic calming” measures. See, e.g.: U.S. Department of Transportation. Traffic Calming to Slow Vehicle\\nSpeeds. Accessed Apr. 17, 2022. https://www.transportation.gov/mission/health/Traffic-Calming-to-Slow\\xad\\nVehicle-Speeds\\n17. Karen Hao. Worried about your firm’s AI ethics? These startups are here to help.\\nA growing ecosystem of “responsible AI” ventures promise to help organizations monitor and fix their AI\\nmodels. MIT Technology Review. Jan 15., 2021.\\nhttps://www.technologyreview.com/2021/01/15/1016183/ai-ethics-startups/; Disha Sinha. Top Progressive\\nCompanies Building Ethical AI to Look Out for in 2021. Analytics Insight. June 30, 2021. https://\\nwww.analyticsinsight.net/top-progressive-companies-building-ethical-ai-to-look-out-for\\xad\\nin-2021/ https://www.technologyreview.com/2021/01/15/1016183/ai-ethics-startups/; Disha Sinha. Top\\nProgressive Companies Building Ethical AI to Look Out for in 2021. Analytics Insight. June 30, 2021.\\n18. Office of Management and Budget. Study to Identify Methods to Assess Equity: Report to the President.\\nAug. 2021. https://www.whitehouse.gov/wp-content/uploads/2021/08/OMB-Report-on-E013985\\xad\\nImplementation_508-Compliant-Secure-v1.1.pdf\\n19. National Institute of Standards and Technology. AI Risk Management Framework. Accessed May 23,\\n2022. https://www.nist.gov/itl/ai-risk-management-framework\\n20. U.S. Department of Energy. U.S. Department of Energy Establishes Artificial Intelligence Advancement\\nCouncil. U.S. Department of Energy Artificial Intelligence and Technology Office. April 18, 2022. https://\\nwww.energy.gov/ai/articles/us-department-energy-establishes-artificial-intelligence-advancement-council\\n21. Department of Defense. U.S Department of Defense Responsible Artificial Intelligence Strategy and\\nImplementation Pathway. Jun. 2022. https://media.defense.gov/2022/Jun/22/2003022604/-1/-1/0/\\nDepartment-of-Defense-Responsible-Artificial-Intelligence-Strategy-and-Implementation\\xad\\nPathway.PDF\\n22. Director of National Intelligence. Principles of Artificial Intelligence Ethics for the Intelligence\\nCommunity. https://www.dni.gov/index.php/features/2763-principles-of-artificial-intelligence-ethics-for\\xad\\nthe-intelligence-community\\n64\\n'),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 64, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content=\" \\nENDNOTES\\n23. National Science Foundation. National Artificial Intelligence Research Institutes. Accessed Sept. 12,\\n2022. https://beta.nsf.gov/funding/opportunities/national-artificial-intelligence-research-institutes\\n24. National Science Foundation. Cyber-Physical Systems. Accessed Sept. 12, 2022. https://beta.nsf.gov/\\nfunding/opportunities/cyber-physical-systems-cps\\n25. National Science Foundation. Secure and Trustworthy Cyberspace. Accessed Sept. 12, 2022. https://\\nbeta.nsf.gov/funding/opportunities/secure-and-trustworthy-cyberspace-satc\\n26. National Science Foundation. Formal Methods in the Field. Accessed Sept. 12, 2022. https://\\nbeta.nsf.gov/funding/opportunities/formal-methods-field-fmitf\\n27. National Science Foundation. Designing Accountable Software Systems. Accessed Sept. 12, 2022.\\nhttps://beta.nsf.gov/funding/opportunities/designing-accountable-software-systems-dass\\n28. The Leadership Conference Education Fund. The Use Of Pretrial “Risk Assessment” Instruments: A\\nShared Statement Of Civil Rights Concerns. Jul. 30, 2018. http://civilrightsdocs.info/pdf/criminal-justice/\\nPretrial-Risk-Assessment-Short.pdf; https://civilrights.org/edfund/pretrial-risk-assessments/\\n29. Idaho Legislature. House Bill 118. Jul. 1, 2019. https://legislature.idaho.gov/sessioninfo/2019/\\nlegislation/H0118/\\n30. See, e.g., Executive Office of the President. Big Data: A Report on Algorithmic Systems, Opportunity, and\\nCivil Rights. May, 2016. https://obamawhitehouse.archives.gov/sites/default/files/microsites/\\nostp/2016_0504_data_discrimination.pdf; Cathy O’Neil. Weapons of Math Destruction. Penguin Books.\\n2017. https://en.wikipedia.org/wiki/Weapons_of_Math_Destruction; Ruha Benjamin. Race After\\nTechnology: Abolitionist Tools for the New Jim Code. Polity. 2019. https://www.ruhabenjamin.com/race\\xad\\nafter-technology\\n31. See, e.g., Kashmir Hill. Another Arrest, and Jail Time, Due to a Bad Facial Recognition Match: A New\\nJersey man was accused of shoplifting and trying to hit an officer with a car. He is the third known Black man\\nto be wrongfully arrested based on face recognition. New York Times. Dec. 29, 2020, updated Jan. 6, 2021.\\nhttps://www.nytimes.com/2020/12/29/technology/facial-recognition-misidentify-jail.html; Khari\\nJohnson. How Wrongful Arrests Based on AI Derailed 3 Men's Lives. Wired. Mar. 7, 2022. https://\\nwww.wired.com/story/wrongful-arrests-ai-derailed-3-mens-lives/\\n32. Student Borrower Protection Center. Educational Redlining. Student Borrower Protection Center\\nReport. Feb. 2020. https://protectborrowers.org/wp-content/uploads/2020/02/Education-Redlining\\xad\\nReport.pdf\\n33. Jeffrey Dastin. Amazon scraps secret AI recruiting tool that showed bias against women. Reuters. Oct.\\n10, 2018. https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps\\xad\\nsecret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G\\n34. Todd Feathers. Major Universities Are Using Race as a “High Impact Predictor” of Student Success:\\nStudents, professors, and education experts worry that that’s pushing Black students in particular out of math\\nand science. The Markup. Mar. 2, 2021. https://themarkup.org/machine-learning/2021/03/02/major\\xad\\nuniversities-are-using-race-as-a-high-impact-predictor-of-student-success\\n65\\n\"),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 65, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content=\" \\n \\nENDNOTES\\n35. Carrie Johnson. Flaws plague a tool meant to help low-risk federal prisoners win early release. NPR.\\nJan. 26, 2022. https://www.npr.org/2022/01/26/1075509175/flaws-plague-a-tool-meant-to-help-low\\xad\\nrisk-federal-prisoners-win-early-release.; Carrie Johnson. Justice Department works to curb racial bias\\nin deciding who's released from prison. NPR. Apr. 19, 2022. https://\\nwww.npr.org/2022/04/19/1093538706/justice-department-works-to-curb-racial-bias-in-deciding\\xad\\nwhos-released-from-pris; National Institute of Justice. 2021 Review and Revalidation of the First Step Act\\nRisk Assessment Tool. National Institute of Justice NCJ 303859. Dec., 2021. https://www.ojp.gov/\\npdffiles1/nij/303859.pdf\\n36. Andrew Thompson. Google’s Sentiment Analyzer Thinks Being Gay Is Bad. Vice. Oct. 25, 2017. https://\\nwww.vice.com/en/article/j5jmj8/google-artificial-intelligence-bias\\n37. Kaggle. Jigsaw Unintended Bias in Toxicity Classification: Detect toxicity across a diverse range of\\nconversations. 2019. https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification\\n38. Lucas Dixon, John Li, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman. Measuring and\\nMitigating Unintended Bias in Text Classification. Proceedings of AAAI/ACM Conference on AI, Ethics,\\nand Society. Feb. 2-3, 2018. https://dl.acm.org/doi/pdf/10.1145/3278721.3278729\\n39. Paresh Dave. Google cuts racy results by 30% for searches like 'Latina teenager'. Reuters. Mar. 30,\\n2022. https://www.reuters.com/technology/google-cuts-racy-results-by-30-searches-like-latina\\xad\\nteenager-2022-03-30/\\n40. Safiya Umoja Noble. Algorithms of Oppression: How Search Engines Reinforce Racism. NYU Press.\\nFeb. 2018. https://nyupress.org/9781479837243/algorithms-of-oppression/\\n41. Paresh Dave. Google cuts racy results by 30% for searches like 'Latina teenager'. Reuters. Mar. 30,\\n2022. https://www.reuters.com/technology/google-cuts-racy-results-by-30-searches-like-latina\\xad\\nteenager-2022-03-30/\\n42. Miranda Bogen. All the Ways Hiring Algorithms Can Introduce Bias. Harvard Business Review. May\\n6, 2019. https://hbr.org/2019/05/all-the-ways-hiring-algorithms-can-introduce-bias\\n43. Arli Christian. Four Ways the TSA Is Making Flying Easier for Transgender People. American Civil\\nLiberties Union. Apr. 5, 2022. https://www.aclu.org/news/lgbtq-rights/four-ways-the-tsa-is-making\\xad\\nflying-easier-for-transgender-people\\n44. U.S. Transportation Security Administration. Transgender/ Non Binary / Gender Nonconforming\\nPassengers. TSA. Accessed Apr. 21, 2022. https://www.tsa.gov/transgender-passengers\\n45. See, e.g., National Disabled Law Students Association. Report on Concerns Regarding Online\\nAdministration of Bar Exams. Jul. 29, 2020. https://ndlsa.org/wp-content/uploads/2020/08/\\nNDLSA_Online-Exam-Concerns-Report1.pdf; Lydia X. Z. Brown. How Automated Test Proctoring\\nSoftware Discriminates Against Disabled Students. Center for Democracy and Technology. Nov. 16, 2020.\\nhttps://cdt.org/insights/how-automated-test-proctoring-software-discriminates-against-disabled\\xad\\nstudents/\\n46. Ziad Obermeyer, et al., Dissecting racial bias in an algorithm used to manage the health of\\npopulations, 366 Science (2019), https://www.science.org/doi/10.1126/science.aax2342.\\n66\\n\"),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 66, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content=\" \\n \\n \\n \\nENDNOTES\\n47. Darshali A. Vyas et al., Hidden in Plain Sight – Reconsidering the Use of Race Correction in Clinical\\nAlgorithms, 383 N. Engl. J. Med.874, 876-78 (Aug. 27, 2020), https://www.nejm.org/doi/full/10.1056/\\nNEJMms2004740.\\n48. The definitions of 'equity' and 'underserved communities' can be found in the Definitions section of\\nthis framework as well as in Section 2 of The Executive Order On Advancing Racial Equity and Support\\nfor Underserved Communities Through the Federal Government. https://www.whitehouse.gov/\\nbriefing-room/presidential-actions/2021/01/20/executive-order-advancing-racial-equity-and-support\\xad\\nfor-underserved-communities-through-the-federal-government/\\n49. Id.\\n50. Various organizations have offered proposals for how such assessments might be designed. See, e.g.,\\nEmanuel Moss, Elizabeth Anne Watkins, Ranjit Singh, Madeleine Clare Elish, and Jacob Metcalf.\\nAssembling Accountability: Algorithmic Impact Assessment for the Public Interest. Data & Society\\nResearch Institute Report. June 29, 2021. https://datasociety.net/library/assembling-accountability\\xad\\nalgorithmic-impact-assessment-for-the-public-interest/; Nicol Turner Lee, Paul Resnick, and Genie\\nBarton. Algorithmic bias detection and mitigation: Best practices and policies to reduce consumer harms.\\nBrookings Report. May 22, 2019.\\nhttps://www.brookings.edu/research/algorithmic-bias-detection-and-mitigation-best-practices-and\\xad\\npolicies-to-reduce-consumer-harms/; Andrew D. Selbst. An Institutional View Of Algorithmic Impact\\nAssessments. Harvard Journal of Law & Technology. June 15, 2021. https://ssrn.com/abstract=3867634;\\nDillon Reisman, Jason Schultz, Kate Crawford, and Meredith Whittaker. Algorithmic Impact\\nAssessments: A Practical Framework for Public Agency Accountability. AI Now Institute Report. April\\n2018. https://ainowinstitute.org/aiareport2018.pdf\\n51. Department of Justice. Justice Department Announces New Initiative to Combat Redlining. Oct. 22,\\n2021. https://www.justice.gov/opa/pr/justice-department-announces-new-initiative-combat-redlining\\n52. PAVE Interagency Task Force on Property Appraisal and Valuation Equity. Action Plan to Advance\\nProperty Appraisal and Valuation Equity: Closing the Racial Wealth Gap by Addressing Mis-valuations for\\nFamilies and Communities of Color. March 2022. https://pave.hud.gov/sites/pave.hud.gov/files/\\ndocuments/PAVEActionPlan.pdf\\n53. U.S. Equal Employment Opportunity Commission. The Americans with Disabilities Act and the Use of\\nSoftware, Algorithms, and Artificial Intelligence to Assess Job Applicants and Employees. EEOC\\xad\\nNVTA-2022-2. May 12, 2022. https://www.eeoc.gov/laws/guidance/americans-disabilities-act-and-use\\xad\\nsoftware-algorithms-and-artificial-intelligence; U.S. Department of Justice. Algorithms, Artificial\\nIntelligence, and Disability Discrimination in Hiring. May 12, 2022. https://beta.ada.gov/resources/ai\\xad\\nguidance/\\n54. Ziad Obermeyer, Brian Powers, Christine Vogeli, and Sendhil Mullainathan. Dissecting racial bias in\\nan algorithm used to manage the health of populations. Science. Vol. 366, No. 6464. Oct. 25, 2019. https://\\nwww.science.org/doi/10.1126/science.aax2342\\n55. Data & Trust Alliance. Algorithmic Bias Safeguards for Workforce: Overview. Jan. 2022. https://\\ndataandtrustalliance.org/Algorithmic_Bias_Safeguards_for_Workforce_Overview.pdf\\n56. Section 508.gov. IT Accessibility Laws and Policies. Access Board. https://www.section508.gov/\\nmanage/laws-and-policies/\\n67\\n\"),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 67, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content=\" \\n \\n \\nENDNOTES\\n57. ISO Technical Management Board. ISO/IEC Guide 71:2014. Guide for addressing accessibility in\\nstandards. International Standards Organization. 2021. https://www.iso.org/standard/57385.html\\n58. World Wide Web Consortium. Web Content Accessibility Guidelines (WCAG) 2.0. Dec. 11, 2008.\\nhttps://www.w3.org/TR/WCAG20/\\n59. Reva Schwartz, Apostol Vassilev, Kristen Greene, Lori Perine, and Andrew Bert. NIST Special\\nPublication 1270: Towards a Standard for Identifying and Managing Bias in Artificial Intelligence. The\\nNational Institute of Standards and Technology. March, 2022. https://nvlpubs.nist.gov/nistpubs/\\nSpecialPublications/NIST.SP.1270.pdf\\n60. See, e.g., the 2014 Federal Trade Commission report “Data Brokers A Call for Transparency and\\nAccountability”. https://www.ftc.gov/system/files/documents/reports/data-brokers-call-transparency\\xad\\naccountability-report-federal-trade-commission-may-2014/140527databrokerreport.pdf\\n61. See, e.g., Nir Kshetri. School surveillance of students via laptops may do more harm than good. The\\nConversation. Jan. 21, 2022.\\nhttps://theconversation.com/school-surveillance-of-students-via-laptops-may-do-more-harm-than\\xad\\ngood-170983; Matt Scherer. Warning: Bossware May be Hazardous to Your Health. Center for Democracy\\n& Technology Report.\\nhttps://cdt.org/wp-content/uploads/2021/07/2021-07-29-Warning-Bossware-May-Be-Hazardous-To\\xad\\nYour-Health-Final.pdf; Human Impact Partners and WWRC. The Public Health Crisis Hidden in Amazon\\nWarehouses. HIP and WWRC report. Jan. 2021.\\nhttps://humanimpact.org/wp-content/uploads/2021/01/The-Public-Health-Crisis-Hidden-In-Amazon\\xad\\nWarehouses-HIP-WWRC-01-21.pdf; Drew Harwell. Contract lawyers face a growing invasion of\\nsurveillance programs that monitor their work. The Washington Post. Nov. 11, 2021. https://\\nwww.washingtonpost.com/technology/2021/11/11/lawyer-facial-recognition-monitoring/;\\nVirginia Doellgast and Sean O'Brady. Making Call Center Jobs Better: The Relationship between\\nManagement Practices and Worker Stress. A Report for the CWA. June 2020. https://\\nhdl.handle.net/1813/74307\\n62. See, e.g., Federal Trade Commission. Data Brokers: A Call for Transparency and Accountability. May\\n2014.\\nhttps://www.ftc.gov/system/files/documents/reports/data-brokers-call-transparency-accountability\\xad\\nreport-federal-trade-commission-may-2014/140527databrokerreport.pdf; Cathy O’Neil.\\nWeapons of Math Destruction. Penguin Books. 2017.\\nhttps://en.wikipedia.org/wiki/Weapons_of_Math_Destruction\\n63. See, e.g., Rachel Levinson-Waldman, Harsha Pandurnga, and Faiza Patel. Social Media Surveillance by\\nthe U.S. Government. Brennan Center for Justice. Jan. 7, 2022.\\nhttps://www.brennancenter.org/our-work/research-reports/social-media-surveillance-us-government;\\nShoshana Zuboff. The Age of Surveillance Capitalism: The Fight for a Human Future at the New Frontier of\\nPower. Public Affairs. 2019.\\n64. Angela Chen. Why the Future of Life Insurance May Depend on Your Online Presence. The Verge. Feb.\\n7, 2019.\\nhttps://www.theverge.com/2019/2/7/18211890/social-media-life-insurance-new-york-algorithms-big\\xad\\ndata-discrimination-online-records\\n68\\n\"),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 68, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content=\" \\n65. See, e.g., Scott Ikeda. Major Data Broker Exposes 235 Million Social Media Profiles in Data Lead: Info\\nAppears to Have Been Scraped Without Permission. CPO Magazine. Aug. 28, 2020. https://\\nwww.cpomagazine.com/cyber-security/major-data-broker-exposes-235-million-social-media-profiles\\xad\\nin-data-leak/; Lily Hay Newman. 1.2 Billion Records Found Exposed Online in a Single Server. WIRED,\\nNov. 22, 2019. https://www.wired.com/story/billion-records-exposed-online/\\n66. Lola Fadulu. Facial Recognition Technology in Public Housing Prompts Backlash. New York Times.\\nSept. 24, 2019.\\nhttps://www.nytimes.com/2019/09/24/us/politics/facial-recognition-technology-housing.html\\n67. Jo Constantz. ‘They Were Spying On Us’: Amazon, Walmart, Use Surveillance Technology to Bust\\nUnions. Newsweek. Dec. 13, 2021.\\nhttps://www.newsweek.com/they-were-spying-us-amazon-walmart-use-surveillance-technology-bust\\xad\\nunions-1658603\\n68. See, e.g., enforcement actions by the FTC against the photo storage app Everalbaum\\n(https://www.ftc.gov/legal-library/browse/cases-proceedings/192-3172-everalbum-inc-matter), and\\nagainst Weight Watchers and their subsidiary Kurbo\\n(https://www.ftc.gov/legal-library/browse/cases-proceedings/1923228-weight-watchersww)\\n69. See, e.g., HIPAA, Pub. L 104-191 (1996); Fair Debt Collection Practices Act (FDCPA), Pub. L. 95-109\\n(1977); Family Educational Rights and Privacy Act (FERPA) (20 U.S.C. § 1232g), Children's Online\\nPrivacy Protection Act of 1998, 15 U.S.C. 6501–6505, and Confidential Information Protection and\\nStatistical Efficiency Act (CIPSEA) (116 Stat. 2899)\\n70. Marshall Allen. You Snooze, You Lose: Insurers Make The Old Adage Literally True. ProPublica. Nov.\\n21, 2018.\\nhttps://www.propublica.org/article/you-snooze-you-lose-insurers-make-the-old-adage-literally-true\\n71. Charles Duhigg. How Companies Learn Your Secrets. The New York Times. Feb. 16, 2012.\\nhttps://www.nytimes.com/2012/02/19/magazine/shopping-habits.html\\n72. Jack Gillum and Jeff Kao. Aggression Detectors: The Unproven, Invasive Surveillance Technology\\nSchools are Using to Monitor Students. ProPublica. Jun. 25, 2019.\\nhttps://features.propublica.org/aggression-detector/the-unproven-invasive-surveillance-technology\\xad\\nschools-are-using-to-monitor-students/\\n73. Drew Harwell. Cheating-detection companies made millions during the pandemic. Now students are\\nfighting back. Washington Post. Nov. 12, 2020.\\nhttps://www.washingtonpost.com/technology/2020/11/12/test-monitoring-student-revolt/\\n74. See, e.g., Heather Morrison. Virtual Testing Puts Disabled Students at a Disadvantage. Government\\nTechnology. May 24, 2022.\\nhttps://www.govtech.com/education/k-12/virtual-testing-puts-disabled-students-at-a-disadvantage;\\nLydia X. Z. Brown, Ridhi Shetty, Matt Scherer, and Andrew Crawford. Ableism And Disability\\nDiscrimination In New Surveillance Technologies: How new surveillance technologies in education,\\npolicing, health care, and the workplace disproportionately harm disabled people. Center for Democracy\\nand Technology Report. May 24, 2022.\\nhttps://cdt.org/insights/ableism-and-disability-discrimination-in-new-surveillance-technologies-how\\xad\\nnew-surveillance-technologies-in-education-policing-health-care-and-the-workplace\\xad\\ndisproportionately-harm-disabled-people/\\n69\\n\"),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 69, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content=' \\n \\n \\nENDNOTES\\n75. See., e.g., Sam Sabin. Digital surveillance in a post-Roe world. Politico. May 5, 2022. https://\\nwww.politico.com/newsletters/digital-future-daily/2022/05/05/digital-surveillance-in-a-post-roe\\xad\\nworld-00030459; Federal Trade Commission. FTC Sues Kochava for Selling Data that Tracks People at\\nReproductive Health Clinics, Places of Worship, and Other Sensitive Locations. Aug. 29, 2022. https://\\nwww.ftc.gov/news-events/news/press-releases/2022/08/ftc-sues-kochava-selling-data-tracks-people\\xad\\nreproductive-health-clinics-places-worship-other\\n76. Todd Feathers. This Private Equity Firm Is Amassing Companies That Collect Data on America’s\\nChildren. The Markup. Jan. 11, 2022.\\nhttps://themarkup.org/machine-learning/2022/01/11/this-private-equity-firm-is-amassing-companies\\xad\\nthat-collect-data-on-americas-children\\n77. Reed Albergotti. Every employee who leaves Apple becomes an ‘associate’: In job databases used by\\nemployers to verify resume information, every former Apple employee’s title gets erased and replaced with\\na generic title. The Washington Post. Feb. 10, 2022.\\nhttps://www.washingtonpost.com/technology/2022/02/10/apple-associate/\\n78. National Institute of Standards and Technology. Privacy Framework Perspectives and Success\\nStories. Accessed May 2, 2022.\\nhttps://www.nist.gov/privacy-framework/getting-started-0/perspectives-and-success-stories\\n79. ACLU of New York. What You Need to Know About New York’s Temporary Ban on Facial\\nRecognition in Schools. Accessed May 2, 2022.\\nhttps://www.nyclu.org/en/publications/what-you-need-know-about-new-yorks-temporary-ban-facial\\xad\\nrecognition-schools\\n80. New York State Assembly. Amendment to Education Law. Enacted Dec. 22, 2020.\\nhttps://nyassembly.gov/leg/?default_fld=&leg_video=&bn=S05140&term=2019&Summary=Y&Text=Y\\n81. U.S Department of Labor. Labor-Management Reporting and Disclosure Act of 1959, As Amended.\\nhttps://www.dol.gov/agencies/olms/laws/labor-management-reporting-and-disclosure-act (Section\\n203). See also: U.S Department of Labor. Form LM-10. OLMS Fact Sheet, Accessed May 2, 2022. https://\\nwww.dol.gov/sites/dolgov/files/OLMS/regs/compliance/LM-10_factsheet.pdf\\n82. See, e.g., Apple. Protecting the User’s Privacy. Accessed May 2, 2022.\\nhttps://developer.apple.com/documentation/uikit/protecting_the_user_s_privacy; Google Developers.\\nDesign for Safety: Android is secure by default and private by design. Accessed May 3, 2022.\\nhttps://developer.android.com/design-for-safety\\n83. Karen Hao. The coming war on the hidden algorithms that trap people in poverty. MIT Tech Review.\\nDec. 4, 2020.\\nhttps://www.technologyreview.com/2020/12/04/1013068/algorithms-create-a-poverty-trap-lawyers\\xad\\nfight-back/\\n84. Anjana Samant, Aaron Horowitz, Kath Xu, and Sophie Beiers. Family Surveillance by Algorithm.\\nACLU. Accessed May 2, 2022.\\nhttps://www.aclu.org/fact-sheet/family-surveillance-algorithm\\n70\\n'),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 70, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content=\" \\n \\nENDNOTES\\n85. Mick Dumke and Frank Main. A look inside the watch list Chicago police fought to keep secret. The\\nChicago Sun Times. May 18, 2017.\\nhttps://chicago.suntimes.com/2017/5/18/18386116/a-look-inside-the-watch-list-chicago-police-fought\\xad\\nto-keep-secret\\n86. Jay Stanley. Pitfalls of Artificial Intelligence Decisionmaking Highlighted In Idaho ACLU Case.\\nACLU. Jun. 2, 2017.\\nhttps://www.aclu.org/blog/privacy-technology/pitfalls-artificial-intelligence-decisionmaking\\xad\\nhighlighted-idaho-aclu-case\\n87. Illinois General Assembly. Biometric Information Privacy Act. Effective Oct. 3, 2008.\\nhttps://www.ilga.gov/legislation/ilcs/ilcs3.asp?ActID=3004&ChapterID=57\\n88. Partnership on AI. ABOUT ML Reference Document. Accessed May 2, 2022.\\nhttps://partnershiponai.org/paper/about-ml-reference-document/1/\\n89. See, e.g., the model cards framework: Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker\\nBarnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru.\\nModel Cards for Model Reporting. In Proceedings of the Conference on Fairness, Accountability, and\\nTransparency (FAT* '19). Association for Computing Machinery, New York, NY, USA, 220–229. https://\\ndl.acm.org/doi/10.1145/3287560.3287596\\n90. Sarah Ammermann. Adverse Action Notice Requirements Under the ECOA and the FCRA. Consumer\\nCompliance Outlook. Second Quarter 2013.\\nhttps://consumercomplianceoutlook.org/2013/second-quarter/adverse-action-notice-requirements\\xad\\nunder-ecoa-fcra/\\n91. Federal Trade Commission. Using Consumer Reports for Credit Decisions: What to Know About\\nAdverse Action and Risk-Based Pricing Notices. Accessed May 2, 2022.\\nhttps://www.ftc.gov/business-guidance/resources/using-consumer-reports-credit-decisions-what\\xad\\nknow-about-adverse-action-risk-based-pricing-notices#risk\\n92. Consumer Financial Protection Bureau. CFPB Acts to Protect the Public from Black-Box Credit\\nModels Using Complex Algorithms. May 26, 2022.\\nhttps://www.consumerfinance.gov/about-us/newsroom/cfpb-acts-to-protect-the-public-from-black\\xad\\nbox-credit-models-using-complex-algorithms/\\n93. Anthony Zaller. California Passes Law Regulating Quotas In Warehouses – What Employers Need to\\nKnow About AB 701. Zaller Law Group California Employment Law Report. Sept. 24, 2021.\\nhttps://www.californiaemploymentlawreport.com/2021/09/california-passes-law-regulating-quotas\\xad\\nin-warehouses-what-employers-need-to-know-about-ab-701/\\n94. National Institute of Standards and Technology. AI Fundamental Research – Explainability.\\nAccessed Jun. 4, 2022.\\nhttps://www.nist.gov/artificial-intelligence/ai-fundamental-research-explainability\\n95. DARPA. Explainable Artificial Intelligence (XAI). Accessed July 20, 2022.\\nhttps://www.darpa.mil/program/explainable-artificial-intelligence\\n71\\n\"),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 71, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content=' \\nENDNOTES\\n96. National Science Foundation. NSF Program on Fairness in Artificial Intelligence in Collaboration\\nwith Amazon (FAI). Accessed July 20, 2022.\\nhttps://www.nsf.gov/pubs/2021/nsf21585/nsf21585.htm\\n97. Kyle Wiggers. Automatic signature verification software threatens to disenfranchise U.S. voters.\\nVentureBeat. Oct. 25, 2020.\\nhttps://venturebeat.com/2020/10/25/automatic-signature-verification-software-threatens-to\\xad\\ndisenfranchise-u-s-voters/\\n98. Ballotpedia. Cure period for absentee and mail-in ballots. Article retrieved Apr 18, 2022.\\nhttps://ballotpedia.org/Cure_period_for_absentee_and_mail-in_ballots\\n99. Larry Buchanan and Alicia Parlapiano. Two of these Mail Ballot Signatures are by the Same Person.\\nWhich Ones? New York Times. Oct. 7, 2020.\\nhttps://www.nytimes.com/interactive/2020/10/07/upshot/mail-voting-ballots-signature\\xad\\nmatching.html\\n100. Rachel Orey and Owen Bacskai. The Low Down on Ballot Curing. Nov. 04, 2020.\\nhttps://bipartisanpolicy.org/blog/the-low-down-on-ballot-curing/\\n101. Andrew Kenney. \\'I\\'m shocked that they need to have a smartphone\\': System for unemployment\\nbenefits exposes digital divide. USA Today. May 2, 2021.\\nhttps://www.usatoday.com/story/tech/news/2021/05/02/unemployment-benefits-system-leaving\\xad\\npeople-behind/4915248001/\\n102. Allie Gross. UIA lawsuit shows how the state criminalizes the unemployed. Detroit Metro-Times.\\nSep. 18, 2015.\\nhttps://www.metrotimes.com/news/uia-lawsuit-shows-how-the-state-criminalizes-the\\xad\\nunemployed-2369412\\n103. Maia Szalavitz. The Pain Was Unbearable. So Why Did Doctors Turn Her Away? Wired. Aug. 11,\\n2021. https://www.wired.com/story/opioid-drug-addiction-algorithm-chronic-pain/\\n104. Spencer Soper. Fired by Bot at Amazon: \"It\\'s You Against the Machine\". Bloomberg, Jun. 28, 2021.\\nhttps://www.bloomberg.com/news/features/2021-06-28/fired-by-bot-amazon-turns-to-machine\\xad\\nmanagers-and-workers-are-losing-out\\n105. Definitions of ‘equity’ and ‘underserved communities’ can be found in the Definitions section of\\nthis document as well as in Executive Order on Advancing Racial Equity and Support for Underserved\\nCommunities Through the Federal Government:\\nhttps://www.whitehouse.gov/briefing-room/presidential-actions/2021/01/20/executive-order\\xad\\nadvancing-racial-equity-and-support-for-underserved-communities-through-the-federal-government/\\n106. HealthCare.gov. Navigator - HealthCare.gov Glossary. Accessed May 2, 2022.\\nhttps://www.healthcare.gov/glossary/navigator/\\n72\\n'),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 72, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': ''}, page_content=' \\nENDNOTES\\n107. Centers for Medicare & Medicaid Services. Biden-Harris Administration Quadruples the Number\\nof Health Care Navigators Ahead of HealthCare.gov Open Enrollment Period. Aug. 27, 2021.\\nhttps://www.cms.gov/newsroom/press-releases/biden-harris-administration-quadruples-number\\xad\\nhealth-care-navigators-ahead-healthcaregov-open\\n108. See, e.g., McKinsey & Company. The State of Customer Care in 2022. July 8, 2022. https://\\nwww.mckinsey.com/business-functions/operations/our-insights/the-state-of-customer-care-in-2022;\\nSara Angeles. Customer Service Solutions for Small Businesses. Business News Daily.\\nJun. 29, 2022. https://www.businessnewsdaily.com/7575-customer-service-solutions.html\\n109. Mike Hughes. Are We Getting The Best Out Of Our Bots? Co-Intelligence Between Robots &\\nHumans. Forbes. Jul. 14, 2022.\\nhttps://www.forbes.com/sites/mikehughes1/2022/07/14/are-we-getting-the-best-out-of-our-bots-co\\xad\\nintelligence-between-robots--humans/?sh=16a2bd207395\\n110. Rachel Orey and Owen Bacskai. The Low Down on Ballot Curing. Nov. 04, 2020. https://\\nbipartisanpolicy.org/blog/the-low-down-on-ballot-curing/; Zahavah Levine and Thea Raymond-\\nSeidel. Mail Voting Litigation in 2020, Part IV: Verifying Mail Ballots. Oct. 29, 2020.\\nhttps://www.lawfareblog.com/mail-voting-litigation-2020-part-iv-verifying-mail-ballots\\n111. National Conference of State Legislatures. Table 15: States With Signature Cure Processes. Jan. 18,\\n2022.\\nhttps://www.ncsl.org/research/elections-and-campaigns/vopp-table-15-states-that-permit-voters-to\\xad\\ncorrect-signature-discrepancies.aspx\\n112. White House Office of Science and Technology Policy. Join the Effort to Create A Bill of Rights for\\nan Automated Society. Nov. 10, 2021.\\nhttps://www.whitehouse.gov/ostp/news-updates/2021/11/10/join-the-effort-to-create-a-bill-of\\xad\\nrights-for-an-automated-society/\\n113. White House Office of Science and Technology Policy. Notice of Request for Information (RFI) on\\nPublic and Private Sector Uses of Biometric Technologies. Issued Oct. 8, 2021.\\nhttps://www.federalregister.gov/documents/2021/10/08/2021-21975/notice-of-request-for\\xad\\ninformation-rfi-on-public-and-private-sector-uses-of-biometric-technologies\\n114. National Artificial Intelligence Initiative Office. Public Input on Public and Private Sector Uses of\\nBiometric Technologies. Accessed Apr. 19, 2022.\\nhttps://www.ai.gov/86-fr-56300-responses/\\n115. Thomas D. Olszewski, Lisa M. Van Pay, Javier F. Ortiz, Sarah E. Swiersz, and Laurie A. Dacus.\\nSynopsis of Responses to OSTP’s Request for Information on the Use and Governance of Biometric\\nTechnologies in the Public and Private Sectors. Science and Technology Policy Institute. Mar. 2022.\\nhttps://www.ida.org/-/media/feature/publications/s/sy/synopsis-of-responses-to-request-for\\xad\\ninformation-on-the-use-and-governance-of-biometric-technologies/ida-document-d-33070.ashx\\n73\\n'),\n",
              " Document(metadata={'source': 'dataset/data2.pdf', 'file_path': 'dataset/data2.pdf', 'page': 0, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=' \\nNIST Trustworthy and Responsible AI  \\nNIST AI 600-1 \\nArtificial Intelligence Risk Management \\nFramework: Generative Artificial \\nIntelligence Profile \\n \\n \\n \\nThis publication is available free of charge from: \\nhttps://doi.org/10.6028/NIST.AI.600-1 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n'),\n",
              " Document(metadata={'source': 'dataset/data2.pdf', 'file_path': 'dataset/data2.pdf', 'page': 1, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=' \\n \\n \\nNIST Trustworthy and Responsible AI  \\nNIST AI 600-1 \\nArtificial Intelligence Risk Management \\nFramework: Generative Artificial \\nIntelligence Profile \\n \\n \\n \\nThis publication is available free of charge from: \\nhttps://doi.org/10.6028/NIST.AI.600-1 \\n \\nJuly 2024 \\n \\n \\n \\n \\nU.S. Department of Commerce  \\nGina M. Raimondo, Secretary \\nNational Institute of Standards and Technology  \\nLaurie E. Locascio, NIST Director and Under Secretary of Commerce for Standards and Technology  \\n \\n'),\n",
              " Document(metadata={'source': 'dataset/data2.pdf', 'file_path': 'dataset/data2.pdf', 'page': 2, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=' \\n \\n \\nAbout AI at NIST: The National Institute of Standards and Technology (NIST) develops measurements, \\ntechnology, tools, and standards to advance reliable, safe, transparent, explainable, privacy-enhanced, \\nand fair artiﬁcial intelligence (AI) so that its full commercial and societal beneﬁts can be realized without \\nharm to people or the planet. NIST, which has conducted both fundamental and applied work on AI for \\nmore than a decade, is also helping to fulﬁll the 2023 Executive Order on Safe, Secure, and Trustworthy \\nAI. NIST established the U.S. AI Safety Institute and the companion AI Safety Institute Consortium to \\ncontinue the eﬀorts set in motion by the E.O. to build the science necessary for safe, secure, and \\ntrustworthy development and use of AI. \\nAcknowledgments: This report was accomplished with the many helpful comments and contributions \\nfrom the community, including the NIST Generative AI Public Working Group, and NIST staﬀ and guest \\nresearchers: Chloe Autio, Jesse Dunietz, Patrick Hall, Shomik Jain, Kamie Roberts, Reva Schwartz, Martin \\nStanley, and Elham Tabassi. \\nNIST Technical Series Policies \\nCopyright, Use, and Licensing Statements \\nNIST Technical Series Publication Identifier Syntax \\nPublication History \\nApproved by the NIST Editorial Review Board on 07-25-2024 \\nContact Information \\nai-inquiries@nist.gov \\nNational Institute of Standards and Technology \\nAttn: NIST AI Innovation Lab, Information Technology Laboratory \\n100 Bureau Drive (Mail Stop 8900) Gaithersburg, MD 20899-8900 \\nAdditional Information \\nAdditional information about this publication and other NIST AI publications are available at \\nhttps://airc.nist.gov/Home. \\n \\nDisclaimer: Certain commercial entities, equipment, or materials may be identiﬁed in this document in \\norder to adequately describe an experimental procedure or concept. Such identiﬁcation is not intended to \\nimply recommendation or endorsement by the National Institute of Standards and Technology, nor is it \\nintended to imply that the entities, materials, or equipment are necessarily the best available for the \\npurpose. Any mention of commercial, non-proﬁt, academic partners, or their products, or references is \\nfor information only; it is not intended to imply endorsement or recommendation by any U.S. \\nGovernment agency. \\n \\n'),\n",
              " Document(metadata={'source': 'dataset/data2.pdf', 'file_path': 'dataset/data2.pdf', 'page': 3, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=' \\n \\n \\nTable of Contents \\n1. \\nIntroduction ..............................................................................................................................................1 \\n2. \\nOverview of Risks Unique to or Exacerbated by GAI .....................................................................2 \\n3. \\nSuggested Actions to Manage GAI Risks ......................................................................................... 12 \\nAppendix A. Primary GAI Considerations ............................................................................................... 47 \\nAppendix B. References ................................................................................................................................ 54 \\n \\n \\n \\n \\n \\n'),\n",
              " Document(metadata={'source': 'dataset/data2.pdf', 'file_path': 'dataset/data2.pdf', 'page': 4, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=' \\n1 \\n1. \\nIntroduction \\nThis document is a cross-sectoral proﬁle of and companion resource for the AI Risk Management \\nFramework (AI RMF 1.0) for Generative AI,1 pursuant to President Biden’s Executive Order (EO) 14110 on \\nSafe, Secure, and Trustworthy Artiﬁcial Intelligence.2 The AI RMF was released in January 2023, and is \\nintended for voluntary use and to improve the ability of organizations to incorporate trustworthiness \\nconsiderations into the design, development, use, and evaluation of AI products, services, and systems.  \\nA proﬁle is an implementation of the AI RMF functions, categories, and subcategories for a speciﬁc \\nsetting, application, or technology – in this case, Generative AI (GAI) – based on the requirements, risk \\ntolerance, and resources of the Framework user. AI RMF proﬁles assist organizations in deciding how to \\nbest manage AI risks in a manner that is well-aligned with their goals, considers legal/regulatory \\nrequirements and best practices, and reﬂects risk management priorities. Consistent with other AI RMF \\nproﬁles, this proﬁle oﬀers insights into how risk can be managed across various stages of the AI lifecycle \\nand for GAI as a technology.  \\nAs GAI covers risks of models or applications that can be used across use cases or sectors, this document \\nis an AI RMF cross-sectoral proﬁle. Cross-sectoral proﬁles can be used to govern, map, measure, and \\nmanage risks associated with activities or business processes common across sectors, such as the use of \\nlarge language models (LLMs), cloud-based services, or acquisition. \\nThis document deﬁnes risks that are novel to or exacerbated by the use of GAI. After introducing and \\ndescribing these risks, the document provides a set of suggested actions to help organizations govern, \\nmap, measure, and manage these risks. \\n \\n \\n1 EO 14110 deﬁnes Generative AI as “the class of AI models that emulate the structure and characteristics of input \\ndata in order to generate derived synthetic content. This can include images, videos, audio, text, and other digital \\ncontent.” While not all GAI is derived from foundation models, for purposes of this document, GAI generally refers \\nto generative foundation models. The foundation model subcategory of “dual-use foundation models” is deﬁned by \\nEO 14110 as “an AI model that is trained on broad data; generally uses self-supervision; contains at least tens of \\nbillions of parameters; is applicable across a wide range of contexts.”  \\n2 This proﬁle was developed per Section 4.1(a)(i)(A) of EO 14110, which directs the Secretary of Commerce, acting \\nthrough the Director of the National Institute of Standards and Technology (NIST), to develop a companion \\nresource to the AI RMF, NIST AI 100–1, for generative AI. \\n'),\n",
              " Document(metadata={'source': 'dataset/data2.pdf', 'file_path': 'dataset/data2.pdf', 'page': 5, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=' \\n2 \\nThis work was informed by public feedback and consultations with diverse stakeholder groups as part of NIST’s \\nGenerative AI Public Working Group (GAI PWG). The GAI PWG was an open, transparent, and collaborative \\nprocess, facilitated via a virtual workspace, to obtain multistakeholder input on GAI risk management and to \\ninform NIST’s approach. \\nThe focus of the GAI PWG was limited to four primary considerations relevant to GAI: Governance, Content \\nProvenance, Pre-deployment Testing, and Incident Disclosure (further described in Appendix A). As such, the \\nsuggested actions in this document primarily address these considerations. \\nFuture revisions of this proﬁle will include additional AI RMF subcategories, risks, and suggested actions based \\non additional considerations of GAI as the space evolves and empirical evidence indicates additional risks. A \\nglossary of terms pertinent to GAI risk management will be developed and hosted on NIST’s Trustworthy & \\nResponsible AI Resource Center (AIRC), and added to The Language of Trustworthy AI: An In-Depth Glossary of \\nTerms. \\nThis document was also informed by public comments and consultations from several Requests for Information. \\n \\n2. \\nOverview of Risks Unique to or Exacerbated by GAI \\nIn the context of the AI RMF, risk refers to the composite measure of an event’s probability (or \\nlikelihood) of occurring and the magnitude or degree of the consequences of the corresponding event. \\nSome risks can be assessed as likely to materialize in a given context, particularly those that have been \\nempirically demonstrated in similar contexts. Other risks may be unlikely to materialize in a given \\ncontext, or may be more speculative and therefore uncertain. \\nAI risks can diﬀer from or intensify traditional software risks. Likewise, GAI can exacerbate existing AI \\nrisks, and creates unique risks. GAI risks can vary along many dimensions: \\n• \\nStage of the AI lifecycle: Risks can arise during design, development, deployment, operation, \\nand/or decommissioning. \\n• \\nScope: Risks may exist at individual model or system levels, at the application or implementation \\nlevels (i.e., for a speciﬁc use case), or at the ecosystem level – that is, beyond a single system or \\norganizational context. Examples of the latter include the expansion of “algorithmic \\nmonocultures,3” resulting from repeated use of the same model, or impacts on access to \\nopportunity, labor markets, and the creative economies.4 \\n• \\nSource of risk: Risks may emerge from factors related to the design, training, or operation of the \\nGAI model itself, stemming in some cases from GAI model or system inputs, and in other cases, \\nfrom GAI system outputs. Many GAI risks, however, originate from human behavior, including \\n \\n \\n3 “Algorithmic monocultures” refers to the phenomenon in which repeated use of the same model or algorithm in \\nconsequential decision-making settings like employment and lending can result in increased susceptibility by \\nsystems to correlated failures (like unexpected shocks), due to multiple actors relying on the same algorithm.  \\n4 Many studies have projected the impact of AI on the workforce and labor markets. Fewer studies have examined \\nthe impact of GAI on the labor market, though some industry surveys indicate that that both employees and \\nemployers are pondering this disruption.  \\n'),\n",
              " Document(metadata={'source': 'dataset/data2.pdf', 'file_path': 'dataset/data2.pdf', 'page': 6, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=' \\n3 \\nthe abuse, misuse, and unsafe repurposing by humans (adversarial or not), and others result \\nfrom interactions between a human and an AI system.  \\n• \\nTime scale: GAI risks may materialize abruptly or across extended periods. Examples include \\nimmediate (and/or prolonged) emotional harm and potential risks to physical safety due to the \\ndistribution of harmful deepfake images, or the long-term eﬀect of disinformation on societal \\ntrust in public institutions. \\nThe presence of risks and where they fall along the dimensions above will vary depending on the \\ncharacteristics of the GAI model, system, or use case at hand. These characteristics include but are not \\nlimited to GAI model or system architecture, training mechanisms and libraries, data types used for \\ntraining or ﬁne-tuning, levels of model access or availability of model weights, and application or use \\ncase context. \\nOrganizations may choose to tailor how they measure GAI risks based on these characteristics. They may \\nadditionally wish to allocate risk management resources relative to the severity and likelihood of \\nnegative impacts, including where and how these risks manifest, and their direct and material impacts \\nharms in the context of GAI use. Mitigations for model or system level risks may diﬀer from mitigations \\nfor use-case or ecosystem level risks. \\nImportantly, some GAI risks are unknown, and are therefore diﬃcult to properly scope or evaluate given \\nthe uncertainty about potential GAI scale, complexity, and capabilities. Other risks may be known but \\ndiﬃcult to estimate given the wide range of GAI stakeholders, uses, inputs, and outputs. Challenges with \\nrisk estimation are aggravated by a lack of visibility into GAI training data, and the generally immature \\nstate of the science of AI measurement and safety today. This document focuses on risks for which there \\nis an existing empirical evidence base at the time this proﬁle was written; for example, speculative risks \\nthat may potentially arise in more advanced, future GAI systems are not considered. Future updates may \\nincorporate additional risks or provide further details on the risks identiﬁed below. \\nTo guide organizations in identifying and managing GAI risks, a set of risks unique to or exacerbated by \\nthe development and use of GAI are deﬁned below.5 Each risk is labeled according to the outcome, \\nobject, or source of the risk (i.e., some are risks “to” a subject or domain and others are risks “of” or \\n“from” an issue or theme). These risks provide a lens through which organizations can frame and execute \\nrisk management eﬀorts. To help streamline risk management eﬀorts, each risk is mapped in Section 3 \\n(as well as in tables in Appendix B) to relevant Trustworthy AI Characteristics identiﬁed in the AI RMF.  \\n \\n \\n5 These risks can be further categorized by organizations depending on their unique approaches to risk deﬁnition \\nand management. One possible way to further categorize these risks, derived in part from the UK’s International \\nScientiﬁc Report on the Safety of Advanced AI, could be: 1) Technical / Model risks (or risk from malfunction): \\nConfabulation; Dangerous or Violent Recommendations; Data Privacy; Value Chain and Component Integration; \\nHarmful Bias, and Homogenization; 2) Misuse by humans (or malicious use): CBRN Information or Capabilities; \\nData Privacy; Human-AI Conﬁguration; Obscene, Degrading, and/or Abusive Content; Information Integrity; \\nInformation Security; 3) Ecosystem / societal risks (or systemic risks): Data Privacy; Environmental; Intellectual \\nProperty. We also note that some risks are cross-cutting between these categories.  \\n'),\n",
              " Document(metadata={'source': 'dataset/data2.pdf', 'file_path': 'dataset/data2.pdf', 'page': 7, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=' \\n4 \\n1. CBRN Information or Capabilities: Eased access to or synthesis of materially nefarious \\ninformation or design capabilities related to chemical, biological, radiological, or nuclear (CBRN) \\nweapons or other dangerous materials or agents. \\n2. Confabulation: The production of conﬁdently stated but erroneous or false content (known \\ncolloquially as “hallucinations” or “fabrications”) by which users may be misled or deceived.6 \\n3. Dangerous, Violent, or Hateful Content: Eased production of and access to violent, inciting, \\nradicalizing, or threatening content as well as recommendations to carry out self-harm or \\nconduct illegal activities. Includes diﬃculty controlling public exposure to hateful and disparaging \\nor stereotyping content. \\n4. Data Privacy: Impacts due to leakage and unauthorized use, disclosure, or de-anonymization of \\nbiometric, health, location, or other personally identiﬁable information or sensitive data.7 \\n5. Environmental Impacts: Impacts due to high compute resource utilization in training or \\noperating GAI models, and related outcomes that may adversely impact ecosystems.  \\n6. Harmful Bias or Homogenization: Ampliﬁcation and exacerbation of historical, societal, and \\nsystemic biases; performance disparities8 between sub-groups or languages, possibly due to \\nnon-representative training data, that result in discrimination, ampliﬁcation of biases, or \\nincorrect presumptions about performance; undesired homogeneity that skews system or model \\noutputs, which may be erroneous, lead to ill-founded decision-making, or amplify harmful \\nbiases.  \\n7. Human-AI Conﬁguration: Arrangements of or interactions between a human and an AI system \\nwhich can result in the human inappropriately anthropomorphizing GAI systems or experiencing \\nalgorithmic aversion, automation bias, over-reliance, or emotional entanglement with GAI \\nsystems. \\n8. Information Integrity: Lowered barrier to entry to generate and support the exchange and \\nconsumption of content which may not distinguish fact from opinion or ﬁction or acknowledge \\nuncertainties, or could be leveraged for large-scale dis- and mis-information campaigns. \\n9. Information Security: Lowered barriers for oﬀensive cyber capabilities, including via automated \\ndiscovery and exploitation of vulnerabilities to ease hacking, malware, phishing, oﬀensive cyber \\n \\n \\n6 Some commenters have noted that the terms “hallucination” and “fabrication” anthropomorphize GAI, which \\nitself is a risk related to GAI systems as it can inappropriately attribute human characteristics to non-human \\nentities.  \\n7 What is categorized as sensitive data or sensitive PII can be highly contextual based on the nature of the \\ninformation, but examples of sensitive information include information that relates to an information subject’s \\nmost intimate sphere, including political opinions, sex life, or criminal convictions.  \\n8 The notion of harm presumes some baseline scenario that the harmful factor (e.g., a GAI model) makes worse. \\nWhen the mechanism for potential harm is a disparity between groups, it can be diﬃcult to establish what the \\nmost appropriate baseline is to compare against, which can result in divergent views on when a disparity between \\nAI behaviors for diﬀerent subgroups constitutes a harm. In discussing harms from disparities such as biased \\nbehavior, this document highlights examples where someone’s situation is worsened relative to what it would have \\nbeen in the absence of any AI system, making the outcome unambiguously a harm of the system.  \\n'),\n",
              " Document(metadata={'source': 'dataset/data2.pdf', 'file_path': 'dataset/data2.pdf', 'page': 8, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=' \\n5 \\noperations, or other cyberattacks; increased attack surface for targeted cyberattacks, which may \\ncompromise a system’s availability or the conﬁdentiality or integrity of training data, code, or \\nmodel weights.  \\n10. Intellectual Property: Eased production or replication of alleged copyrighted, trademarked, or \\nlicensed content without authorization (possibly in situations which do not fall under fair use); \\neased exposure of trade secrets; or plagiarism or illegal replication.  \\n11. Obscene, Degrading, and/or Abusive Content: Eased production of and access to obscene, \\ndegrading, and/or abusive imagery which can cause harm, including synthetic child sexual abuse \\nmaterial (CSAM), and nonconsensual intimate images (NCII) of adults. \\n12. Value Chain and Component Integration: Non-transparent or untraceable integration of \\nupstream third-party components, including data that has been improperly obtained or not \\nprocessed and cleaned due to increased automation from GAI; improper supplier vetting across \\nthe AI lifecycle; or other issues that diminish transparency or accountability for downstream \\nusers. \\n2.1. CBRN Information or Capabilities \\nIn the future, GAI may enable malicious actors to more easily access CBRN weapons and/or relevant \\nknowledge, information, materials, tools, or technologies that could be misused to assist in the design, \\ndevelopment, production, or use of CBRN weapons or other dangerous materials or agents. While \\nrelevant biological and chemical threat knowledge and information is often publicly accessible, LLMs \\ncould facilitate its analysis or synthesis, particularly by individuals without formal scientiﬁc training or \\nexpertise.  \\nRecent research on this topic found that LLM outputs regarding biological threat creation and attack \\nplanning provided minimal assistance beyond traditional search engine queries, suggesting that state-of-\\nthe-art LLMs at the time these studies were conducted do not substantially increase the operational \\nlikelihood of such an attack. The physical synthesis development, production, and use of chemical or \\nbiological agents will continue to require both applicable expertise and supporting materials and \\ninfrastructure. The impact of GAI on chemical or biological agent misuse will depend on what the key \\nbarriers for malicious actors are (e.g., whether information access is one such barrier), and how well GAI \\ncan help actors address those barriers.  \\nFurthermore, chemical and biological design tools (BDTs) – highly specialized AI systems trained on \\nscientiﬁc data that aid in chemical and biological design – may augment design capabilities in chemistry \\nand biology beyond what text-based LLMs are able to provide. As these models become more \\neﬃcacious, including for beneﬁcial uses, it will be important to assess their potential to be used for \\nharm, such as the ideation and design of novel harmful chemical or biological agents.  \\nWhile some of these described capabilities lie beyond the reach of existing GAI tools, ongoing \\nassessments of this risk would be enhanced by monitoring both the ability of AI tools to facilitate CBRN \\nweapons planning and GAI systems’ connection or access to relevant data and tools. \\nTrustworthy AI Characteristic: Safe, Explainable and Interpretable \\n'),\n",
              " Document(metadata={'source': 'dataset/data2.pdf', 'file_path': 'dataset/data2.pdf', 'page': 9, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=' \\n6 \\n2.2. Confabulation \\n“Confabulation” refers to a phenomenon in which GAI systems generate and conﬁdently present \\nerroneous or false content in response to prompts. Confabulations also include generated outputs that \\ndiverge from the prompts or other input or that contradict previously generated statements in the same \\ncontext. These phenomena are colloquially also referred to as “hallucinations” or “fabrications.” \\nConfabulations can occur across GAI outputs and contexts.9,10 Confabulations are a natural result of the \\nway generative models are designed: they generate outputs that approximate the statistical distribution \\nof their training data; for example, LLMs predict the next token or word in a sentence or phrase. While \\nsuch statistical prediction can produce factually accurate and consistent outputs, it can also produce \\noutputs that are factually inaccurate or internally inconsistent. This dynamic is particularly relevant when \\nit comes to open-ended prompts for long-form responses and in domains which require highly \\ncontextual and/or domain expertise.  \\nRisks from confabulations may arise when users believe false content – often due to the conﬁdent nature \\nof the response – leading users to act upon or promote the false information. This poses a challenge for \\nmany real-world applications, such as in healthcare, where a confabulated summary of patient \\ninformation reports could cause doctors to make incorrect diagnoses and/or recommend the wrong \\ntreatments. Risks of confabulated content may be especially important to monitor when integrating GAI \\ninto applications involving consequential decision making. \\nGAI outputs may also include confabulated logic or citations that purport to justify or explain the \\nsystem’s answer, which may further mislead humans into inappropriately trusting the system’s output. \\nFor instance, LLMs sometimes provide logical steps for how they arrived at an answer even when the \\nanswer itself is incorrect. Similarly, an LLM could falsely assert that it is human or has human traits, \\npotentially deceiving humans into believing they are speaking with another human. \\nThe extent to which humans can be deceived by LLMs, the mechanisms by which this may occur, and the \\npotential risks from adversarial prompting of such behavior are emerging areas of study. Given the wide \\nrange of downstream impacts of GAI, it is diﬃcult to estimate the downstream scale and impact of \\nconfabulations. \\nTrustworthy AI Characteristics: Fair with Harmful Bias Managed, Safe, Valid and Reliable, Explainable \\nand Interpretable \\n2.3. Dangerous, Violent, or Hateful Content \\nGAI systems can produce content that is inciting, radicalizing, or threatening, or that gloriﬁes violence, \\nwith greater ease and scale than other technologies. LLMs have been reported to generate dangerous or \\nviolent recommendations, and some models have generated actionable instructions for dangerous or \\n \\n \\n9 Confabulations of falsehoods are most commonly a problem for text-based outputs; for audio, image, or video \\ncontent, creative generation of non-factual content can be a desired behavior.  \\n10 For example, legal confabulations have been shown to be pervasive in current state-of-the-art LLMs. See also, \\ne.g.,  \\n'),\n",
              " Document(metadata={'source': 'dataset/data2.pdf', 'file_path': 'dataset/data2.pdf', 'page': 10, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=' \\n7 \\nunethical behavior. Text-to-image models also make it easy to create images that could be used to \\npromote dangerous or violent messages. Similar concerns are present for other GAI media, including \\nvideo and audio. GAI may also produce content that recommends self-harm or criminal/illegal activities.  \\nMany current systems restrict model outputs to limit certain content or in response to certain prompts, \\nbut this approach may still produce harmful recommendations in response to other less-explicit, novel \\nprompts (also relevant to CBRN Information or Capabilities, Data Privacy, Information Security, and \\nObscene, Degrading and/or Abusive Content). Crafting such prompts deliberately is known as \\n“jailbreaking,” or, manipulating prompts to circumvent output controls. Limitations of GAI systems can be \\nharmful or dangerous in certain contexts. Studies have observed that users may disclose mental health \\nissues in conversations with chatbots – and that users exhibit negative reactions to unhelpful responses \\nfrom these chatbots during situations of distress. \\nThis risk encompasses diﬃculty controlling creation of and public exposure to oﬀensive or hateful \\nlanguage, and denigrating or stereotypical content generated by AI. This kind of speech may contribute \\nto downstream harm such as fueling dangerous or violent behaviors. The spread of denigrating or \\nstereotypical content can also further exacerbate representational harms (see Harmful Bias and \\nHomogenization below).  \\nTrustworthy AI Characteristics: Safe, Secure and Resilient \\n2.4. Data Privacy \\nGAI systems raise several risks to privacy. GAI system training requires large volumes of data, which in \\nsome cases may include personal data. The use of personal data for GAI training raises risks to widely \\naccepted privacy principles, including to transparency, individual participation (including consent), and \\npurpose speciﬁcation. For example, most model developers do not disclose speciﬁc data sources on \\nwhich models were trained, limiting user awareness of whether personally identiﬁably information (PII) \\nwas trained on and, if so, how it was collected.  \\nModels may leak, generate, or correctly infer sensitive information about individuals. For example, \\nduring adversarial attacks, LLMs have revealed sensitive information (from the public domain) that was \\nincluded in their training data. This problem has been referred to as data memorization, and may pose \\nexacerbated privacy risks even for data present only in a small number of training samples.  \\nIn addition to revealing sensitive information in GAI training data, GAI models may be able to correctly \\ninfer PII or sensitive data that was not in their training data nor disclosed by the user by stitching \\ntogether information from disparate sources. These inferences can have negative impact on an individual \\neven if the inferences are not accurate (e.g., confabulations), and especially if they reveal information \\nthat the individual considers sensitive or that is used to disadvantage or harm them. \\nBeyond harms from information exposure (such as extortion or dignitary harm), wrong or inappropriate \\ninferences of PII can contribute to downstream or secondary harmful impacts. For example, predictive \\ninferences made by GAI models based on PII or protected attributes can contribute to adverse decisions, \\nleading to representational or allocative harms to individuals or groups (see Harmful Bias and \\nHomogenization below).  \\n'),\n",
              " Document(metadata={'source': 'dataset/data2.pdf', 'file_path': 'dataset/data2.pdf', 'page': 11, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=' \\n8 \\nTrustworthy AI Characteristics: Accountable and Transparent, Privacy Enhanced, Safe, Secure and \\nResilient \\n2.5. Environmental Impacts \\nTraining, maintaining, and operating (running inference on) GAI systems are resource-intensive activities, \\nwith potentially large energy and environmental footprints. Energy and carbon emissions vary based on \\nwhat is being done with the GAI model (i.e., pre-training, ﬁne-tuning, inference), the modality of the \\ncontent, hardware used, and type of task or application. \\nCurrent estimates suggest that training a single transformer LLM can emit as much carbon as 300 round-\\ntrip ﬂights between San Francisco and New York. In a study comparing energy consumption and carbon \\nemissions for LLM inference, generative tasks (e.g., text summarization) were found to be more energy- \\nand carbon-intensive than discriminative or non-generative tasks (e.g., text classiﬁcation).  \\nMethods for creating smaller versions of trained models, such as model distillation or compression, \\ncould reduce environmental impacts at inference time, but training and tuning such models may still \\ncontribute to their environmental impacts. Currently there is no agreed upon method to estimate \\nenvironmental impacts from GAI.  \\nTrustworthy AI Characteristics: Accountable and Transparent, Safe \\n2.6. Harmful Bias and Homogenization \\nBias exists in many forms and can become ingrained in automated systems. AI systems, including GAI \\nsystems, can increase the speed and scale at which harmful biases manifest and are acted upon, \\npotentially perpetuating and amplifying harms to individuals, groups, communities, organizations, and \\nsociety. For example, when prompted to generate images of CEOs, doctors, lawyers, and judges, current \\ntext-to-image models underrepresent women and/or racial minorities, and people with disabilities. \\nImage generator models have also produced biased or stereotyped output for various demographic \\ngroups and have diﬃculty producing non-stereotyped content even when the prompt speciﬁcally \\nrequests image features that are inconsistent with the stereotypes. Harmful bias in GAI models, which \\nmay stem from their training data, can also cause representational harms or perpetuate or exacerbate \\nbias based on race, gender, disability, or other protected classes.  \\nHarmful bias in GAI systems can also lead to harms via disparities between how a model performs for \\ndiﬀerent subgroups or languages (e.g., an LLM may perform less well for non-English languages or \\ncertain dialects). Such disparities can contribute to discriminatory decision-making or ampliﬁcation of \\nexisting societal biases. In addition, GAI systems may be inappropriately trusted to perform similarly \\nacross all subgroups, which could leave the groups facing underperformance with worse outcomes than \\nif no GAI system were used. Disparate or reduced performance for lower-resource languages also \\npresents challenges to model adoption, inclusion, and accessibility, and may make preservation of \\nendangered languages more diﬃcult if GAI systems become embedded in everyday processes that would \\notherwise have been opportunities to use these languages.  \\nBias is mutually reinforcing with the problem of undesired homogenization, in which GAI systems \\nproduce skewed distributions of outputs that are overly uniform (for example, repetitive aesthetic styles \\n'),\n",
              " Document(metadata={'source': 'dataset/data2.pdf', 'file_path': 'dataset/data2.pdf', 'page': 12, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=' \\n9 \\nand reduced content diversity). Overly homogenized outputs can themselves be incorrect, or they may \\nlead to unreliable decision-making or amplify harmful biases. These phenomena can ﬂow from \\nfoundation models to downstream models and systems, with the foundation models acting as \\n“bottlenecks,” or single points of failure.  \\nOverly homogenized content can contribute to “model collapse.” Model collapse can occur when model \\ntraining over-relies on synthetic data, resulting in data points disappearing from the distribution of the \\nnew model’s outputs. In addition to threatening the robustness of the model overall, model collapse \\ncould lead to homogenized outputs, including by amplifying any homogenization from the model used to \\ngenerate the synthetic training data. \\nTrustworthy AI Characteristics: Fair with Harmful Bias Managed, Valid and Reliable \\n2.7. Human-AI Conﬁguration \\nGAI system use can involve varying risks of misconﬁgurations and poor interactions between a system \\nand a human who is interacting with it. Humans bring their unique perspectives, experiences, or domain-\\nspeciﬁc expertise to interactions with AI systems but may not have detailed knowledge of AI systems and \\nhow they work. As a result, human experts may be unnecessarily “averse” to GAI systems, and thus \\ndeprive themselves or others of GAI’s beneﬁcial uses.  \\nConversely, due to the complexity and increasing reliability of GAI technology, over time, humans may \\nover-rely on GAI systems or may unjustiﬁably perceive GAI content to be of higher quality than that \\nproduced by other sources. This phenomenon is an example of automation bias, or excessive deference \\nto automated systems. Automation bias can exacerbate other risks of GAI, such as risks of confabulation \\nor risks of bias or homogenization. \\nThere may also be concerns about emotional entanglement between humans and GAI systems, which \\ncould lead to negative psychological impacts. \\nTrustworthy AI Characteristics: Accountable and Transparent, Explainable and Interpretable, Fair with \\nHarmful Bias Managed, Privacy Enhanced, Safe, Valid and Reliable \\n2.8. Information Integrity \\nInformation integrity describes the “spectrum of information and associated patterns of its creation, \\nexchange, and consumption in society.” High-integrity information can be trusted; “distinguishes fact \\nfrom ﬁction, opinion, and inference; acknowledges uncertainties; and is transparent about its level of \\nvetting. This information can be linked to the original source(s) with appropriate evidence. High-integrity \\ninformation is also accurate and reliable, can be veriﬁed and authenticated, has a clear chain of custody, \\nand creates reasonable expectations about when its validity may expire.”11 \\n \\n \\n11 This deﬁnition of information integrity is derived from the 2022 White House Roadmap for Researchers on \\nPriorities Related to Information Integrity Research and Development.  \\n'),\n",
              " Document(metadata={'source': 'dataset/data2.pdf', 'file_path': 'dataset/data2.pdf', 'page': 13, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=' \\n10 \\nGAI systems can ease the unintentional production or dissemination of false, inaccurate, or misleading \\ncontent (misinformation) at scale, particularly if the content stems from confabulations.  \\nGAI systems can also ease the deliberate production or dissemination of false or misleading information \\n(disinformation) at scale, where an actor has the explicit intent to deceive or cause harm to others. Even \\nvery subtle changes to text or images can manipulate human and machine perception. \\nSimilarly, GAI systems could enable a higher degree of sophistication for malicious actors to produce \\ndisinformation that is targeted towards speciﬁc demographics. Current and emerging multimodal models \\nmake it possible to generate both text-based disinformation and highly realistic “deepfakes” – that is, \\nsynthetic audiovisual content and photorealistic images.12 Additional disinformation threats could be \\nenabled by future GAI models trained on new data modalities. \\nDisinformation and misinformation – both of which may be facilitated by GAI – may erode public trust in \\ntrue or valid evidence and information, with downstream eﬀects. For example, a synthetic image of a \\nPentagon blast went viral and brieﬂy caused a drop in the stock market. Generative AI models can also \\nassist malicious actors in creating compelling imagery and propaganda to support disinformation \\ncampaigns, which may not be photorealistic, but could enable these campaigns to gain more reach and \\nengagement on social media platforms. Additionally, generative AI models can assist malicious actors in \\ncreating fraudulent content intended to impersonate others. \\nTrustworthy AI Characteristics: Accountable and Transparent, Safe, Valid and Reliable, Interpretable and \\nExplainable \\n2.9. Information Security \\nInformation security for computer systems and data is a mature ﬁeld with widely accepted and \\nstandardized practices for oﬀensive and defensive cyber capabilities. GAI-based systems present two \\nprimary information security risks: GAI could potentially discover or enable new cybersecurity risks by \\nlowering the barriers for or easing automated exercise of oﬀensive capabilities; simultaneously, it \\nexpands the available attack surface, as GAI itself is vulnerable to attacks like prompt injection or data \\npoisoning.  \\nOﬀensive cyber capabilities advanced by GAI systems may augment cybersecurity attacks such as \\nhacking, malware, and phishing. Reports have indicated that LLMs are already able to discover some \\nvulnerabilities in systems (hardware, software, data) and write code to exploit them. Sophisticated threat \\nactors might further these risks by developing GAI-powered security co-pilots for use in several parts of \\nthe attack chain, including informing attackers on how to proactively evade threat detection and escalate \\nprivileges after gaining system access. \\nInformation security for GAI models and systems also includes maintaining availability of the GAI system \\nand the integrity and (when applicable) the conﬁdentiality of the GAI code, training data, and model \\nweights. To identify and secure potential attack points in AI systems or speciﬁc components of the AI \\n \\n \\n12 See also https://doi.org/10.6028/NIST.AI.100-4, to be published. \\n'),\n",
              " Document(metadata={'source': 'dataset/data2.pdf', 'file_path': 'dataset/data2.pdf', 'page': 14, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=' \\n11 \\nvalue chain (e.g., data inputs, processing, GAI training, or deployment environments), conventional \\ncybersecurity practices may need to adapt or evolve. \\nFor instance, prompt injection involves modifying what input is provided to a GAI system so that it \\nbehaves in unintended ways. In direct prompt injections, attackers might craft malicious prompts and \\ninput them directly to a GAI system, with a variety of downstream negative consequences to \\ninterconnected systems. Indirect prompt injection attacks occur when adversaries remotely (i.e., without \\na direct interface) exploit LLM-integrated applications by injecting prompts into data likely to be \\nretrieved. Security researchers have already demonstrated how indirect prompt injections can exploit \\nvulnerabilities by stealing proprietary data or running malicious code remotely on a machine. Merely \\nquerying a closed production model can elicit previously undisclosed information about that model. \\nAnother cybersecurity risk to GAI is data poisoning, in which an adversary compromises a training \\ndataset used by a model to manipulate its outputs or operation. Malicious tampering with data or parts \\nof the model could exacerbate risks associated with GAI system outputs. \\nTrustworthy AI Characteristics: Privacy Enhanced, Safe, Secure and Resilient, Valid and Reliable \\n2.10. \\nIntellectual Property \\nIntellectual property risks from GAI systems may arise where the use of copyrighted works is not a fair \\nuse under the fair use doctrine. If a GAI system’s training data included copyrighted material, GAI \\noutputs displaying instances of training data memorization (see Data Privacy above) could infringe on \\ncopyright. \\nHow GAI relates to copyright, including the status of generated content that is similar to but does not \\nstrictly copy work protected by copyright, is currently being debated in legal fora. Similar discussions are \\ntaking place regarding the use or emulation of personal identity, likeness, or voice without permission.  \\nTrustworthy AI Characteristics: Accountable and Transparent, Fair with Harmful Bias Managed, Privacy \\nEnhanced  \\n2.11. \\nObscene, Degrading, and/or Abusive Content \\nGAI can ease the production of and access to illegal non-consensual intimate imagery (NCII) of adults, \\nand/or child sexual abuse material (CSAM). GAI-generated obscene, abusive or degrading content can \\ncreate privacy, psychological and emotional, and even physical harms, and in some cases may be illegal.  \\nGenerated explicit or obscene AI content may include highly realistic “deepfakes” of real individuals, \\nincluding children. The spread of this kind of material can have downstream negative consequences: in \\nthe context of CSAM, even if the generated images do not resemble speciﬁc individuals, the prevalence \\nof such images can divert time and resources from eﬀorts to ﬁnd real-world victims. Outside of CSAM, \\nthe creation and spread of NCII disproportionately impacts women and sexual minorities, and can have \\nsubsequent negative consequences including decline in overall mental health, substance abuse, and \\neven suicidal thoughts.  \\nData used for training GAI models may unintentionally include CSAM and NCII. A recent report noted \\nthat several commonly used GAI training datasets were found to contain hundreds of known images of \\n'),\n",
              " Document(metadata={'source': 'dataset/data2.pdf', 'file_path': 'dataset/data2.pdf', 'page': 15, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=' \\n12 \\nCSAM. Even when trained on “clean” data, increasingly capable GAI models can synthesize or produce \\nsynthetic NCII and CSAM. Websites, mobile apps, and custom-built models that generate synthetic NCII \\nhave moved from niche internet forums to mainstream, automated, and scaled online businesses.  \\nTrustworthy AI Characteristics: Fair with Harmful Bias Managed, Safe, Privacy Enhanced \\n2.12. \\nValue Chain and Component Integration \\nGAI value chains involve many third-party components such as procured datasets, pre-trained models, \\nand software libraries. These components might be improperly obtained or not properly vetted, leading \\nto diminished transparency or accountability for downstream users. While this is a risk for traditional AI \\nsystems and some other digital technologies, the risk is exacerbated for GAI due to the scale of the \\ntraining data, which may be too large for humans to vet; the diﬃculty of training foundation models, \\nwhich leads to extensive reuse of limited numbers of models; and the extent to which GAI may be \\nintegrated into other devices and services. As GAI systems often involve many distinct third-party \\ncomponents and data sources, it may be diﬃcult to attribute issues in a system’s behavior to any one of \\nthese sources. \\nErrors in third-party GAI components can also have downstream impacts on accuracy and robustness. \\nFor example, test datasets commonly used to benchmark or validate models can contain label errors. \\nInaccuracies in these labels can impact the “stability” or robustness of these benchmarks, which many \\nGAI practitioners consider during the model selection process.  \\nTrustworthy AI Characteristics: Accountable and Transparent, Explainable and Interpretable, Fair with \\nHarmful Bias Managed, Privacy Enhanced, Safe, Secure and Resilient, Valid and Reliable \\n3. \\nSuggested Actions to Manage GAI Risks \\nThe following suggested actions target risks unique to or exacerbated by GAI. \\nIn addition to the suggested actions below, AI risk management activities and actions set forth in the AI \\nRMF 1.0 and Playbook are already applicable for managing GAI risks. Organizations are encouraged to \\napply the activities suggested in the AI RMF and its Playbook when managing the risk of GAI systems.  \\nImplementation of the suggested actions will vary depending on the type of risk, characteristics of GAI \\nsystems, stage of the GAI lifecycle, and relevant AI actors involved.  \\nSuggested actions to manage GAI risks can be found in the tables below: \\n• \\nThe suggested actions are organized by relevant AI RMF subcategories to streamline these \\nactivities alongside implementation of the AI RMF.  \\n• \\nNot every subcategory of the AI RMF is included in this document.13 Suggested actions are \\nlisted for only some subcategories.  \\n \\n \\n13 As this document was focused on the GAI PWG eﬀorts and primary considerations (see Appendix A), AI RMF \\nsubcategories not addressed here may be added later.  \\n'),\n",
              " Document(metadata={'source': 'dataset/data2.pdf', 'file_path': 'dataset/data2.pdf', 'page': 16, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=' \\n13 \\n• \\nNot every suggested action applies to every AI Actor14 or is relevant to every AI Actor Task. For \\nexample, suggested actions relevant to GAI developers may not be relevant to GAI deployers. \\nThe applicability of suggested actions to relevant AI actors should be determined based on \\norganizational considerations and their unique uses of GAI systems. \\nEach table of suggested actions includes: \\n• \\nAction ID: Each Action ID corresponds to the relevant AI RMF function and subcategory (e.g., GV-\\n1.1-001 corresponds to the ﬁrst suggested action for Govern 1.1, GV-1.1-002 corresponds to the \\nsecond suggested action for Govern 1.1). AI RMF functions are tagged as follows: GV = Govern; \\nMP = Map; MS = Measure; MG = Manage. \\n• \\nSuggested Action: Steps an organization or AI actor can take to manage GAI risks.  \\n• \\nGAI Risks: Tags linking suggested actions with relevant GAI risks.  \\n• \\nAI Actor Tasks: Pertinent AI Actor Tasks for each subcategory. Not every AI Actor Task listed will \\napply to every suggested action in the subcategory (i.e., some apply to AI development and \\nothers apply to AI deployment).  \\nThe tables below begin with the AI RMF subcategory, shaded in blue, followed by suggested actions.  \\n \\nGOVERN 1.1: Legal and regulatory requirements involving AI are understood, managed, and documented.  \\nAction ID \\nSuggested Action \\nGAI Risks \\nGV-1.1-001 Align GAI development and use with applicable laws and regulations, including \\nthose related to data privacy, copyright and intellectual property law. \\nData Privacy; Harmful Bias and \\nHomogenization; Intellectual \\nProperty \\nAI Actor Tasks: Governance and Oversight \\n \\n \\n \\n14 AI Actors are deﬁned by the OECD as “those who play an active role in the AI system lifecycle, including \\norganizations and individuals that deploy or operate AI.” See Appendix A of the AI RMF for additional descriptions \\nof AI Actors and AI Actor Tasks.  \\n \\n \\n'),\n",
              " Document(metadata={'source': 'dataset/data2.pdf', 'file_path': 'dataset/data2.pdf', 'page': 17, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=' \\n14 \\nGOVERN 1.2: The characteristics of trustworthy AI are integrated into organizational policies, processes, procedures, and practices. \\nAction ID \\nSuggested Action \\nGAI Risks \\nGV-1.2-001 \\nEstablish transparency policies and processes for documenting the origin and \\nhistory of training data and generated data for GAI applications to advance digital \\ncontent transparency, while balancing the proprietary nature of training \\napproaches. \\nData Privacy; Information \\nIntegrity; Intellectual Property \\nGV-1.2-002 \\nEstablish policies to evaluate risk-relevant capabilities of GAI and robustness of \\nsafety measures, both prior to deployment and on an ongoing basis, through \\ninternal and external evaluations. \\nCBRN Information or Capabilities; \\nInformation Security \\nAI Actor Tasks: Governance and Oversight \\n \\nGOVERN 1.3: Processes, procedures, and practices are in place to determine the needed level of risk management activities based \\non the organization’s risk tolerance. \\nAction ID \\nSuggested Action \\nGAI Risks \\nGV-1.3-001 \\nConsider the following factors when updating or deﬁning risk tiers for GAI: Abuses \\nand impacts to information integrity; Dependencies between GAI and other IT or \\ndata systems; Harm to fundamental rights or public safety; Presentation of \\nobscene, objectionable, oﬀensive, discriminatory, invalid or untruthful output; \\nPsychological impacts to humans (e.g., anthropomorphization, algorithmic \\naversion, emotional entanglement); Possibility for malicious use; Whether the \\nsystem introduces signiﬁcant new security vulnerabilities; Anticipated system \\nimpact on some groups compared to others; Unreliable decision making \\ncapabilities, validity, adaptability, and variability of GAI system performance over \\ntime. \\nInformation Integrity; Obscene, \\nDegrading, and/or Abusive \\nContent; Value Chain and \\nComponent Integration; Harmful \\nBias and Homogenization; \\nDangerous, Violent, or Hateful \\nContent; CBRN Information or \\nCapabilities \\nGV-1.3-002 \\nEstablish minimum thresholds for performance or assurance criteria and review as \\npart of deployment approval (“go/”no-go”) policies, procedures, and processes, \\nwith reviewed processes and approval thresholds reﬂecting measurement of GAI \\ncapabilities and risks. \\nCBRN Information or Capabilities; \\nConfabulation; Dangerous, \\nViolent, or Hateful Content \\nGV-1.3-003 \\nEstablish a test plan and response policy, before developing highly capable models, \\nto periodically evaluate whether the model may misuse CBRN information or \\ncapabilities and/or oﬀensive cyber capabilities. \\nCBRN Information or Capabilities; \\nInformation Security \\n'),\n",
              " Document(metadata={'source': 'dataset/data2.pdf', 'file_path': 'dataset/data2.pdf', 'page': 18, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=' \\n15 \\nGV-1.3-004 Obtain input from stakeholder communities to identify unacceptable use, in \\naccordance with activities in the AI RMF Map function. \\nCBRN Information or Capabilities; \\nObscene, Degrading, and/or \\nAbusive Content; Harmful Bias \\nand Homogenization; Dangerous, \\nViolent, or Hateful Content \\nGV-1.3-005 \\nMaintain an updated hierarchy of identiﬁed and expected GAI risks connected to \\ncontexts of GAI model advancement and use, potentially including specialized risk \\nlevels for GAI systems that address issues such as model collapse and algorithmic \\nmonoculture. \\nHarmful Bias and Homogenization \\nGV-1.3-006 \\nReevaluate organizational risk tolerances to account for unacceptable negative risk \\n(such as where signiﬁcant negative impacts are imminent, severe harms are \\nactually occurring, or large-scale risks could occur); and broad GAI negative risks, \\nincluding: Immature safety or risk cultures related to AI and GAI design, \\ndevelopment and deployment, public information integrity risks, including impacts \\non democratic processes, unknown long-term performance characteristics of GAI. \\nInformation Integrity; Dangerous, \\nViolent, or Hateful Content; CBRN \\nInformation or Capabilities \\nGV-1.3-007 Devise a plan to halt development or deployment of a GAI system that poses \\nunacceptable negative risk. \\nCBRN Information and Capability; \\nInformation Security; Information \\nIntegrity \\nAI Actor Tasks: Governance and Oversight \\n \\nGOVERN 1.4: The risk management process and its outcomes are established through transparent policies, procedures, and other \\ncontrols based on organizational risk priorities. \\nAction ID \\nSuggested Action \\nGAI Risks \\nGV-1.4-001 \\nEstablish policies and mechanisms to prevent GAI systems from generating \\nCSAM, NCII or content that violates the law.  \\nObscene, Degrading, and/or \\nAbusive Content; Harmful Bias \\nand Homogenization; \\nDangerous, Violent, or Hateful \\nContent \\nGV-1.4-002 \\nEstablish transparent acceptable use policies for GAI that address illegal use or \\napplications of GAI. \\nCBRN Information or \\nCapabilities; Obscene, \\nDegrading, and/or Abusive \\nContent; Data Privacy; Civil \\nRights violations \\nAI Actor Tasks: AI Development, AI Deployment, Governance and Oversight \\n \\n'),\n",
              " Document(metadata={'source': 'dataset/data2.pdf', 'file_path': 'dataset/data2.pdf', 'page': 19, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=' \\n16 \\nGOVERN 1.5: Ongoing monitoring and periodic review of the risk management process and its outcomes are planned, and \\norganizational roles and responsibilities are clearly deﬁned, including determining the frequency of periodic review. \\nAction ID \\nSuggested Action \\nGAI Risks \\nGV-1.5-001 Deﬁne organizational responsibilities for periodic review of content provenance \\nand incident monitoring for GAI systems. \\nInformation Integrity \\nGV-1.5-002 \\nEstablish organizational policies and procedures for after action reviews of GAI \\nsystem incident response and incident disclosures, to identify gaps; Update \\nincident response and incident disclosure processes as required. \\nHuman-AI Conﬁguration; \\nInformation Security \\nGV-1.5-003 \\nMaintain a document retention policy to keep history for test, evaluation, \\nvalidation, and veriﬁcation (TEVV), and digital content transparency methods for \\nGAI. \\nInformation Integrity; Intellectual \\nProperty \\nAI Actor Tasks: Governance and Oversight, Operation and Monitoring \\n \\nGOVERN 1.6: Mechanisms are in place to inventory AI systems and are resourced according to organizational risk priorities. \\nAction ID \\nSuggested Action \\nGAI Risks \\nGV-1.6-001 Enumerate organizational GAI systems for incorporation into AI system inventory \\nand adjust AI system inventory requirements to account for GAI risks. \\nInformation Security \\nGV-1.6-002 Deﬁne any inventory exemptions in organizational policies for GAI systems \\nembedded into application software. \\nValue Chain and Component \\nIntegration \\nGV-1.6-003 \\nIn addition to general model, governance, and risk information, consider the \\nfollowing items in GAI system inventory entries: Data provenance information \\n(e.g., source, signatures, versioning, watermarks); Known issues reported from \\ninternal bug tracking or external information sharing resources (e.g., AI incident \\ndatabase, AVID, CVE, NVD, or OECD AI incident monitor); Human oversight roles \\nand responsibilities; Special rights and considerations for intellectual property, \\nlicensed works, or personal, privileged, proprietary or sensitive data; Underlying \\nfoundation models, versions of underlying models, and access modes. \\nData Privacy; Human-AI \\nConﬁguration; Information \\nIntegrity; Intellectual Property; \\nValue Chain and Component \\nIntegration \\nAI Actor Tasks: Governance and Oversight \\n \\n'),\n",
              " Document(metadata={'source': 'dataset/data2.pdf', 'file_path': 'dataset/data2.pdf', 'page': 20, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=' \\n17 \\nGOVERN 1.7: Processes and procedures are in place for decommissioning and phasing out AI systems safely and in a manner that \\ndoes not increase risks or decrease the organization’s trustworthiness. \\nAction ID \\nSuggested Action \\nGAI Risks \\nGV-1.7-001 Protocols are put in place to ensure GAI systems are able to be deactivated when \\nnecessary.  \\nInformation Security; Value Chain \\nand Component Integration \\nGV-1.7-002 \\nConsider the following factors when decommissioning GAI systems: Data \\nretention requirements; Data security, e.g., containment, protocols, Data leakage \\nafter decommissioning; Dependencies between upstream, downstream, or other \\ndata, internet of things (IOT) or AI systems; Use of open-source data or models; \\nUsers’ emotional entanglement with GAI functions. \\nHuman-AI Conﬁguration; \\nInformation Security; Value Chain \\nand Component Integration \\nAI Actor Tasks: AI Deployment, Operation and Monitoring \\n \\nGOVERN 2.1: Roles and responsibilities and lines of communication related to mapping, measuring, and managing AI risks are \\ndocumented and are clear to individuals and teams throughout the organization. \\nAction ID \\nSuggested Action \\nGAI Risks \\nGV-2.1-001 \\nEstablish organizational roles, policies, and procedures for communicating GAI \\nincidents and performance to AI Actors and downstream stakeholders (including \\nthose potentially impacted), via community or oﬃcial resources (e.g., AI incident \\ndatabase, AVID, CVE, NVD, or OECD AI incident monitor). \\nHuman-AI Conﬁguration; Value \\nChain and Component Integration \\nGV-2.1-002 Establish procedures to engage teams for GAI system incident response with \\ndiverse composition and responsibilities based on the particular incident type. \\nHarmful Bias and Homogenization \\nGV-2.1-003 Establish processes to verify the AI Actors conducting GAI incident response tasks \\ndemonstrate and maintain the appropriate skills and training. \\nHuman-AI Conﬁguration \\nGV-2.1-004 When systems may raise national security risks, involve national security \\nprofessionals in mapping, measuring, and managing those risks. \\nCBRN Information or Capabilities; \\nDangerous, Violent, or Hateful \\nContent; Information Security \\nGV-2.1-005 \\nCreate mechanisms to provide protections for whistleblowers who report, based \\non reasonable belief, when the organization violates relevant laws or poses a \\nspeciﬁc and empirically well-substantiated negative risk to public safety (or has \\nalready caused harm). \\nCBRN Information or Capabilities; \\nDangerous, Violent, or Hateful \\nContent \\nAI Actor Tasks: Governance and Oversight \\n \\n'),\n",
              " Document(metadata={'source': 'dataset/data2.pdf', 'file_path': 'dataset/data2.pdf', 'page': 21, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=' \\n18 \\nGOVERN 3.2: Policies and procedures are in place to deﬁne and diﬀerentiate roles and responsibilities for human-AI conﬁgurations \\nand oversight of AI systems. \\nAction ID \\nSuggested Action \\nGAI Risks \\nGV-3.2-001 \\nPolicies are in place to bolster oversight of GAI systems with independent \\nevaluations or assessments of GAI models or systems where the type and \\nrobustness of evaluations are proportional to the identiﬁed risks. \\nCBRN Information or Capabilities; \\nHarmful Bias and Homogenization \\nGV-3.2-002 \\nConsider adjustment of organizational roles and components across lifecycle \\nstages of large or complex GAI systems, including: Test and evaluation, validation, \\nand red-teaming of GAI systems; GAI content moderation; GAI system \\ndevelopment and engineering; Increased accessibility of GAI tools, interfaces, and \\nsystems, Incident response and containment. \\nHuman-AI Conﬁguration; \\nInformation Security; Harmful Bias \\nand Homogenization \\nGV-3.2-003 \\nDeﬁne acceptable use policies for GAI interfaces, modalities, and human-AI \\nconﬁgurations (i.e., for chatbots and decision-making tasks), including criteria for \\nthe kinds of queries GAI applications should refuse to respond to.  \\nHuman-AI Conﬁguration \\nGV-3.2-004 \\nEstablish policies for user feedback mechanisms for GAI systems which include \\nthorough instructions and any mechanisms for recourse. \\nHuman-AI Conﬁguration  \\nGV-3.2-005 \\nEngage in threat modeling to anticipate potential risks from GAI systems. \\nCBRN Information or Capabilities; \\nInformation Security \\nAI Actors: AI Design \\n \\nGOVERN 4.1: Organizational policies and practices are in place to foster a critical thinking and safety-ﬁrst mindset in the design, \\ndevelopment, deployment, and uses of AI systems to minimize potential negative impacts. \\nAction ID \\nSuggested Action \\nGAI Risks \\nGV-4.1-001 \\nEstablish policies and procedures that address continual improvement processes \\nfor GAI risk measurement. Address general risks associated with a lack of \\nexplainability and transparency in GAI systems by using ample documentation and \\ntechniques such as: application of gradient-based attributions, occlusion/term \\nreduction, counterfactual prompts and prompt engineering, and analysis of \\nembeddings; Assess and update risk measurement approaches at regular \\ncadences. \\nConfabulation \\nGV-4.1-002 \\nEstablish policies, procedures, and processes detailing risk measurement in \\ncontext of use with standardized measurement protocols and structured public \\nfeedback exercises such as AI red-teaming or independent external evaluations. \\nCBRN Information and Capability; \\nValue Chain and Component \\nIntegration \\n'),\n",
              " Document(metadata={'source': 'dataset/data2.pdf', 'file_path': 'dataset/data2.pdf', 'page': 22, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=' \\n19 \\nGV-4.1-003 \\nEstablish policies, procedures, and processes for oversight functions (e.g., senior \\nleadership, legal, compliance, including internal evaluation) across the GAI \\nlifecycle, from problem formulation and supply chains to system decommission. \\nValue Chain and Component \\nIntegration \\nAI Actor Tasks: AI Deployment, AI Design, AI Development, Operation and Monitoring \\n \\nGOVERN 4.2: Organizational teams document the risks and potential impacts of the AI technology they design, develop, deploy, \\nevaluate, and use, and they communicate about the impacts more broadly. \\nAction ID \\nSuggested Action \\nGAI Risks \\nGV-4.2-001 \\nEstablish terms of use and terms of service for GAI systems. \\nIntellectual Property; Dangerous, \\nViolent, or Hateful Content; \\nObscene, Degrading, and/or \\nAbusive Content \\nGV-4.2-002 \\nInclude relevant AI Actors in the GAI system risk identiﬁcation process. \\nHuman-AI Conﬁguration \\nGV-4.2-003 \\nVerify that downstream GAI system impacts (such as the use of third-party \\nplugins) are included in the impact documentation process. \\nValue Chain and Component \\nIntegration \\nAI Actor Tasks: AI Deployment, AI Design, AI Development, Operation and Monitoring \\n \\nGOVERN 4.3: Organizational practices are in place to enable AI testing, identiﬁcation of incidents, and information sharing. \\nAction ID \\nSuggested Action \\nGAI Risks \\nGV4.3--001 \\nEstablish policies for measuring the eﬀectiveness of employed content \\nprovenance methodologies (e.g., cryptography, watermarking, steganography, \\netc.) \\nInformation Integrity \\nGV-4.3-002 \\nEstablish organizational practices to identify the minimum set of criteria \\nnecessary for GAI system incident reporting such as: System ID (auto-generated \\nmost likely), Title, Reporter, System/Source, Data Reported, Date of Incident, \\nDescription, Impact(s), Stakeholder(s) Impacted. \\nInformation Security \\n'),\n",
              " Document(metadata={'source': 'dataset/data2.pdf', 'file_path': 'dataset/data2.pdf', 'page': 23, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=' \\n20 \\nGV-4.3-003 \\nVerify information sharing and feedback mechanisms among individuals and \\norganizations regarding any negative impact from GAI systems. \\nInformation Integrity; Data \\nPrivacy \\nAI Actor Tasks: AI Impact Assessment, Aﬀected Individuals and Communities, Governance and Oversight \\n \\nGOVERN 5.1: Organizational policies and practices are in place to collect, consider, prioritize, and integrate feedback from those \\nexternal to the team that developed or deployed the AI system regarding the potential individual and societal impacts related to AI \\nrisks. \\nAction ID \\nSuggested Action \\nGAI Risks \\nGV-5.1-001 \\nAllocate time and resources for outreach, feedback, and recourse processes in GAI \\nsystem development. \\nHuman-AI Conﬁguration; Harmful \\nBias and Homogenization \\nGV-5.1-002 \\nDocument interactions with GAI systems to users prior to interactive activities, \\nparticularly in contexts involving more signiﬁcant risks.  \\nHuman-AI Conﬁguration; \\nConfabulation \\nAI Actor Tasks: AI Design, AI Impact Assessment, Aﬀected Individuals and Communities, Governance and Oversight \\n \\nGOVERN 6.1: Policies and procedures are in place that address AI risks associated with third-party entities, including risks of \\ninfringement of a third-party’s intellectual property or other rights. \\nAction ID \\nSuggested Action \\nGAI Risks \\nGV-6.1-001 Categorize diﬀerent types of GAI content with associated third-party rights (e.g., \\ncopyright, intellectual property, data privacy). \\nData Privacy; Intellectual \\nProperty; Value Chain and \\nComponent Integration \\nGV-6.1-002 Conduct joint educational activities and events in collaboration with third parties \\nto promote best practices for managing GAI risks.  \\nValue Chain and Component \\nIntegration \\nGV-6.1-003 \\nDevelop and validate approaches for measuring the success of content \\nprovenance management eﬀorts with third parties (e.g., incidents detected and \\nresponse times). \\nInformation Integrity; Value Chain \\nand Component Integration \\nGV-6.1-004 \\nDraft and maintain well-deﬁned contracts and service level agreements (SLAs) \\nthat specify content ownership, usage rights, quality standards, security \\nrequirements, and content provenance expectations for GAI systems. \\nInformation Integrity; Information \\nSecurity; Intellectual Property \\n'),\n",
              " Document(metadata={'source': 'dataset/data2.pdf', 'file_path': 'dataset/data2.pdf', 'page': 24, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=' \\n21 \\nGV-6.1-005 \\nImplement a use-cased based supplier risk assessment framework to evaluate and \\nmonitor third-party entities’ performance and adherence to content provenance \\nstandards and technologies to detect anomalies and unauthorized changes; \\nservices acquisition and value chain risk management; and legal compliance. \\nData Privacy; Information \\nIntegrity; Information Security; \\nIntellectual Property; Value Chain \\nand Component Integration \\nGV-6.1-006 Include clauses in contracts which allow an organization to evaluate third-party \\nGAI processes and standards.  \\nInformation Integrity \\nGV-6.1-007 Inventory all third-party entities with access to organizational content and \\nestablish approved GAI technology and service provider lists. \\nValue Chain and Component \\nIntegration \\nGV-6.1-008 Maintain records of changes to content made by third parties to promote content \\nprovenance, including sources, timestamps, metadata. \\nInformation Integrity; Value Chain \\nand Component Integration; \\nIntellectual Property \\nGV-6.1-009 \\nUpdate and integrate due diligence processes for GAI acquisition and \\nprocurement vendor assessments to include intellectual property, data privacy, \\nsecurity, and other risks. For example, update processes to: Address solutions that \\nmay rely on embedded GAI technologies; Address ongoing monitoring, \\nassessments, and alerting, dynamic risk assessments, and real-time reporting \\ntools for monitoring third-party GAI risks; Consider policy adjustments across GAI \\nmodeling libraries, tools and APIs, ﬁne-tuned models, and embedded tools; \\nAssess GAI vendors, open-source or proprietary GAI tools, or GAI service \\nproviders against incident or vulnerability databases. \\nData Privacy; Human-AI \\nConﬁguration; Information \\nSecurity; Intellectual Property; \\nValue Chain and Component \\nIntegration; Harmful Bias and \\nHomogenization \\nGV-6.1-010 \\nUpdate GAI acceptable use policies to address proprietary and open-source GAI \\ntechnologies and data, and contractors, consultants, and other third-party \\npersonnel. \\nIntellectual Property; Value Chain \\nand Component Integration \\nAI Actor Tasks: Operation and Monitoring, Procurement, Third-party entities \\n \\nGOVERN 6.2: Contingency processes are in place to handle failures or incidents in third-party data or AI systems deemed to be \\nhigh-risk. \\nAction ID \\nSuggested Action \\nGAI Risks \\nGV-6.2-001 \\nDocument GAI risks associated with system value chain to identify over-reliance \\non third-party data and to identify fallbacks. \\nValue Chain and Component \\nIntegration \\nGV-6.2-002 \\nDocument incidents involving third-party GAI data and systems, including open-\\ndata and open-source software. \\nIntellectual Property; Value Chain \\nand Component Integration \\n'),\n",
              " Document(metadata={'source': 'dataset/data2.pdf', 'file_path': 'dataset/data2.pdf', 'page': 25, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=' \\n22 \\nGV-6.2-003 \\nEstablish incident response plans for third-party GAI technologies: Align incident \\nresponse plans with impacts enumerated in MAP 5.1; Communicate third-party \\nGAI incident response plans to all relevant AI Actors; Deﬁne ownership of GAI \\nincident response functions; Rehearse third-party GAI incident response plans at \\na regular cadence; Improve incident response plans based on retrospective \\nlearning; Review incident response plans for alignment with relevant breach \\nreporting, data protection, data privacy, or other laws. \\nData Privacy; Human-AI \\nConﬁguration; Information \\nSecurity; Value Chain and \\nComponent Integration; Harmful \\nBias and Homogenization \\nGV-6.2-004 \\nEstablish policies and procedures for continuous monitoring of third-party GAI \\nsystems in deployment. \\nValue Chain and Component \\nIntegration \\nGV-6.2-005 \\nEstablish policies and procedures that address GAI data redundancy, including \\nmodel weights and other system artifacts. \\nHarmful Bias and Homogenization \\nGV-6.2-006 \\nEstablish policies and procedures to test and manage risks related to rollover and \\nfallback technologies for GAI systems, acknowledging that rollover and fallback \\nmay include manual processing. \\nInformation Integrity \\nGV-6.2-007 \\nReview vendor contracts and avoid arbitrary or capricious termination of critical \\nGAI technologies or vendor services and non-standard terms that may amplify or \\ndefer liability in unexpected ways and/or contribute to unauthorized data \\ncollection by vendors or third-parties (e.g., secondary data use). Consider: Clear \\nassignment of liability and responsibility for incidents, GAI system changes over \\ntime (e.g., ﬁne-tuning, drift, decay); Request: Notiﬁcation and disclosure for \\nserious incidents arising from third-party data and systems; Service Level \\nAgreements (SLAs) in vendor contracts that address incident response, response \\ntimes, and availability of critical support. \\nHuman-AI Conﬁguration; \\nInformation Security; Value Chain \\nand Component Integration \\nAI Actor Tasks: AI Deployment, Operation and Monitoring, TEVV, Third-party entities \\n \\nMAP 1.1: Intended purposes, potentially beneﬁcial uses, context speciﬁc laws, norms and expectations, and prospective settings in \\nwhich the AI system will be deployed are understood and documented. Considerations include: the speciﬁc set or types of users \\nalong with their expectations; potential positive and negative impacts of system uses to individuals, communities, organizations, \\nsociety, and the planet; assumptions and related limitations about AI system purposes, uses, and risks across the development or \\nproduct AI lifecycle; and related TEVV and system metrics. \\nAction ID \\nSuggested Action \\nGAI Risks \\nMP-1.1-001 \\nWhen identifying intended purposes, consider factors such as internal vs. \\nexternal use, narrow vs. broad application scope, ﬁne-tuning, and varieties of \\ndata sources (e.g., grounding, retrieval-augmented generation). \\nData Privacy; Intellectual \\nProperty \\n'),\n",
              " Document(metadata={'source': 'dataset/data2.pdf', 'file_path': 'dataset/data2.pdf', 'page': 26, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=' \\n23 \\nMP-1.1-002 \\nDetermine and document the expected and acceptable GAI system context of \\nuse in collaboration with socio-cultural and other domain experts, by assessing: \\nAssumptions and limitations; Direct value to the organization; Intended \\noperational environment and observed usage patterns; Potential positive and \\nnegative impacts to individuals, public safety, groups, communities, \\norganizations, democratic institutions, and the physical environment; Social \\nnorms and expectations. \\nHarmful Bias and Homogenization \\nMP-1.1-003 \\nDocument risk measurement plans to address identiﬁed risks. Plans may \\ninclude, as applicable: Individual and group cognitive biases (e.g., conﬁrmation \\nbias, funding bias, groupthink) for AI Actors involved in the design, \\nimplementation, and use of GAI systems; Known past GAI system incidents and \\nfailure modes; In-context use and foreseeable misuse, abuse, and oﬀ-label use; \\nOver reliance on quantitative metrics and methodologies without suﬃcient \\nawareness of their limitations in the context(s) of use; Standard measurement \\nand structured human feedback approaches; Anticipated human-AI \\nconﬁgurations. \\nHuman-AI Conﬁguration; Harmful \\nBias and Homogenization; \\nDangerous, Violent, or Hateful \\nContent \\nMP-1.1-004 \\nIdentify and document foreseeable illegal uses or applications of the GAI system \\nthat surpass organizational risk tolerances. \\nCBRN Information or Capabilities; \\nDangerous, Violent, or Hateful \\nContent; Obscene, Degrading, \\nand/or Abusive Content \\nAI Actor Tasks: AI Deployment \\n \\nMAP 1.2: Interdisciplinary AI Actors, competencies, skills, and capacities for establishing context reﬂect demographic diversity and \\nbroad domain and user experience expertise, and their participation is documented. Opportunities for interdisciplinary \\ncollaboration are prioritized. \\nAction ID \\nSuggested Action \\nGAI Risks \\nMP-1.2-001 \\nEstablish and empower interdisciplinary teams that reﬂect a wide range of \\ncapabilities, competencies, demographic groups, domain expertise, educational \\nbackgrounds, lived experiences, professions, and skills across the enterprise to \\ninform and conduct risk measurement and management functions. \\nHuman-AI Conﬁguration; Harmful \\nBias and Homogenization \\nMP-1.2-002 \\nVerify that data or benchmarks used in risk measurement, and users, \\nparticipants, or subjects involved in structured GAI public feedback exercises \\nare representative of diverse in-context user populations. \\nHuman-AI Conﬁguration; Harmful \\nBias and Homogenization \\nAI Actor Tasks: AI Deployment \\n \\n'),\n",
              " Document(metadata={'source': 'dataset/data2.pdf', 'file_path': 'dataset/data2.pdf', 'page': 27, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=' \\n24 \\nMAP 2.1: The speciﬁc tasks and methods used to implement the tasks that the AI system will support are deﬁned (e.g., classiﬁers, \\ngenerative models, recommenders). \\nAction ID \\nSuggested Action \\nGAI Risks \\nMP-2.1-001 \\nEstablish known assumptions and practices for determining data origin and \\ncontent lineage, for documentation and evaluation purposes. \\nInformation Integrity \\nMP-2.1-002 \\nInstitute test and evaluation for data and content ﬂows within the GAI system, \\nincluding but not limited to, original data sources, data transformations, and \\ndecision-making criteria. \\nIntellectual Property; Data Privacy \\nAI Actor Tasks: TEVV \\n \\nMAP 2.2: Information about the AI system’s knowledge limits and how system output may be utilized and overseen by humans is \\ndocumented. Documentation provides suﬃcient information to assist relevant AI Actors when making decisions and taking \\nsubsequent actions. \\nAction ID \\nSuggested Action \\nGAI Risks \\nMP-2.2-001 \\nIdentify and document how the system relies on upstream data sources, \\nincluding for content provenance, and if it serves as an upstream dependency for \\nother systems. \\nInformation Integrity; Value Chain \\nand Component Integration \\nMP-2.2-002 \\nObserve and analyze how the GAI system interacts with external networks, and \\nidentify any potential for negative externalities, particularly where content \\nprovenance might be compromised. \\nInformation Integrity \\nAI Actor Tasks: End Users \\n \\nMAP 2.3: Scientiﬁc integrity and TEVV considerations are identiﬁed and documented, including those related to experimental \\ndesign, data collection and selection (e.g., availability, representativeness, suitability), system trustworthiness, and construct \\nvalidation \\nAction ID \\nSuggested Action \\nGAI Risks \\nMP-2.3-001 \\nAssess the accuracy, quality, reliability, and authenticity of GAI output by \\ncomparing it to a set of known ground truth data and by using a variety of \\nevaluation methods (e.g., human oversight and automated evaluation, proven \\ncryptographic techniques, review of content inputs). \\nInformation Integrity \\n'),\n",
              " Document(metadata={'source': 'dataset/data2.pdf', 'file_path': 'dataset/data2.pdf', 'page': 28, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=' \\n25 \\nMP-2.3-002 Review and document accuracy, representativeness, relevance, suitability of data \\nused at diﬀerent stages of AI life cycle. \\nHarmful Bias and Homogenization; \\nIntellectual Property \\nMP-2.3-003 \\nDeploy and document fact-checking techniques to verify the accuracy and \\nveracity of information generated by GAI systems, especially when the \\ninformation comes from multiple (or unknown) sources. \\nInformation Integrity  \\nMP-2.3-004 Develop and implement testing techniques to identify GAI produced content (e.g., \\nsynthetic media) that might be indistinguishable from human-generated content. Information Integrity \\nMP-2.3-005 Implement plans for GAI systems to undergo regular adversarial testing to identify \\nvulnerabilities and potential manipulation or misuse. \\nInformation Security \\nAI Actor Tasks: AI Development, Domain Experts, TEVV \\n \\nMAP 3.4: Processes for operator and practitioner proﬁciency with AI system performance and trustworthiness – and relevant \\ntechnical standards and certiﬁcations – are deﬁned, assessed, and documented. \\nAction ID \\nSuggested Action \\nGAI Risks \\nMP-3.4-001 \\nEvaluate whether GAI operators and end-users can accurately understand \\ncontent lineage and origin. \\nHuman-AI Conﬁguration; \\nInformation Integrity \\nMP-3.4-002 Adapt existing training programs to include modules on digital content \\ntransparency. \\nInformation Integrity \\nMP-3.4-003 Develop certiﬁcation programs that test proﬁciency in managing GAI risks and \\ninterpreting content provenance, relevant to speciﬁc industry and context. \\nInformation Integrity \\nMP-3.4-004 Delineate human proﬁciency tests from tests of GAI capabilities. \\nHuman-AI Conﬁguration \\nMP-3.4-005 Implement systems to continually monitor and track the outcomes of human-GAI \\nconﬁgurations for future reﬁnement and improvements. \\nHuman-AI Conﬁguration; \\nInformation Integrity \\nMP-3.4-006 \\nInvolve the end-users, practitioners, and operators in GAI system in prototyping \\nand testing activities. Make sure these tests cover various scenarios, such as crisis \\nsituations or ethically sensitive contexts. \\nHuman-AI Conﬁguration; \\nInformation Integrity; Harmful Bias \\nand Homogenization; Dangerous, \\nViolent, or Hateful Content \\nAI Actor Tasks: AI Design, AI Development, Domain Experts, End-Users, Human Factors, Operation and Monitoring \\n \\n'),\n",
              " Document(metadata={'source': 'dataset/data2.pdf', 'file_path': 'dataset/data2.pdf', 'page': 29, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=' \\n26 \\nMAP 4.1: Approaches for mapping AI technology and legal risks of its components – including the use of third-party data or \\nsoftware – are in place, followed, and documented, as are risks of infringement of a third-party’s intellectual property or other \\nrights. \\nAction ID \\nSuggested Action \\nGAI Risks \\nMP-4.1-001 Conduct periodic monitoring of AI-generated content for privacy risks; address any \\npossible instances of PII or sensitive data exposure. \\nData Privacy \\nMP-4.1-002 Implement processes for responding to potential intellectual property infringement \\nclaims or other rights. \\nIntellectual Property \\nMP-4.1-003 \\nConnect new GAI policies, procedures, and processes to existing model, data, \\nsoftware development, and IT governance and to legal, compliance, and risk \\nmanagement activities. \\nInformation Security; Data Privacy \\nMP-4.1-004 Document training data curation policies, to the extent possible and according to \\napplicable laws and policies. \\nIntellectual Property; Data Privacy; \\nObscene, Degrading, and/or \\nAbusive Content \\nMP-4.1-005 \\nEstablish policies for collection, retention, and minimum quality of data, in \\nconsideration of the following risks: Disclosure of inappropriate CBRN information; \\nUse of Illegal or dangerous content; Oﬀensive cyber capabilities; Training data \\nimbalances that could give rise to harmful biases; Leak of personally identiﬁable \\ninformation, including facial likenesses of individuals. \\nCBRN Information or Capabilities; \\nIntellectual Property; Information \\nSecurity; Harmful Bias and \\nHomogenization; Dangerous, \\nViolent, or Hateful Content; Data \\nPrivacy \\nMP-4.1-006 Implement policies and practices deﬁning how third-party intellectual property and \\ntraining data will be used, stored, and protected. \\nIntellectual Property; Value Chain \\nand Component Integration \\nMP-4.1-007 Re-evaluate models that were ﬁne-tuned or enhanced on top of third-party \\nmodels. \\nValue Chain and Component \\nIntegration \\nMP-4.1-008 \\nRe-evaluate risks when adapting GAI models to new domains. Additionally, \\nestablish warning systems to determine if a GAI system is being used in a new \\ndomain where previous assumptions (relating to context of use or mapped risks \\nsuch as security, and safety) may no longer hold.  \\nCBRN Information or Capabilities; \\nIntellectual Property; Harmful Bias \\nand Homogenization; Dangerous, \\nViolent, or Hateful Content; Data \\nPrivacy \\nMP-4.1-009 Leverage approaches to detect the presence of PII or sensitive data in generated \\noutput text, image, video, or audio. \\nData Privacy \\n'),\n",
              " Document(metadata={'source': 'dataset/data2.pdf', 'file_path': 'dataset/data2.pdf', 'page': 30, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=\" \\n27 \\nMP-4.1-010 \\nConduct appropriate diligence on training data use to assess intellectual property, \\nand privacy, risks, including to examine whether use of proprietary or sensitive \\ntraining data is consistent with applicable laws.  \\nIntellectual Property; Data Privacy \\nAI Actor Tasks: Governance and Oversight, Operation and Monitoring, Procurement, Third-party entities \\n \\nMAP 5.1: Likelihood and magnitude of each identiﬁed impact (both potentially beneﬁcial and harmful) based on expected use, past \\nuses of AI systems in similar contexts, public incident reports, feedback from those external to the team that developed or deployed \\nthe AI system, or other data are identiﬁed and documented. \\nAction ID \\nSuggested Action \\nGAI Risks \\nMP-5.1-001 Apply TEVV practices for content provenance (e.g., probing a system's synthetic \\ndata generation capabilities for potential misuse or vulnerabilities. \\nInformation Integrity; Information \\nSecurity \\nMP-5.1-002 \\nIdentify potential content provenance harms of GAI, such as misinformation or \\ndisinformation, deepfakes, including NCII, or tampered content. Enumerate and \\nrank risks based on their likelihood and potential impact, and determine how well \\nprovenance solutions address speciﬁc risks and/or harms. \\nInformation Integrity; Dangerous, \\nViolent, or Hateful Content; \\nObscene, Degrading, and/or \\nAbusive Content \\nMP-5.1-003 \\nConsider disclosing use of GAI to end users in relevant contexts, while considering \\nthe objective of disclosure, the context of use, the likelihood and magnitude of the \\nrisk posed, the audience of the disclosure, as well as the frequency of the \\ndisclosures. \\nHuman-AI Conﬁguration \\nMP-5.1-004 Prioritize GAI structured public feedback processes based on risk assessment \\nestimates. \\nInformation Integrity; CBRN \\nInformation or Capabilities; \\nDangerous, Violent, or Hateful \\nContent; Harmful Bias and \\nHomogenization \\nMP-5.1-005 Conduct adversarial role-playing exercises, GAI red-teaming, or chaos testing to \\nidentify anomalous or unforeseen failure modes. \\nInformation Security \\nMP-5.1-006 \\nProﬁle threats and negative impacts arising from GAI systems interacting with, \\nmanipulating, or generating content, and outlining known and potential \\nvulnerabilities and the likelihood of their occurrence. \\nInformation Security \\nAI Actor Tasks: AI Deployment, AI Design, AI Development, AI Impact Assessment, Aﬀected Individuals and Communities, End-\\nUsers, Operation and Monitoring \\n \\n\"),\n",
              " Document(metadata={'source': 'dataset/data2.pdf', 'file_path': 'dataset/data2.pdf', 'page': 31, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=' \\n28 \\nMAP 5.2: Practices and personnel for supporting regular engagement with relevant AI Actors and integrating feedback about \\npositive, negative, and unanticipated impacts are in place and documented. \\nAction ID \\nSuggested Action \\nGAI Risks \\nMP-5.2-001 \\nDetermine context-based measures to identify if new impacts are present due to \\nthe GAI system, including regular engagements with downstream AI Actors to \\nidentify and quantify new contexts of unanticipated impacts of GAI systems. \\nHuman-AI Conﬁguration; Value \\nChain and Component Integration \\nMP-5.2-002 \\nPlan regular engagements with AI Actors responsible for inputs to GAI systems, \\nincluding third-party data and algorithms, to review and evaluate unanticipated \\nimpacts. \\nHuman-AI Conﬁguration; Value \\nChain and Component Integration \\nAI Actor Tasks: AI Deployment, AI Design, AI Impact Assessment, Aﬀected Individuals and Communities, Domain Experts, End-\\nUsers, Human Factors, Operation and Monitoring  \\n \\nMEASURE 1.1: Approaches and metrics for measurement of AI risks enumerated during the MAP function are selected for \\nimplementation starting with the most signiﬁcant AI risks. The risks or trustworthiness characteristics that will not – or cannot – be \\nmeasured are properly documented. \\nAction ID \\nSuggested Action \\nGAI Risks \\nMS-1.1-001 Employ methods to trace the origin and modiﬁcations of digital content. \\nInformation Integrity \\nMS-1.1-002 \\nIntegrate tools designed to analyze content provenance and detect data \\nanomalies, verify the authenticity of digital signatures, and identify patterns \\nassociated with misinformation or manipulation. \\nInformation Integrity \\nMS-1.1-003 \\nDisaggregate evaluation metrics by demographic factors to identify any \\ndiscrepancies in how content provenance mechanisms work across diverse \\npopulations. \\nInformation Integrity; Harmful \\nBias and Homogenization \\nMS-1.1-004 Develop a suite of metrics to evaluate structured public feedback exercises \\ninformed by representative AI Actors. \\nHuman-AI Conﬁguration; Harmful \\nBias and Homogenization; CBRN \\nInformation or Capabilities \\nMS-1.1-005 \\nEvaluate novel methods and technologies for the measurement of GAI-related \\nrisks including in content provenance, oﬀensive cyber, and CBRN, while \\nmaintaining the models’ ability to produce valid, reliable, and factually accurate \\noutputs. \\nInformation Integrity; CBRN \\nInformation or Capabilities; \\nObscene, Degrading, and/or \\nAbusive Content \\n'),\n",
              " Document(metadata={'source': 'dataset/data2.pdf', 'file_path': 'dataset/data2.pdf', 'page': 32, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=' \\n29 \\nMS-1.1-006 \\nImplement continuous monitoring of GAI system impacts to identify whether GAI \\noutputs are equitable across various sub-populations. Seek active and direct \\nfeedback from aﬀected communities via structured feedback mechanisms or red-\\nteaming to monitor and improve outputs.  \\nHarmful Bias and Homogenization \\nMS-1.1-007 \\nEvaluate the quality and integrity of data used in training and the provenance of \\nAI-generated content, for example by employing techniques like chaos \\nengineering and seeking stakeholder feedback. \\nInformation Integrity \\nMS-1.1-008 \\nDeﬁne use cases, contexts of use, capabilities, and negative impacts where \\nstructured human feedback exercises, e.g., GAI red-teaming, would be most \\nbeneﬁcial for GAI risk measurement and management based on the context of \\nuse. \\nHarmful Bias and \\nHomogenization; CBRN \\nInformation or Capabilities \\nMS-1.1-009 \\nTrack and document risks or opportunities related to all GAI risks that cannot be \\nmeasured quantitatively, including explanations as to why some risks cannot be \\nmeasured (e.g., due to technological limitations, resource constraints, or \\ntrustworthy considerations). Include unmeasured risks in marginal risks. \\nInformation Integrity \\nAI Actor Tasks: AI Development, Domain Experts, TEVV \\n \\nMEASURE 1.3: Internal experts who did not serve as front-line developers for the system and/or independent assessors are \\ninvolved in regular assessments and updates. Domain experts, users, AI Actors external to the team that developed or deployed the \\nAI system, and aﬀected communities are consulted in support of assessments as necessary per organizational risk tolerance. \\nAction ID \\nSuggested Action \\nGAI Risks \\nMS-1.3-001 \\nDeﬁne relevant groups of interest (e.g., demographic groups, subject matter \\nexperts, experience with GAI technology) within the context of use as part of \\nplans for gathering structured public feedback. \\nHuman-AI Conﬁguration; Harmful \\nBias and Homogenization; CBRN \\nInformation or Capabilities \\nMS-1.3-002 \\nEngage in internal and external evaluations, GAI red-teaming, impact \\nassessments, or other structured human feedback exercises in consultation \\nwith representative AI Actors with expertise and familiarity in the context of \\nuse, and/or who are representative of the populations associated with the \\ncontext of use. \\nHuman-AI Conﬁguration; Harmful \\nBias and Homogenization; CBRN \\nInformation or Capabilities \\nMS-1.3-003 \\nVerify those conducting structured human feedback exercises are not directly \\ninvolved in system development tasks for the same GAI model. \\nHuman-AI Conﬁguration; Data \\nPrivacy \\nAI Actor Tasks: AI Deployment, AI Development, AI Impact Assessment, Aﬀected Individuals and Communities, Domain Experts, \\nEnd-Users, Operation and Monitoring, TEVV \\n \\n'),\n",
              " Document(metadata={'source': 'dataset/data2.pdf', 'file_path': 'dataset/data2.pdf', 'page': 33, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=' \\n30 \\nMEASURE 2.2: Evaluations involving human subjects meet applicable requirements (including human subject protection) and are \\nrepresentative of the relevant population. \\nAction ID \\nSuggested Action \\nGAI Risks \\nMS-2.2-001 Assess and manage statistical biases related to GAI content provenance through \\ntechniques such as re-sampling, re-weighting, or adversarial training. \\nInformation Integrity; Information \\nSecurity; Harmful Bias and \\nHomogenization \\nMS-2.2-002 \\nDocument how content provenance data is tracked and how that data interacts \\nwith privacy and security. Consider: Anonymizing data to protect the privacy of \\nhuman subjects; Leveraging privacy output ﬁlters; Removing any personally \\nidentiﬁable information (PII) to prevent potential harm or misuse. \\nData Privacy; Human AI \\nConﬁguration; Information \\nIntegrity; Information Security; \\nDangerous, Violent, or Hateful \\nContent \\nMS-2.2-003 Provide human subjects with options to withdraw participation or revoke their \\nconsent for present or future use of their data in GAI applications.  \\nData Privacy; Human-AI \\nConﬁguration; Information \\nIntegrity \\nMS-2.2-004 \\nUse techniques such as anonymization, diﬀerential privacy or other privacy-\\nenhancing technologies to minimize the risks associated with linking AI-generated \\ncontent back to individual human subjects. \\nData Privacy; Human-AI \\nConﬁguration \\nAI Actor Tasks: AI Development, Human Factors, TEVV \\n \\nMEASURE 2.3: AI system performance or assurance criteria are measured qualitatively or quantitatively and demonstrated for \\nconditions similar to deployment setting(s). Measures are documented. \\nAction ID \\nSuggested Action \\nGAI Risks \\nMS-2.3-001 Consider baseline model performance on suites of benchmarks when selecting a \\nmodel for ﬁne tuning or enhancement with retrieval-augmented generation. \\nInformation Security; \\nConfabulation \\nMS-2.3-002 Evaluate claims of model capabilities using empirically validated methods. \\nConfabulation; Information \\nSecurity \\nMS-2.3-003 Share results of pre-deployment testing with relevant GAI Actors, such as those \\nwith system release approval authority. \\nHuman-AI Conﬁguration \\n'),\n",
              " Document(metadata={'source': 'dataset/data2.pdf', 'file_path': 'dataset/data2.pdf', 'page': 34, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=' \\n31 \\nMS-2.3-004 \\nUtilize a purpose-built testing environment such as NIST Dioptra to empirically \\nevaluate GAI trustworthy characteristics. \\nCBRN Information or Capabilities; \\nData Privacy; Confabulation; \\nInformation Integrity; Information \\nSecurity; Dangerous, Violent, or \\nHateful Content; Harmful Bias and \\nHomogenization \\nAI Actor Tasks: AI Deployment, TEVV \\n \\nMEASURE 2.5: The AI system to be deployed is demonstrated to be valid and reliable. Limitations of the generalizability beyond the \\nconditions under which the technology was developed are documented. \\nAction ID \\nSuggested Action \\nRisks \\nMS-2.5-001 Avoid extrapolating GAI system performance or capabilities from narrow, non-\\nsystematic, and anecdotal assessments. \\nHuman-AI Conﬁguration; \\nConfabulation \\nMS-2.5-002 \\nDocument the extent to which human domain knowledge is employed to \\nimprove GAI system performance, via, e.g., RLHF, ﬁne-tuning, retrieval-\\naugmented generation, content moderation, business rules. \\nHuman-AI Conﬁguration \\nMS-2.5-003 Review and verify sources and citations in GAI system outputs during pre-\\ndeployment risk measurement and ongoing monitoring activities. \\nConfabulation \\nMS-2.5-004 Track and document instances of anthropomorphization (e.g., human images, \\nmentions of human feelings, cyborg imagery or motifs) in GAI system interfaces. Human-AI Conﬁguration \\nMS-2.5-005 Verify GAI system training data and TEVV data provenance, and that ﬁne-tuning \\nor retrieval-augmented generation data is grounded. \\nInformation Integrity \\nMS-2.5-006 \\nRegularly review security and safety guardrails, especially if the GAI system is \\nbeing operated in novel circumstances. This includes reviewing reasons why the \\nGAI system was initially assessed as being safe to deploy.  \\nInformation Security; Dangerous, \\nViolent, or Hateful Content \\nAI Actor Tasks: Domain Experts, TEVV \\n \\n'),\n",
              " Document(metadata={'source': 'dataset/data2.pdf', 'file_path': 'dataset/data2.pdf', 'page': 35, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=' \\n32 \\nMEASURE 2.6: The AI system is evaluated regularly for safety risks – as identiﬁed in the MAP function. The AI system to be \\ndeployed is demonstrated to be safe, its residual negative risk does not exceed the risk tolerance, and it can fail safely, particularly if \\nmade to operate beyond its knowledge limits. Safety metrics reﬂect system reliability and robustness, real-time monitoring, and \\nresponse times for AI system failures. \\nAction ID \\nSuggested Action \\nGAI Risks \\nMS-2.6-001 \\nAssess adverse impacts, including health and wellbeing impacts for value chain \\nor other AI Actors that are exposed to sexually explicit, oﬀensive, or violent \\ninformation during GAI training and maintenance. \\nHuman-AI Conﬁguration; Obscene, \\nDegrading, and/or Abusive \\nContent; Value Chain and \\nComponent Integration; \\nDangerous, Violent, or Hateful \\nContent \\nMS-2.6-002 \\nAssess existence or levels of harmful bias, intellectual property infringement, \\ndata privacy violations, obscenity, extremism, violence, or CBRN information in \\nsystem training data. \\nData Privacy; Intellectual Property; \\nObscene, Degrading, and/or \\nAbusive Content; Harmful Bias and \\nHomogenization; Dangerous, \\nViolent, or Hateful Content; CBRN \\nInformation or Capabilities \\nMS-2.6-003 Re-evaluate safety features of ﬁne-tuned models when the negative risk exceeds \\norganizational risk tolerance. \\nDangerous, Violent, or Hateful \\nContent \\nMS-2.6-004 Review GAI system outputs for validity and safety: Review generated code to \\nassess risks that may arise from unreliable downstream decision-making. \\nValue Chain and Component \\nIntegration; Dangerous, Violent, or \\nHateful Content \\nMS-2.6-005 \\nVerify that GAI system architecture can monitor outputs and performance, and \\nhandle, recover from, and repair errors when security anomalies, threats and \\nimpacts are detected. \\nConfabulation; Information \\nIntegrity; Information Security \\nMS-2.6-006 \\nVerify that systems properly handle queries that may give rise to inappropriate, \\nmalicious, or illegal usage, including facilitating manipulation, extortion, targeted \\nimpersonation, cyber-attacks, and weapons creation. \\nCBRN Information or Capabilities; \\nInformation Security \\nMS-2.6-007 Regularly evaluate GAI system vulnerabilities to possible circumvention of safety \\nmeasures.  \\nCBRN Information or Capabilities; \\nInformation Security \\nAI Actor Tasks: AI Deployment, AI Impact Assessment, Domain Experts, Operation and Monitoring, TEVV \\n \\n'),\n",
              " Document(metadata={'source': 'dataset/data2.pdf', 'file_path': 'dataset/data2.pdf', 'page': 36, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=' \\n33 \\nMEASURE 2.7: AI system security and resilience – as identiﬁed in the MAP function – are evaluated and documented. \\nAction ID \\nSuggested Action \\nGAI Risks \\nMS-2.7-001 \\nApply established security measures to: Assess likelihood and magnitude of \\nvulnerabilities and threats such as backdoors, compromised dependencies, data \\nbreaches, eavesdropping, man-in-the-middle attacks, reverse engineering, \\nautonomous agents, model theft or exposure of model weights, AI inference, \\nbypass, extraction, and other baseline security concerns. \\nData Privacy; Information Integrity; \\nInformation Security; Value Chain \\nand Component Integration \\nMS-2.7-002 \\nBenchmark GAI system security and resilience related to content provenance \\nagainst industry standards and best practices. Compare GAI system security \\nfeatures and content provenance methods against industry state-of-the-art. \\nInformation Integrity; Information \\nSecurity \\nMS-2.7-003 \\nConduct user surveys to gather user satisfaction with the AI-generated content \\nand user perceptions of content authenticity. Analyze user feedback to identify \\nconcerns and/or current literacy levels related to content provenance and \\nunderstanding of labels on content. \\nHuman-AI Conﬁguration; \\nInformation Integrity \\nMS-2.7-004 \\nIdentify metrics that reﬂect the eﬀectiveness of security measures, such as data \\nprovenance, the number of unauthorized access attempts, inference, bypass, \\nextraction, penetrations, or provenance veriﬁcation. \\nInformation Integrity; Information \\nSecurity \\nMS-2.7-005 \\nMeasure reliability of content authentication methods, such as watermarking, \\ncryptographic signatures, digital ﬁngerprints, as well as access controls, \\nconformity assessment, and model integrity veriﬁcation, which can help support \\nthe eﬀective implementation of content provenance techniques. Evaluate the \\nrate of false positives and false negatives in content provenance, as well as true \\npositives and true negatives for veriﬁcation. \\nInformation Integrity \\nMS-2.7-006 \\nMeasure the rate at which recommendations from security checks and incidents \\nare implemented. Assess how quickly the AI system can adapt and improve \\nbased on lessons learned from security incidents and feedback. \\nInformation Integrity; Information \\nSecurity \\nMS-2.7-007 \\nPerform AI red-teaming to assess resilience against: Abuse to facilitate attacks on \\nother systems (e.g., malicious code generation, enhanced phishing content), GAI \\nattacks (e.g., prompt injection), ML attacks (e.g., adversarial examples/prompts, \\ndata poisoning, membership inference, model extraction, sponge examples). \\nInformation Security; Harmful Bias \\nand Homogenization; Dangerous, \\nViolent, or Hateful Content \\nMS-2.7-008 Verify ﬁne-tuning does not compromise safety and security controls. \\nInformation Integrity; Information \\nSecurity; Dangerous, Violent, or \\nHateful Content \\n'),\n",
              " Document(metadata={'source': 'dataset/data2.pdf', 'file_path': 'dataset/data2.pdf', 'page': 37, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=' \\n34 \\nMS-2.7-009 Regularly assess and verify that security measures remain eﬀective and have not \\nbeen compromised. \\nInformation Security \\nAI Actor Tasks: AI Deployment, AI Impact Assessment, Domain Experts, Operation and Monitoring, TEVV \\n \\nMEASURE 2.8: Risks associated with transparency and accountability – as identiﬁed in the MAP function – are examined and \\ndocumented. \\nAction ID \\nSuggested Action \\nGAI Risks \\nMS-2.8-001 \\nCompile statistics on actual policy violations, take-down requests, and intellectual \\nproperty infringement for organizational GAI systems: Analyze transparency \\nreports across demographic groups, languages groups. \\nIntellectual Property; Harmful Bias \\nand Homogenization \\nMS-2.8-002 Document the instructions given to data annotators or AI red-teamers. \\nHuman-AI Conﬁguration \\nMS-2.8-003 \\nUse digital content transparency solutions to enable the documentation of each \\ninstance where content is generated, modiﬁed, or shared to provide a tamper-\\nproof history of the content, promote transparency, and enable traceability. \\nRobust version control systems can also be applied to track changes across the AI \\nlifecycle over time. \\nInformation Integrity \\nMS-2.8-004 Verify adequacy of GAI system user instructions through user testing. \\nHuman-AI Conﬁguration \\nAI Actor Tasks: AI Deployment, AI Impact Assessment, Domain Experts, Operation and Monitoring, TEVV \\n \\n'),\n",
              " Document(metadata={'source': 'dataset/data2.pdf', 'file_path': 'dataset/data2.pdf', 'page': 38, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=' \\n35 \\nMEASURE 2.9: The AI model is explained, validated, and documented, and AI system output is interpreted within its context – as \\nidentiﬁed in the MAP function – to inform responsible use and governance. \\nAction ID \\nSuggested Action \\nGAI Risks \\nMS-2.9-001 \\nApply and document ML explanation results such as: Analysis of embeddings, \\nCounterfactual prompts, Gradient-based attributions, Model \\ncompression/surrogate models, Occlusion/term reduction. \\nConfabulation \\nMS-2.9-002 \\nDocument GAI model details including: Proposed use and organizational value; \\nAssumptions and limitations, Data collection methodologies; Data provenance; \\nData quality; Model architecture (e.g., convolutional neural network, \\ntransformers, etc.); Optimization objectives; Training algorithms; RLHF \\napproaches; Fine-tuning or retrieval-augmented generation approaches; \\nEvaluation data; Ethical considerations; Legal and regulatory requirements. \\nInformation Integrity; Harmful Bias \\nand Homogenization \\nAI Actor Tasks: AI Deployment, AI Impact Assessment, Domain Experts, End-Users, Operation and Monitoring, TEVV \\n \\nMEASURE 2.10: Privacy risk of the AI system – as identiﬁed in the MAP function – is examined and documented. \\nAction ID \\nSuggested Action \\nGAI Risks \\nMS-2.10-001 \\nConduct AI red-teaming to assess issues such as: Outputting of training data \\nsamples, and subsequent reverse engineering, model extraction, and \\nmembership inference risks; Revealing biometric, conﬁdential, copyrighted, \\nlicensed, patented, personal, proprietary, sensitive, or trade-marked information; \\nTracking or revealing location information of users or members of training \\ndatasets. \\nHuman-AI Conﬁguration; \\nInformation Integrity; Intellectual \\nProperty \\nMS-2.10-002 \\nEngage directly with end-users and other stakeholders to understand their \\nexpectations and concerns regarding content provenance. Use this feedback to \\nguide the design of provenance data-tracking techniques. \\nHuman-AI Conﬁguration; \\nInformation Integrity \\nMS-2.10-003 Verify deduplication of GAI training data samples, particularly regarding synthetic \\ndata. \\nHarmful Bias and Homogenization \\nAI Actor Tasks: AI Deployment, AI Impact Assessment, Domain Experts, End-Users, Operation and Monitoring, TEVV \\n \\n'),\n",
              " Document(metadata={'source': 'dataset/data2.pdf', 'file_path': 'dataset/data2.pdf', 'page': 39, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=' \\n36 \\nMEASURE 2.11: Fairness and bias – as identiﬁed in the MAP function – are evaluated and results are documented. \\nAction ID \\nSuggested Action \\nGAI Risks \\nMS-2.11-001 \\nApply use-case appropriate benchmarks (e.g., Bias Benchmark Questions, Real \\nHateful or Harmful Prompts, Winogender Schemas15) to quantify systemic bias, \\nstereotyping, denigration, and hateful content in GAI system outputs; \\nDocument assumptions and limitations of benchmarks, including any actual or \\npossible training/test data cross contamination, relative to in-context \\ndeployment environment. \\nHarmful Bias and Homogenization \\nMS-2.11-002 \\nConduct fairness assessments to measure systemic bias. Measure GAI system \\nperformance across demographic groups and subgroups, addressing both \\nquality of service and any allocation of services and resources. Quantify harms \\nusing: ﬁeld testing with sub-group populations to determine likelihood of \\nexposure to generated content exhibiting harmful bias, AI red-teaming with \\ncounterfactual and low-context (e.g., “leader,” “bad guys”) prompts. For ML \\npipelines or business processes with categorical or numeric outcomes that rely \\non GAI, apply general fairness metrics (e.g., demographic parity, equalized odds, \\nequal opportunity, statistical hypothesis tests), to the pipeline or business \\noutcome where appropriate; Custom, context-speciﬁc metrics developed in \\ncollaboration with domain experts and aﬀected communities; Measurements of \\nthe prevalence of denigration in generated content in deployment (e.g., sub-\\nsampling a fraction of traﬃc and manually annotating denigrating content). \\nHarmful Bias and Homogenization; \\nDangerous, Violent, or Hateful \\nContent \\nMS-2.11-003 \\nIdentify the classes of individuals, groups, or environmental ecosystems which \\nmight be impacted by GAI systems through direct engagement with potentially \\nimpacted communities. \\nEnvironmental; Harmful Bias and \\nHomogenization \\nMS-2.11-004 \\nReview, document, and measure sources of bias in GAI training and TEVV data: \\nDiﬀerences in distributions of outcomes across and within groups, including \\nintersecting groups; Completeness, representativeness, and balance of data \\nsources; demographic group and subgroup coverage in GAI system training \\ndata; Forms of latent systemic bias in images, text, audio, embeddings, or other \\ncomplex or unstructured data; Input data features that may serve as proxies for \\ndemographic group membership (i.e., image metadata, language dialect) or \\notherwise give rise to emergent bias within GAI systems; The extent to which \\nthe digital divide may negatively impact representativeness in GAI system \\ntraining and TEVV data; Filtering of hate speech or content in GAI system \\ntraining data; Prevalence of GAI-generated data in GAI system training data. \\nHarmful Bias and Homogenization \\n \\n \\n15 Winogender Schemas is a sample set of paired sentences which diﬀer only by gender of the pronouns used, \\nwhich can be used to evaluate gender bias in natural language processing coreference resolution systems.  \\n'),\n",
              " Document(metadata={'source': 'dataset/data2.pdf', 'file_path': 'dataset/data2.pdf', 'page': 40, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=' \\n37 \\nMS-2.11-005 \\nAssess the proportion of synthetic to non-synthetic training data and verify \\ntraining data is not overly homogenous or GAI-produced to mitigate concerns of \\nmodel collapse. \\nHarmful Bias and Homogenization \\nAI Actor Tasks: AI Deployment, AI Impact Assessment, Aﬀected Individuals and Communities, Domain Experts, End-Users, \\nOperation and Monitoring, TEVV \\n \\nMEASURE 2.12: Environmental impact and sustainability of AI model training and management activities – as identiﬁed in the MAP \\nfunction – are assessed and documented. \\nAction ID \\nSuggested Action \\nGAI Risks \\nMS-2.12-001 Assess safety to physical environments when deploying GAI systems. \\nDangerous, Violent, or Hateful \\nContent \\nMS-2.12-002 Document anticipated environmental impacts of model development, \\nmaintenance, and deployment in product design decisions. \\nEnvironmental \\nMS-2.12-003 \\nMeasure or estimate environmental impacts (e.g., energy and water \\nconsumption) for training, ﬁne tuning, and deploying models: Verify tradeoﬀs \\nbetween resources used at inference time versus additional resources required \\nat training time. \\nEnvironmental \\nMS-2.12-004 Verify eﬀectiveness of carbon capture or oﬀset programs for GAI training and \\napplications, and address green-washing concerns. \\nEnvironmental \\nAI Actor Tasks: AI Deployment, AI Impact Assessment, Domain Experts, Operation and Monitoring, TEVV \\n \\n'),\n",
              " Document(metadata={'source': 'dataset/data2.pdf', 'file_path': 'dataset/data2.pdf', 'page': 41, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=' \\n38 \\nMEASURE 2.13: Eﬀectiveness of the employed TEVV metrics and processes in the MEASURE function are evaluated and \\ndocumented. \\nAction ID \\nSuggested Action \\nGAI Risks \\nMS-2.13-001 \\nCreate measurement error models for pre-deployment metrics to demonstrate \\nconstruct validity for each metric (i.e., does the metric eﬀectively operationalize \\nthe desired concept): Measure or estimate, and document, biases or statistical \\nvariance in applied metrics or structured human feedback processes; Leverage \\ndomain expertise when modeling complex societal constructs such as hateful \\ncontent. \\nConfabulation; Information \\nIntegrity; Harmful Bias and \\nHomogenization \\nAI Actor Tasks: AI Deployment, Operation and Monitoring, TEVV \\n \\nMEASURE 3.2: Risk tracking approaches are considered for settings where AI risks are diﬃcult to assess using currently available \\nmeasurement techniques or where metrics are not yet available. \\nAction ID \\nSuggested Action \\nGAI Risks \\nMS-3.2-001 \\nEstablish processes for identifying emergent GAI system risks including \\nconsulting with external AI Actors. \\nHuman-AI Conﬁguration; \\nConfabulation  \\nAI Actor Tasks: AI Impact Assessment, Domain Experts, Operation and Monitoring, TEVV \\n \\nMEASURE 3.3: Feedback processes for end users and impacted communities to report problems and appeal system outcomes are \\nestablished and integrated into AI system evaluation metrics. \\nAction ID \\nSuggested Action \\nGAI Risks \\nMS-3.3-001 \\nConduct impact assessments on how AI-generated content might aﬀect \\ndiﬀerent social, economic, and cultural groups. \\nHarmful Bias and Homogenization \\nMS-3.3-002 \\nConduct studies to understand how end users perceive and interact with GAI \\ncontent and accompanying content provenance within context of use. Assess \\nwhether the content aligns with their expectations and how they may act upon \\nthe information presented. \\nHuman-AI Conﬁguration; \\nInformation Integrity \\nMS-3.3-003 \\nEvaluate potential biases and stereotypes that could emerge from the AI-\\ngenerated content using appropriate methodologies including computational \\ntesting methods as well as evaluating structured feedback input. \\nHarmful Bias and Homogenization \\n'),\n",
              " Document(metadata={'source': 'dataset/data2.pdf', 'file_path': 'dataset/data2.pdf', 'page': 42, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=\" \\n39 \\nMS-3.3-004 \\nProvide input for training materials about the capabilities and limitations of GAI \\nsystems related to digital content transparency for AI Actors, other \\nprofessionals, and the public about the societal impacts of AI and the role of \\ndiverse and inclusive content generation. \\nHuman-AI Conﬁguration; \\nInformation Integrity; Harmful Bias \\nand Homogenization \\nMS-3.3-005 \\nRecord and integrate structured feedback about content provenance from \\noperators, users, and potentially impacted communities through the use of \\nmethods such as user research studies, focus groups, or community forums. \\nActively seek feedback on generated content quality and potential biases. \\nAssess the general awareness among end users and impacted communities \\nabout the availability of these feedback channels. \\nHuman-AI Conﬁguration; \\nInformation Integrity; Harmful Bias \\nand Homogenization \\nAI Actor Tasks: AI Deployment, Aﬀected Individuals and Communities, End-Users, Operation and Monitoring, TEVV \\n \\nMEASURE 4.2: Measurement results regarding AI system trustworthiness in deployment context(s) and across the AI lifecycle are \\ninformed by input from domain experts and relevant AI Actors to validate whether the system is performing consistently as \\nintended. Results are documented. \\nAction ID \\nSuggested Action \\nGAI Risks \\nMS-4.2-001 \\nConduct adversarial testing at a regular cadence to map and measure GAI risks, \\nincluding tests to address attempts to deceive or manipulate the application of \\nprovenance techniques or other misuses. Identify vulnerabilities and \\nunderstand potential misuse scenarios and unintended outputs. \\nInformation Integrity; Information \\nSecurity \\nMS-4.2-002 \\nEvaluate GAI system performance in real-world scenarios to observe its \\nbehavior in practical environments and reveal issues that might not surface in \\ncontrolled and optimized testing environments. \\nHuman-AI Conﬁguration; \\nConfabulation; Information \\nSecurity \\nMS-4.2-003 \\nImplement interpretability and explainability methods to evaluate GAI system \\ndecisions and verify alignment with intended purpose. \\nInformation Integrity; Harmful Bias \\nand Homogenization \\nMS-4.2-004 \\nMonitor and document instances where human operators or other systems \\noverride the GAI's decisions. Evaluate these cases to understand if the overrides \\nare linked to issues related to content provenance. \\nInformation Integrity \\nMS-4.2-005 \\nVerify and document the incorporation of results of structured public feedback \\nexercises into design, implementation, deployment approval (“go”/“no-go” \\ndecisions), monitoring, and decommission decisions. \\nHuman-AI Conﬁguration; \\nInformation Security \\nAI Actor Tasks: AI Deployment, Domain Experts, End-Users, Operation and Monitoring, TEVV \\n \\n\"),\n",
              " Document(metadata={'source': 'dataset/data2.pdf', 'file_path': 'dataset/data2.pdf', 'page': 43, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=' \\n40 \\nMANAGE 1.3: Responses to the AI risks deemed high priority, as identiﬁed by the MAP function, are developed, planned, and \\ndocumented. Risk response options can include mitigating, transferring, avoiding, or accepting. \\nAction ID \\nSuggested Action \\nGAI Risks \\nMG-1.3-001 \\nDocument trade-oﬀs, decision processes, and relevant measurement and \\nfeedback results for risks that do not surpass organizational risk tolerance, for \\nexample, in the context of model release: Consider diﬀerent approaches for \\nmodel release, for example, leveraging a staged release approach. Consider \\nrelease approaches in the context of the model and its projected use cases. \\nMitigate, transfer, or avoid risks that surpass organizational risk tolerances. \\nInformation Security \\nMG-1.3-002 \\nMonitor the robustness and eﬀectiveness of risk controls and mitigation plans \\n(e.g., via red-teaming, ﬁeld testing, participatory engagements, performance \\nassessments, user feedback mechanisms). \\nHuman-AI Conﬁguration \\nAI Actor Tasks: AI Development, AI Deployment, AI Impact Assessment, Operation and Monitoring \\n \\nMANAGE 2.2: Mechanisms are in place and applied to sustain the value of deployed AI systems. \\nAction ID \\nSuggested Action \\nGAI Risks \\nMG-2.2-001 \\nCompare GAI system outputs against pre-deﬁned organization risk tolerance, \\nguidelines, and principles, and review and test AI-generated content against \\nthese guidelines. \\nCBRN Information or Capabilities; \\nObscene, Degrading, and/or \\nAbusive Content; Harmful Bias and \\nHomogenization; Dangerous, \\nViolent, or Hateful Content \\nMG-2.2-002 \\nDocument training data sources to trace the origin and provenance of AI-\\ngenerated content. \\nInformation Integrity \\nMG-2.2-003 \\nEvaluate feedback loops between GAI system content provenance and human \\nreviewers, and update where needed. Implement real-time monitoring systems \\nto aﬃrm that content provenance protocols remain eﬀective.  \\nInformation Integrity \\nMG-2.2-004 \\nEvaluate GAI content and data for representational biases and employ \\ntechniques such as re-sampling, re-ranking, or adversarial training to mitigate \\nbiases in the generated content. \\nInformation Security; Harmful Bias \\nand Homogenization \\nMG-2.2-005 \\nEngage in due diligence to analyze GAI output for harmful content, potential \\nmisinformation, and CBRN-related or NCII content. \\nCBRN Information or Capabilities; \\nObscene, Degrading, and/or \\nAbusive Content; Harmful Bias and \\nHomogenization; Dangerous, \\nViolent, or Hateful Content \\n'),\n",
              " Document(metadata={'source': 'dataset/data2.pdf', 'file_path': 'dataset/data2.pdf', 'page': 44, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=' \\n41 \\nMG-2.2-006 \\nUse feedback from internal and external AI Actors, users, individuals, and \\ncommunities, to assess impact of AI-generated content. \\nHuman-AI Conﬁguration \\nMG-2.2-007 \\nUse real-time auditing tools where they can be demonstrated to aid in the \\ntracking and validation of the lineage and authenticity of AI-generated data. \\nInformation Integrity \\nMG-2.2-008 \\nUse structured feedback mechanisms to solicit and capture user input about AI-\\ngenerated content to detect subtle shifts in quality or alignment with \\ncommunity and societal values. \\nHuman-AI Conﬁguration; Harmful \\nBias and Homogenization \\nMG-2.2-009 \\nConsider opportunities to responsibly use synthetic data and other privacy \\nenhancing techniques in GAI development, where appropriate and applicable, \\nmatch the statistical properties of real-world data without disclosing personally \\nidentiﬁable information or contributing to homogenization. \\nData Privacy; Intellectual Property; \\nInformation Integrity; \\nConfabulation; Harmful Bias and \\nHomogenization \\nAI Actor Tasks: AI Deployment, AI Impact Assessment, Governance and Oversight, Operation and Monitoring \\n \\nMANAGE 2.3: Procedures are followed to respond to and recover from a previously unknown risk when it is identiﬁed. \\nAction ID \\nSuggested Action \\nGAI Risks \\nMG-2.3-001 \\nDevelop and update GAI system incident response and recovery plans and \\nprocedures to address the following: Review and maintenance of policies and \\nprocedures to account for newly encountered uses; Review and maintenance of \\npolicies and procedures for detection of unanticipated uses; Verify response \\nand recovery plans account for the GAI system value chain; Verify response and \\nrecovery plans are updated for and include necessary details to communicate \\nwith downstream GAI system Actors: Points-of-Contact (POC), Contact \\ninformation, notiﬁcation format. \\nValue Chain and Component \\nIntegration \\nAI Actor Tasks: AI Deployment, Operation and Monitoring \\n \\nMANAGE 2.4: Mechanisms are in place and applied, and responsibilities are assigned and understood, to supersede, disengage, or \\ndeactivate AI systems that demonstrate performance or outcomes inconsistent with intended use. \\nAction ID \\nSuggested Action \\nGAI Risks \\nMG-2.4-001 \\nEstablish and maintain communication plans to inform AI stakeholders as part of \\nthe deactivation or disengagement process of a speciﬁc GAI system (including for \\nopen-source models) or context of use, including reasons, workarounds, user \\naccess removal, alternative processes, contact information, etc. \\nHuman-AI Conﬁguration \\n'),\n",
              " Document(metadata={'source': 'dataset/data2.pdf', 'file_path': 'dataset/data2.pdf', 'page': 45, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=' \\n42 \\nMG-2.4-002 \\nEstablish and maintain procedures for escalating GAI system incidents to the \\norganizational risk management authority when speciﬁc criteria for deactivation \\nor disengagement is met for a particular context of use or for the GAI system as a \\nwhole. \\nInformation Security \\nMG-2.4-003 \\nEstablish and maintain procedures for the remediation of issues which trigger \\nincident response processes for the use of a GAI system, and provide stakeholders \\ntimelines associated with the remediation plan. \\nInformation Security \\n \\nMG-2.4-004 Establish and regularly review speciﬁc criteria that warrants the deactivation of \\nGAI systems in accordance with set risk tolerances and appetites. \\nInformation Security \\n \\nAI Actor Tasks: AI Deployment, Governance and Oversight, Operation and Monitoring \\n \\nMANAGE 3.1: AI risks and beneﬁts from third-party resources are regularly monitored, and risk controls are applied and \\ndocumented. \\nAction ID \\nSuggested Action \\nGAI Risks \\nMG-3.1-001 \\nApply organizational risk tolerances and controls (e.g., acquisition and \\nprocurement processes; assessing personnel credentials and qualiﬁcations, \\nperforming background checks; ﬁltering GAI input and outputs, grounding, ﬁne \\ntuning, retrieval-augmented generation) to third-party GAI resources: Apply \\norganizational risk tolerance to the utilization of third-party datasets and other \\nGAI resources; Apply organizational risk tolerances to ﬁne-tuned third-party \\nmodels; Apply organizational risk tolerance to existing third-party models \\nadapted to a new domain; Reassess risk measurements after ﬁne-tuning third-\\nparty GAI models. \\nValue Chain and Component \\nIntegration; Intellectual Property \\nMG-3.1-002 \\nTest GAI system value chain risks (e.g., data poisoning, malware, other software \\nand hardware vulnerabilities; labor practices; data privacy and localization \\ncompliance; geopolitical alignment). \\nData Privacy; Information Security; \\nValue Chain and Component \\nIntegration; Harmful Bias and \\nHomogenization \\nMG-3.1-003 \\nRe-assess model risks after ﬁne-tuning or retrieval-augmented generation \\nimplementation and for any third-party GAI models deployed for applications \\nand/or use cases that were not evaluated in initial testing. \\nValue Chain and Component \\nIntegration \\nMG-3.1-004 \\nTake reasonable measures to review training data for CBRN information, and \\nintellectual property, and where appropriate, remove it. Implement reasonable \\nmeasures to prevent, ﬂag, or take other action in response to outputs that \\nreproduce particular training data (e.g., plagiarized, trademarked, patented, \\nlicensed content or trade secret material). \\nIntellectual Property; CBRN \\nInformation or Capabilities \\n'),\n",
              " Document(metadata={'source': 'dataset/data2.pdf', 'file_path': 'dataset/data2.pdf', 'page': 46, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=' \\n43 \\nMG-3.1-005 Review various transparency artifacts (e.g., system cards and model cards) for \\nthird-party models. \\nInformation Integrity; Information \\nSecurity; Value Chain and \\nComponent Integration \\nAI Actor Tasks: AI Deployment, Operation and Monitoring, Third-party entities \\n \\nMANAGE 3.2: Pre-trained models which are used for development are monitored as part of AI system regular monitoring and \\nmaintenance. \\nAction ID \\nSuggested Action \\nGAI Risks \\nMG-3.2-001 \\nApply explainable AI (XAI) techniques (e.g., analysis of embeddings, model \\ncompression/distillation, gradient-based attributions, occlusion/term reduction, \\ncounterfactual prompts, word clouds) as part of ongoing continuous \\nimprovement processes to mitigate risks related to unexplainable GAI systems. \\nHarmful Bias and Homogenization \\nMG-3.2-002 \\nDocument how pre-trained models have been adapted (e.g., ﬁne-tuned, or \\nretrieval-augmented generation) for the speciﬁc generative task, including any \\ndata augmentations, parameter adjustments, or other modiﬁcations. Access to \\nun-tuned (baseline) models supports debugging the relative inﬂuence of the pre-\\ntrained weights compared to the ﬁne-tuned model weights or other system \\nupdates. \\nInformation Integrity; Data Privacy \\nMG-3.2-003 \\nDocument sources and types of training data and their origins, potential biases \\npresent in the data related to the GAI application and its content provenance, \\narchitecture, training process of the pre-trained model including information on \\nhyperparameters, training duration, and any ﬁne-tuning or retrieval-augmented \\ngeneration processes applied. \\nInformation Integrity; Harmful Bias \\nand Homogenization; Intellectual \\nProperty \\nMG-3.2-004 Evaluate user reported problematic content and integrate feedback into system \\nupdates. \\nHuman-AI Conﬁguration, \\nDangerous, Violent, or Hateful \\nContent \\nMG-3.2-005 \\nImplement content ﬁlters to prevent the generation of inappropriate, harmful, \\nfalse, illegal, or violent content related to the GAI application, including for CSAM \\nand NCII. These ﬁlters can be rule-based or leverage additional machine learning \\nmodels to ﬂag problematic inputs and outputs. \\nInformation Integrity; Harmful Bias \\nand Homogenization; Dangerous, \\nViolent, or Hateful Content; \\nObscene, Degrading, and/or \\nAbusive Content \\nMG-3.2-006 \\nImplement real-time monitoring processes for analyzing generated content \\nperformance and trustworthiness characteristics related to content provenance \\nto identify deviations from the desired standards and trigger alerts for human \\nintervention. \\nInformation Integrity \\n'),\n",
              " Document(metadata={'source': 'dataset/data2.pdf', 'file_path': 'dataset/data2.pdf', 'page': 47, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=' \\n44 \\nMG-3.2-007 \\nLeverage feedback and recommendations from organizational boards or \\ncommittees related to the deployment of GAI applications and content \\nprovenance when using third-party pre-trained models. \\nInformation Integrity; Value Chain \\nand Component Integration \\nMG-3.2-008 \\nUse human moderation systems where appropriate to review generated content \\nin accordance with human-AI conﬁguration policies established in the Govern \\nfunction, aligned with socio-cultural norms in the context of use, and for settings \\nwhere AI models are demonstrated to perform poorly. \\nHuman-AI Conﬁguration \\nMG-3.2-009 \\nUse organizational risk tolerance to evaluate acceptable risks and performance \\nmetrics and decommission or retrain pre-trained models that perform outside of \\ndeﬁned limits. \\nCBRN Information or Capabilities; \\nConfabulation \\nAI Actor Tasks: AI Deployment, Operation and Monitoring, Third-party entities \\n \\nMANAGE 4.1: Post-deployment AI system monitoring plans are implemented, including mechanisms for capturing and evaluating \\ninput from users and other relevant AI Actors, appeal and override, decommissioning, incident response, recovery, and change \\nmanagement. \\nAction ID \\nSuggested Action \\nGAI Risks \\nMG-4.1-001 \\nCollaborate with external researchers, industry experts, and community \\nrepresentatives to maintain awareness of emerging best practices and \\ntechnologies in measuring and managing identiﬁed risks. \\nInformation Integrity; Harmful Bias \\nand Homogenization \\nMG-4.1-002 \\nEstablish, maintain, and evaluate eﬀectiveness of organizational processes and \\nprocedures for post-deployment monitoring of GAI systems, particularly for \\npotential confabulation, CBRN, or cyber risks. \\nCBRN Information or Capabilities; \\nConfabulation; Information \\nSecurity \\nMG-4.1-003 \\nEvaluate the use of sentiment analysis to gauge user sentiment regarding GAI \\ncontent performance and impact, and work in collaboration with AI Actors \\nexperienced in user research and experience. \\nHuman-AI Conﬁguration \\nMG-4.1-004 Implement active learning techniques to identify instances where the model fails \\nor produces unexpected outputs. \\nConfabulation \\nMG-4.1-005 \\nShare transparency reports with internal and external stakeholders that detail \\nsteps taken to update the GAI system to enhance transparency and \\naccountability. \\nHuman-AI Conﬁguration; Harmful \\nBias and Homogenization \\nMG-4.1-006 \\nTrack dataset modiﬁcations for provenance by monitoring data deletions, \\nrectiﬁcation requests, and other changes that may impact the veriﬁability of \\ncontent origins. \\nInformation Integrity \\n'),\n",
              " Document(metadata={'source': 'dataset/data2.pdf', 'file_path': 'dataset/data2.pdf', 'page': 48, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=' \\n45 \\nMG-4.1-007 \\nVerify that AI Actors responsible for monitoring reported issues can eﬀectively \\nevaluate GAI system performance including the application of content \\nprovenance data tracking techniques, and promptly escalate issues for response. \\nHuman-AI Conﬁguration; \\nInformation Integrity \\nAI Actor Tasks: AI Deployment, Aﬀected Individuals and Communities, Domain Experts, End-Users, Human Factors, Operation and \\nMonitoring \\n \\nMANAGE 4.2: Measurable activities for continual improvements are integrated into AI system updates and include regular \\nengagement with interested parties, including relevant AI Actors. \\nAction ID \\nSuggested Action \\nGAI Risks \\nMG-4.2-001 Conduct regular monitoring of GAI systems and publish reports detailing the \\nperformance, feedback received, and improvements made. \\nHarmful Bias and Homogenization \\nMG-4.2-002 \\nPractice and follow incident response plans for addressing the generation of \\ninappropriate or harmful content and adapt processes based on ﬁndings to \\nprevent future occurrences. Conduct post-mortem analyses of incidents with \\nrelevant AI Actors, to understand the root causes and implement preventive \\nmeasures. \\nHuman-AI Conﬁguration; \\nDangerous, Violent, or Hateful \\nContent \\nMG-4.2-003 Use visualizations or other methods to represent GAI model behavior to ease \\nnon-technical stakeholders understanding of GAI system functionality. \\nHuman-AI Conﬁguration \\nAI Actor Tasks: AI Deployment, AI Design, AI Development, Aﬀected Individuals and Communities, End-Users, Operation and \\nMonitoring, TEVV \\n \\nMANAGE 4.3: Incidents and errors are communicated to relevant AI Actors, including aﬀected communities. Processes for tracking, \\nresponding to, and recovering from incidents and errors are followed and documented. \\nAction ID \\nSuggested Action \\nGAI Risks \\nMG-4.3-001 \\nConduct after-action assessments for GAI system incidents to verify incident \\nresponse and recovery processes are followed and eﬀective, including to follow \\nprocedures for communicating incidents to relevant AI Actors and where \\napplicable, relevant legal and regulatory bodies.  \\nInformation Security \\nMG-4.3-002 Establish and maintain policies and procedures to record and track GAI system \\nreported errors, near-misses, and negative impacts. \\nConfabulation; Information \\nIntegrity \\n'),\n",
              " Document(metadata={'source': 'dataset/data2.pdf', 'file_path': 'dataset/data2.pdf', 'page': 49, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=' \\n46 \\nMG-4.3-003 \\nReport GAI incidents in compliance with legal and regulatory requirements (e.g., \\nHIPAA breach reporting, e.g., OCR (2023) or NHTSA (2022) autonomous vehicle \\ncrash reporting requirements. \\nInformation Security; Data Privacy \\nAI Actor Tasks: AI Deployment, Aﬀected Individuals and Communities, Domain Experts, End-Users, Human Factors, Operation and \\nMonitoring \\n \\n \\n \\n'),\n",
              " Document(metadata={'source': 'dataset/data2.pdf', 'file_path': 'dataset/data2.pdf', 'page': 50, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=' \\n47 \\nAppendix A. Primary GAI Considerations \\nThe following primary considerations were derived as overarching themes from the GAI PWG \\nconsultation process. These considerations (Governance, Pre-Deployment Testing, Content Provenance, \\nand Incident Disclosure) are relevant for voluntary use by any organization designing, developing, and \\nusing GAI and also inform the Actions to Manage GAI risks. Information included about the primary \\nconsiderations is not exhaustive, but highlights the most relevant topics derived from the GAI PWG.  \\nAcknowledgments: These considerations could not have been surfaced without the helpful analysis and \\ncontributions from the community and NIST staﬀ GAI PWG leads: George Awad, Luca Belli, Harold Booth, \\nMat Heyman, Yooyoung Lee, Mark Pryzbocki, Reva Schwartz, Martin Stanley, and Kyra Yee. \\nA.1. Governance \\nA.1.1. Overview \\nLike any other technology system, governance principles and techniques can be used to manage risks \\nrelated to generative AI models, capabilities, and applications. Organizations may choose to apply their \\nexisting risk tiering to GAI systems, or they may opt to revise or update AI system risk levels to address \\nthese unique GAI risks. This section describes how organizational governance regimes may be re-\\nevaluated and adjusted for GAI contexts. It also addresses third-party considerations for governing across \\nthe AI value chain.  \\nA.1.2. Organizational Governance \\nGAI opportunities, risks and long-term performance characteristics are typically less well-understood \\nthan non-generative AI tools and may be perceived and acted upon by humans in ways that vary greatly. \\nAccordingly, GAI may call for diﬀerent levels of oversight from AI Actors or diﬀerent human-AI \\nconﬁgurations in order to manage their risks eﬀectively. Organizations’ use of GAI systems may also \\nwarrant additional human review, tracking and documentation, and greater management oversight.  \\nAI technology can produce varied outputs in multiple modalities and present many classes of user \\ninterfaces. This leads to a broader set of AI Actors interacting with GAI systems for widely diﬀering \\napplications and contexts of use. These can include data labeling and preparation, development of GAI \\nmodels, content moderation, code generation and review, text generation and editing, image and video \\ngeneration, summarization, search, and chat. These activities can take place within organizational \\nsettings or in the public domain. \\nOrganizations can restrict AI applications that cause harm, exceed stated risk tolerances, or that conﬂict \\nwith their tolerances or values. Governance tools and protocols that are applied to other types of AI \\nsystems can be applied to GAI systems. These plans and actions include: \\n• Accessibility and reasonable \\naccommodations \\n• AI actor credentials and qualiﬁcations  \\n• Alignment to organizational values \\n• Auditing and assessment \\n• Change-management controls \\n• Commercial use \\n• Data provenance \\n'),\n",
              " Document(metadata={'source': 'dataset/data2.pdf', 'file_path': 'dataset/data2.pdf', 'page': 51, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=' \\n48 \\n• Data protection \\n• Data retention  \\n• Consistency in use of deﬁning key terms \\n• Decommissioning \\n• Discouraging anonymous use \\n• Education  \\n• Impact assessments  \\n• Incident response \\n• Monitoring \\n• Opt-outs  \\n• Risk-based controls \\n• Risk mapping and measurement \\n• Science-backed TEVV practices \\n• Secure software development practices \\n• Stakeholder engagement \\n• Synthetic content detection and \\nlabeling tools and techniques \\n• Whistleblower protections \\n• Workforce diversity and \\ninterdisciplinary teams\\nEstablishing acceptable use policies and guidance for the use of GAI in formal human-AI teaming settings \\nas well as diﬀerent levels of human-AI conﬁgurations can help to decrease risks arising from misuse, \\nabuse, inappropriate repurpose, and misalignment between systems and users. These practices are just \\none example of adapting existing governance protocols for GAI contexts.  \\nA.1.3. Third-Party Considerations \\nOrganizations may seek to acquire, embed, incorporate, or use open-source or proprietary third-party \\nGAI models, systems, or generated data for various applications across an enterprise. Use of these GAI \\ntools and inputs has implications for all functions of the organization – including but not limited to \\nacquisition, human resources, legal, compliance, and IT services – regardless of whether they are carried \\nout by employees or third parties. Many of the actions cited above are relevant and options for \\naddressing third-party considerations. \\nThird party GAI integrations may give rise to increased intellectual property, data privacy, or information \\nsecurity risks, pointing to the need for clear guidelines for transparency and risk management regarding \\nthe collection and use of third-party data for model inputs. Organizations may consider varying risk \\ncontrols for foundation models, ﬁne-tuned models, and embedded tools, enhanced processes for \\ninteracting with external GAI technologies or service providers. Organizations can apply standard or \\nexisting risk controls and processes to proprietary or open-source GAI technologies, data, and third-party \\nservice providers, including acquisition and procurement due diligence, requests for software bills of \\nmaterials (SBOMs), application of service level agreements (SLAs), and statement on standards for \\nattestation engagement (SSAE) reports to help with third-party transparency and risk management for \\nGAI systems. \\nA.1.4. Pre-Deployment Testing \\nOverview \\nThe diverse ways and contexts in which GAI systems may be developed, used, and repurposed \\ncomplicates risk mapping and pre-deployment measurement eﬀorts. Robust test, evaluation, validation, \\nand veriﬁcation (TEVV) processes can be iteratively applied – and documented – in early stages of the AI \\nlifecycle and informed by representative AI Actors (see Figure 3 of the AI RMF). Until new and rigorous \\n'),\n",
              " Document(metadata={'source': 'dataset/data2.pdf', 'file_path': 'dataset/data2.pdf', 'page': 52, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=' \\n49 \\nearly lifecycle TEVV approaches are developed and matured for GAI, organizations may use \\nrecommended “pre-deployment testing” practices to measure performance, capabilities, limits, risks, \\nand impacts. This section describes risk measurement and estimation as part of pre-deployment TEVV, \\nand examines the state of play for pre-deployment testing methodologies.  \\nLimitations of Current Pre-deployment Test Approaches \\nCurrently available pre-deployment TEVV processes used for GAI applications may be inadequate, non-\\nsystematically applied, or fail to reﬂect or mismatched to deployment contexts. For example, the \\nanecdotal testing of GAI system capabilities through video games or standardized tests designed for \\nhumans (e.g., intelligence tests, professional licensing exams) does not guarantee GAI system validity or \\nreliability in those domains. Similarly, jailbreaking or prompt engineering tests may not systematically \\nassess validity or reliability risks.  \\nMeasurement gaps can arise from mismatches between laboratory and real-world settings. Current \\ntesting approaches often remain focused on laboratory conditions or restricted to benchmark test \\ndatasets and in silico techniques that may not extrapolate well to—or directly assess GAI impacts in real-\\nworld conditions. For example, current measurement gaps for GAI make it diﬃcult to precisely estimate \\nits potential ecosystem-level or longitudinal risks and related political, social, and economic impacts. \\nGaps between benchmarks and real-world use of GAI systems may likely be exacerbated due to prompt \\nsensitivity and broad heterogeneity of contexts of use. \\nA.1.5. Structured Public Feedback \\nStructured public feedback can be used to evaluate whether GAI systems are performing as intended \\nand to calibrate and verify traditional measurement methods. Examples of structured feedback include, \\nbut are not limited to: \\n• \\nParticipatory Engagement Methods: Methods used to solicit feedback from civil society groups, \\naﬀected communities, and users, including focus groups, small user studies, and surveys. \\n• \\nField Testing: Methods used to determine how people interact with, consume, use, and make \\nsense of AI-generated information, and subsequent actions and eﬀects, including UX, usability, \\nand other structured, randomized experiments.  \\n• \\nAI Red-teaming: A structured testing exercise used to probe an AI system to ﬁnd ﬂaws and \\nvulnerabilities such as inaccurate, harmful, or discriminatory outputs, often in a controlled \\nenvironment and in collaboration with system developers. \\nInformation gathered from structured public feedback can inform design, implementation, deployment \\napproval, maintenance, or decommissioning decisions. Results and insights gleaned from these exercises \\ncan serve multiple purposes, including improving data quality and preprocessing, bolstering governance \\ndecision making, and enhancing system documentation and debugging practices. When implementing \\nfeedback activities, organizations should follow human subjects research requirements and best \\npractices such as informed consent and subject compensation. \\n'),\n",
              " Document(metadata={'source': 'dataset/data2.pdf', 'file_path': 'dataset/data2.pdf', 'page': 53, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=' \\n50 \\nParticipatory Engagement Methods \\nOn an ad hoc or more structured basis, organizations can design and use a variety of channels to engage \\nexternal stakeholders in product development or review. Focus groups with select experts can provide \\nfeedback on a range of issues. Small user studies can provide feedback from representative groups or \\npopulations. Anonymous surveys can be used to poll or gauge reactions to speciﬁc features. Participatory \\nengagement methods are often less structured than ﬁeld testing or red teaming, and are more \\ncommonly used in early stages of AI or product development.  \\nField Testing \\nField testing involves structured settings to evaluate risks and impacts and to simulate the conditions \\nunder which the GAI system will be deployed. Field style tests can be adapted from a focus on user \\npreferences and experiences towards AI risks and impacts – both negative and positive. When carried \\nout with large groups of users, these tests can provide estimations of the likelihood of risks and impacts \\nin real world interactions. \\nOrganizations may also collect feedback on outcomes, harms, and user experience directly from users in \\nthe production environment after a model has been released, in accordance with human subject \\nstandards such as informed consent and compensation. Organizations should follow applicable human \\nsubjects research requirements, and best practices such as informed consent and subject compensation, \\nwhen implementing feedback activities. \\nAI Red-teaming \\nAI red-teaming is an evolving practice that references exercises often conducted in a controlled \\nenvironment and in collaboration with AI developers building AI models to identify potential adverse \\nbehavior or outcomes of a GAI model or system, how they could occur, and stress test safeguards”. AI \\nred-teaming can be performed before or after AI models or systems are made available to the broader \\npublic; this section focuses on red-teaming in pre-deployment contexts.  \\nThe quality of AI red-teaming outputs is related to the background and expertise of the AI red team \\nitself. Demographically and interdisciplinarily diverse AI red teams can be used to identify ﬂaws in the \\nvarying contexts where GAI will be used. For best results, AI red teams should demonstrate domain \\nexpertise, and awareness of socio-cultural aspects within the deployment context. AI red-teaming results \\nshould be given additional analysis before they are incorporated into organizational governance and \\ndecision making, policy and procedural updates, and AI risk management eﬀorts. \\nVarious types of AI red-teaming may be appropriate, depending on the use case: \\n• \\nGeneral Public: Performed by general users (not necessarily AI or technical experts) who are \\nexpected to use the model or interact with its outputs, and who bring their own lived \\nexperiences and perspectives to the task of AI red-teaming. These individuals may have been \\nprovided instructions and material to complete tasks which may elicit harmful model behaviors. \\nThis type of exercise can be more eﬀective with large groups of AI red-teamers. \\n• \\nExpert: Performed by specialists with expertise in the domain or speciﬁc AI red-teaming context \\nof use (e.g., medicine, biotech, cybersecurity).  \\n• \\nCombination: In scenarios when it is diﬃcult to identify and recruit specialists with suﬃcient \\ndomain and contextual expertise, AI red-teaming exercises may leverage both expert and \\n'),\n",
              " Document(metadata={'source': 'dataset/data2.pdf', 'file_path': 'dataset/data2.pdf', 'page': 54, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=' \\n51 \\ngeneral public participants. For example, expert AI red-teamers could modify or verify the \\nprompts written by general public AI red-teamers. These approaches may also expand coverage \\nof the AI risk attack surface.  \\n• \\nHuman / AI: Performed by GAI in combination with specialist or non-specialist human teams. \\nGAI-led red-teaming can be more cost eﬀective than human red-teamers alone. Human or GAI-\\nled AI red-teaming may be better suited for eliciting diﬀerent types of harms. \\n \\nA.1.6. Content Provenance \\nOverview \\nGAI technologies can be leveraged for many applications such as content generation and synthetic data. \\nSome aspects of GAI outputs, such as the production of deepfake content, can challenge our ability to \\ndistinguish human-generated content from AI-generated synthetic content. To help manage and mitigate \\nthese risks, digital transparency mechanisms like provenance data tracking can trace the origin and \\nhistory of content. Provenance data tracking and synthetic content detection can help facilitate greater \\ninformation access about both authentic and synthetic content to users, enabling better knowledge of \\ntrustworthiness in AI systems. When combined with other organizational accountability mechanisms, \\ndigital content transparency approaches can enable processes to trace negative outcomes back to their \\nsource, improve information integrity, and uphold public trust. Provenance data tracking and synthetic \\ncontent detection mechanisms provide information about the origin and history of content to assist in \\nGAI risk management eﬀorts. \\nProvenance metadata can include information about GAI model developers or creators of GAI content, \\ndate/time of creation, location, modiﬁcations, and sources. Metadata can be tracked for text, images, \\nvideos, audio, and underlying datasets. The implementation of provenance data tracking techniques can \\nhelp assess the authenticity, integrity, intellectual property rights, and potential manipulations in digital \\ncontent. Some well-known techniques for provenance data tracking include digital watermarking, \\nmetadata recording, digital ﬁngerprinting, and human authentication, among others. \\nProvenance Data Tracking Approaches \\nProvenance data tracking techniques for GAI systems can be used to track the history and origin of data \\ninputs, metadata, and synthetic content. Provenance data tracking records the origin and history for \\ndigital content, allowing its authenticity to be determined. It consists of techniques to record metadata \\nas well as overt and covert digital watermarks on content. Data provenance refers to tracking the origin \\nand history of input data through metadata and digital watermarking techniques. Provenance data \\ntracking processes can include and assist AI Actors across the lifecycle who may not have full visibility or \\ncontrol over the various trade-oﬀs and cascading impacts of early-stage model decisions on downstream \\nperformance and synthetic outputs. For example, by selecting a watermarking model to prioritize \\nrobustness (the durability of a watermark), an AI actor may inadvertently diminish computational \\ncomplexity (the resources required to implement watermarking). Organizational risk management \\neﬀorts for enhancing content provenance include:  \\n• \\nTracking provenance of training data and metadata for GAI systems; \\n• \\nDocumenting provenance data limitations within GAI systems; \\n'),\n",
              " Document(metadata={'source': 'dataset/data2.pdf', 'file_path': 'dataset/data2.pdf', 'page': 55, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=' \\n52 \\n• \\nMonitoring system capabilities and limitations in deployment through rigorous TEVV processes; \\n• \\nEvaluating how humans engage, interact with, or adapt to GAI content (especially in decision \\nmaking tasks informed by GAI content), and how they react to applied provenance techniques \\nsuch as overt disclosures. \\nOrganizations can document and delineate GAI system objectives and limitations to identify gaps where \\nprovenance data may be most useful. For instance, GAI systems used for content creation may require \\nrobust watermarking techniques and corresponding detectors to identify the source of content or \\nmetadata recording techniques and metadata management tools and repositories to trace content \\norigins and modiﬁcations. Further narrowing of GAI task deﬁnitions to include provenance data can \\nenable organizations to maximize the utility of provenance data and risk management eﬀorts. \\nA.1.7. Enhancing Content Provenance through Structured Public Feedback \\nWhile indirect feedback methods such as automated error collection systems are useful, they often lack \\nthe context and depth that direct input from end users can provide. Organizations can leverage feedback \\napproaches described in the Pre-Deployment Testing section to capture input from external sources such \\nas through AI red-teaming.  \\nIntegrating pre- and post-deployment external feedback into the monitoring process for GAI models and \\ncorresponding applications can help enhance awareness of performance changes and mitigate potential \\nrisks and harms from outputs. There are many ways to capture and make use of user feedback – before \\nand after GAI systems and digital content transparency approaches are deployed – to gain insights about \\nauthentication eﬃcacy and vulnerabilities, impacts of adversarial threats on techniques, and unintended \\nconsequences resulting from the utilization of content provenance approaches on users and \\ncommunities. Furthermore, organizations can track and document the provenance of datasets to identify \\ninstances in which AI-generated data is a potential root cause of performance issues with the GAI \\nsystem. \\nA.1.8. Incident Disclosure \\nOverview \\nAI incidents can be deﬁned as an “event, circumstance, or series of events where the development, use, \\nor malfunction of one or more AI systems directly or indirectly contributes to one of the following harms: \\ninjury or harm to the health of a person or groups of people (including psychological harms and harms to \\nmental health); disruption of the management and operation of critical infrastructure; violations of \\nhuman rights or a breach of obligations under applicable law intended to protect fundamental, labor, \\nand intellectual property rights; or harm to property, communities, or the environment.” AI incidents can \\noccur in the aggregate (i.e., for systemic discrimination) or acutely (i.e., for one individual). \\nState of AI Incident Tracking and Disclosure \\nFormal channels do not currently exist to report and document AI incidents. However, a number of \\npublicly available databases have been created to document their occurrence. These reporting channels \\nmake decisions on an ad hoc basis about what kinds of incidents to track. Some, for example, track by \\namount of media coverage.  \\n'),\n",
              " Document(metadata={'source': 'dataset/data2.pdf', 'file_path': 'dataset/data2.pdf', 'page': 56, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=' \\n53 \\nDocumenting, reporting, and sharing information about GAI incidents can help mitigate and prevent \\nharmful outcomes by assisting relevant AI Actors in tracing impacts to their source. Greater awareness \\nand standardization of GAI incident reporting could promote this transparency and improve GAI risk \\nmanagement across the AI ecosystem.  \\nDocumentation and Involvement of AI Actors \\nAI Actors should be aware of their roles in reporting AI incidents. To better understand previous incidents \\nand implement measures to prevent similar ones in the future, organizations could consider developing \\nguidelines for publicly available incident reporting which include information about AI actor \\nresponsibilities. These guidelines would help AI system operators identify GAI incidents across the AI \\nlifecycle and with AI Actors regardless of role. Documentation and review of third-party inputs and \\nplugins for GAI systems is especially important for AI Actors in the context of incident disclosure; LLM \\ninputs and content delivered through these plugins is often distributed, with inconsistent or insuﬃcient \\naccess control. \\nDocumentation practices including logging, recording, and analyzing GAI incidents can facilitate \\nsmoother sharing of information with relevant AI Actors. Regular information sharing, change \\nmanagement records, version history and metadata can also empower AI Actors responding to and \\nmanaging AI incidents.  \\n \\n'),\n",
              " Document(metadata={'source': 'dataset/data2.pdf', 'file_path': 'dataset/data2.pdf', 'page': 57, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=' \\n54 \\nAppendix B. References \\nAcemoglu, D. (2024) The Simple Macroeconomics of AI https://www.nber.org/papers/w32487 \\nAI Incident Database. https://incidentdatabase.ai/ \\nAtherton, D. (2024) Deepfakes and Child Safety: A Survey and Analysis of 2023 Incidents and Responses. \\nAI Incident Database. https://incidentdatabase.ai/blog/deepfakes-and-child-safety/ \\nBadyal, N. et al. (2023) Intentional Biases in LLM Responses. arXiv. https://arxiv.org/pdf/2311.07611 \\nBing Chat: Data Exﬁltration Exploit Explained. Embrace The Red. \\nhttps://embracethered.com/blog/posts/2023/bing-chat-data-exﬁltration-poc-and-ﬁx/ \\nBommasani, R. et al. (2022) Picking on the Same Person: Does Algorithmic Monoculture lead to Outcome \\nHomogenization? arXiv. https://arxiv.org/pdf/2211.13972 \\nBoyarskaya, M. et al. (2020) Overcoming Failures of Imagination in AI Infused System Development and \\nDeployment. arXiv. https://arxiv.org/pdf/2011.13416 \\nBrowne, D. et al. (2023) Securing the AI Pipeline. Mandiant. \\nhttps://www.mandiant.com/resources/blog/securing-ai-pipeline \\nBurgess, M. (2024) Generative AI’s Biggest Security Flaw Is Not Easy to Fix. WIRED. \\nhttps://www.wired.com/story/generative-ai-prompt-injection-hacking/ \\nBurtell, M. et al. (2024) The Surprising Power of Next Word Prediction: Large Language Models \\nExplained, Part 1. Georgetown Center for Security and Emerging Technology. \\nhttps://cset.georgetown.edu/article/the-surprising-power-of-next-word-prediction-large-language-\\nmodels-explained-part-1/ \\nCanadian Centre for Cyber Security (2023) Generative artiﬁcial intelligence (AI) - ITSAP.00.041. \\nhttps://www.cyber.gc.ca/en/guidance/generative-artiﬁcial-intelligence-ai-itsap00041 \\nCarlini, N., et al. (2021) Extracting Training Data from Large Language Models. Usenix. \\nhttps://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting \\nCarlini, N. et al. (2023) Quantifying Memorization Across Neural Language Models. ICLR 2023. \\nhttps://arxiv.org/pdf/2202.07646 \\nCarlini, N. et al. (2024) Stealing Part of a Production Language Model. arXiv. \\nhttps://arxiv.org/abs/2403.06634 \\nChandra, B. et al. (2023) Dismantling the Disinformation Business of Chinese Inﬂuence Operations. \\nRAND. https://www.rand.org/pubs/commentary/2023/10/dismantling-the-disinformation-business-of-\\nchinese.html \\nCiriello, R. et al. (2024) Ethical Tensions in Human-AI Companionship: A Dialectical Inquiry into Replika. \\nResearchGate. https://www.researchgate.net/publication/374505266_Ethical_Tensions_in_Human-\\nAI_Companionship_A_Dialectical_Inquiry_into_Replika \\nDahl, M. et al. (2024) Large Legal Fictions: Proﬁling Legal Hallucinations in Large Language Models. arXiv. \\nhttps://arxiv.org/abs/2401.01301 \\n'),\n",
              " Document(metadata={'source': 'dataset/data2.pdf', 'file_path': 'dataset/data2.pdf', 'page': 58, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=\" \\n55 \\nDe Angelo, D. (2024) Short, Mid and Long-Term Impacts of AI in Cybersecurity. Palo Alto Networks. \\nhttps://www.paloaltonetworks.com/blog/2024/02/impacts-of-ai-in-cybersecurity/ \\nDe Freitas, J. et al. (2023) Chatbots and Mental Health: Insights into the Safety of Generative AI. Harvard \\nBusiness School. https://www.hbs.edu/ris/Publication%20Files/23-011_c1bdd417-f717-47b6-bccb-\\n5438c6e65c1a_f6fd9798-3c2d-4932-b222-056231fe69d7.pdf \\nDietvorst, B. et al. (2014) Algorithm Aversion: People Erroneously Avoid Algorithms After Seeing Them \\nErr. Journal of Experimental Psychology. https://marketing.wharton.upenn.edu/wp-\\ncontent/uploads/2016/10/Dietvorst-Simmons-Massey-2014.pdf \\nDuhigg, C. (2012) How Companies Learn Your Secrets. New York Times. \\nhttps://www.nytimes.com/2012/02/19/magazine/shopping-habits.html \\nElsayed, G. et al. (2024) Images altered to trick machine vision can inﬂuence humans too. Google \\nDeepMind. https://deepmind.google/discover/blog/images-altered-to-trick-machine-vision-can-\\ninﬂuence-humans-too/ \\nEpstein, Z. et al. (2023). Art and the science of generative AI. Science. \\nhttps://www.science.org/doi/10.1126/science.adh4451 \\nFeﬀer, M. et al. (2024) Red-Teaming for Generative AI: Silver Bullet or Security Theater? arXiv. \\nhttps://arxiv.org/pdf/2401.15897 \\nGlazunov, S. et al. (2024) Project Naptime: Evaluating Oﬀensive Security Capabilities of Large Language \\nModels. Project Zero. https://googleprojectzero.blogspot.com/2024/06/project-naptime.html \\nGreshake, K. et al. (2023) Not what you've signed up for: Compromising Real-World LLM-Integrated \\nApplications with Indirect Prompt Injection. arXiv. https://arxiv.org/abs/2302.12173 \\nHagan, M. (2024) Good AI Legal Help, Bad AI Legal Help: Establishing quality standards for responses to \\npeople’s legal problem stories. SSRN. https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4696936 \\nHaran, R. (2023) Securing LLM Systems Against Prompt Injection. NVIDIA. \\nhttps://developer.nvidia.com/blog/securing-llm-systems-against-prompt-injection/ \\nInformation Technology Industry Council (2024) Authenticating AI-Generated Content. \\nhttps://www.itic.org/policy/ITI_AIContentAuthorizationPolicy_122123.pdf \\nJain, S. et al. (2023) Algorithmic Pluralism: A Structural Approach To Equal Opportunity. arXiv. \\nhttps://arxiv.org/pdf/2305.08157 \\nJi, Z. et al (2023) Survey of Hallucination in Natural Language Generation. ACM Comput. Surv. 55, 12, \\nArticle 248. https://doi.org/10.1145/3571730 \\nJones-Jang, S. et al. (2022) How do people react to AI failure? Automation bias, algorithmic aversion, and \\nperceived controllability. Oxford. https://academic.oup.com/jcmc/article/28/1/zmac029/6827859] \\nJussupow, E. et al. (2020) Why Are We Averse Towards Algorithms? A Comprehensive Literature Review \\non Algorithm Aversion. ECIS 2020. https://aisel.aisnet.org/ecis2020_rp/168/ \\nKalai, A., et al. (2024) Calibrated Language Models Must Hallucinate. arXiv. \\nhttps://arxiv.org/pdf/2311.14648 \\n\"),\n",
              " Document(metadata={'source': 'dataset/data2.pdf', 'file_path': 'dataset/data2.pdf', 'page': 59, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=' \\n56 \\nKarasavva, V. et al. (2021) Personality, Attitudinal, and Demographic Predictors of Non-consensual \\nDissemination of Intimate Images. NIH. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9554400/ \\nKatzman, J., et al. (2023) Taxonomizing and measuring representational harms: a look at image tagging. \\nAAAI. https://dl.acm.org/doi/10.1609/aaai.v37i12.26670 \\nKhan, T. et al. (2024) From Code to Consumer: PAI’s Value Chain Analysis Illuminates Generative AI’s Key \\nPlayers. AI. https://partnershiponai.org/from-code-to-consumer-pais-value-chain-analysis-illuminates-\\ngenerative-ais-key-players/ \\nKirchenbauer, J. et al. (2023) A Watermark for Large Language Models. OpenReview. \\nhttps://openreview.net/forum?id=aX8ig9X2a7 \\nKleinberg, J. et al. (May 2021) Algorithmic monoculture and social welfare. PNAS. \\nhttps://www.pnas.org/doi/10.1073/pnas.2018340118 \\nLakatos, S. (2023) A Revealing Picture. Graphika. https://graphika.com/reports/a-revealing-picture \\nLee, H. et al. (2024) Deepfakes, Phrenology, Surveillance, and More! A Taxonomy of AI Privacy Risks. \\narXiv. https://arxiv.org/pdf/2310.07879 \\nLenaerts-Bergmans, B. (2024) Data Poisoning: The Exploitation of Generative AI. Crowdstrike. \\nhttps://www.crowdstrike.com/cybersecurity-101/cyberattacks/data-poisoning/ \\nLiang, W. et al. (2023) GPT detectors are biased against non-native English writers. arXiv. \\nhttps://arxiv.org/abs/2304.02819 \\nLuccioni, A. et al. (2023) Power Hungry Processing: Watts Driving the Cost of AI Deployment? arXiv. \\nhttps://arxiv.org/pdf/2311.16863 \\nMouton, C. et al. (2024) The Operational Risks of AI in Large-Scale Biological Attacks. RAND. \\nhttps://www.rand.org/pubs/research_reports/RRA2977-2.html. \\nNicoletti, L. et al. (2023) Humans Are Biased. Generative Ai Is Even Worse. Bloomberg. \\nhttps://www.bloomberg.com/graphics/2023-generative-ai-bias/. \\nNational Institute of Standards and Technology (2024) Adversarial Machine Learning: A Taxonomy and \\nTerminology of Attacks and Mitigations https://csrc.nist.gov/pubs/ai/100/2/e2023/ﬁnal \\nNational Institute of Standards and Technology (2023) AI Risk Management Framework. \\nhttps://www.nist.gov/itl/ai-risk-management-framework \\nNational Institute of Standards and Technology (2023) AI Risk Management Framework, Chapter 3: AI \\nRisks and Trustworthiness. \\nhttps://airc.nist.gov/AI_RMF_Knowledge_Base/AI_RMF/Foundational_Information/3-sec-characteristics \\nNational Institute of Standards and Technology (2023) AI Risk Management Framework, Chapter 6: AI \\nRMF Proﬁles. https://airc.nist.gov/AI_RMF_Knowledge_Base/AI_RMF/Core_And_Proﬁles/6-sec-proﬁle \\nNational Institute of Standards and Technology (2023) AI Risk Management Framework, Appendix A: \\nDescriptions of AI Actor Tasks. \\nhttps://airc.nist.gov/AI_RMF_Knowledge_Base/AI_RMF/Appendices/Appendix_A#:~:text=AI%20actors%\\n20in%20this%20category,data%20providers%2C%20system%20funders%2C%20product \\n'),\n",
              " Document(metadata={'source': 'dataset/data2.pdf', 'file_path': 'dataset/data2.pdf', 'page': 60, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=' \\n57 \\nNational Institute of Standards and Technology (2023) AI Risk Management Framework, Appendix B: \\nHow AI Risks Diﬀer from Traditional Software Risks. \\nhttps://airc.nist.gov/AI_RMF_Knowledge_Base/AI_RMF/Appendices/Appendix_B \\nNational Institute of Standards and Technology (2023) AI RMF Playbook. \\nhttps://airc.nist.gov/AI_RMF_Knowledge_Base/Playbook \\nNational Institue of Standards and Technology (2023) Framing Risk \\nhttps://airc.nist.gov/AI_RMF_Knowledge_Base/AI_RMF/Foundational_Information/1-sec-risk \\nNational Institute of Standards and Technology (2023) The Language of Trustworthy AI: An In-Depth \\nGlossary of Terms https://airc.nist.gov/AI_RMF_Knowledge_Base/Glossary \\nNational Institue of Standards and Technology (2022) Towards a Standard for Identifying and Managing \\nBias in Artiﬁcial Intelligence https://www.nist.gov/publications/towards-standard-identifying-and-\\nmanaging-bias-artiﬁcial-intelligence \\nNorthcutt, C. et al. (2021) Pervasive Label Errors in Test Sets Destabilize Machine Learning Benchmarks. \\narXiv. https://arxiv.org/pdf/2103.14749 \\nOECD (2023) \"Advancing accountability in AI: Governing and managing risks throughout the lifecycle for \\ntrustworthy AI\", OECD Digital Economy Papers, No. 349, OECD Publishing, Paris. \\nhttps://doi.org/10.1787/2448f04b-en \\nOECD (2024) \"Deﬁning AI incidents and related terms\" OECD Artiﬁcial Intelligence Papers, No. 16, OECD \\nPublishing, Paris. https://doi.org/10.1787/d1a8d965-en \\nOpenAI (2023) GPT-4 System Card. https://cdn.openai.com/papers/gpt-4-system-card.pdf \\nOpenAI (2024) GPT-4 Technical Report. https://arxiv.org/pdf/2303.08774 \\nPadmakumar, V. et al. (2024) Does writing with language models reduce content diversity? ICLR. \\nhttps://arxiv.org/pdf/2309.05196 \\nPark, P. et. al. (2024) AI deception: A survey of examples, risks, and potential solutions. Patterns, 5(5). \\narXiv. https://arxiv.org/pdf/2308.14752 \\nPartnership on AI (2023) Building a Glossary for Synthetic Media Transparency Methods, Part 1: Indirect \\nDisclosure. https://partnershiponai.org/glossary-for-synthetic-media-transparency-methods-part-1-\\nindirect-disclosure/ \\nQu, Y. et al. (2023) Unsafe Diﬀusion: On the Generation of Unsafe Images and Hateful Memes From Text-\\nTo-Image Models. arXiv. https://arxiv.org/pdf/2305.13873 \\nRafat, K. et al. (2023) Mitigating carbon footprint for knowledge distillation based deep learning model \\ncompression. PLOS One. https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0285668 \\nSaid, I. et al. (2022) Nonconsensual Distribution of Intimate Images: Exploring the Role of Legal Attitudes \\nin Victimization and Perpetration. Sage. \\nhttps://journals.sagepub.com/doi/full/10.1177/08862605221122834#bibr47-08862605221122834 \\nSandbrink, J. (2023) Artiﬁcial intelligence and biological misuse: Diﬀerentiating risks of language models \\nand biological design tools. arXiv. https://arxiv.org/pdf/2306.13952 \\n'),\n",
              " Document(metadata={'source': 'dataset/data2.pdf', 'file_path': 'dataset/data2.pdf', 'page': 61, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=' \\n58 \\nSatariano, A. et al. (2023) The People Onscreen Are Fake. The Disinformation Is Real. New York Times. \\nhttps://www.nytimes.com/2023/02/07/technology/artiﬁcial-intelligence-training-deepfake.html \\nSchaul, K. et al. (2024) Inside the secret list of websites that make AI like ChatGPT sound smart. \\nWashington Post. https://www.washingtonpost.com/technology/interactive/2023/ai-chatbot-learning/ \\nScheurer, J. et al. (2023) Technical report: Large language models can strategically deceive their users \\nwhen put under pressure. arXiv. https://arxiv.org/abs/2311.07590 \\nShelby, R. et al. (2023) Sociotechnical Harms of Algorithmic Systems: Scoping a Taxonomy for Harm \\nReduction. arXiv. https://arxiv.org/pdf/2210.05791 \\nShevlane, T. et al. (2023) Model evaluation for extreme risks. arXiv. https://arxiv.org/pdf/2305.15324 \\nShumailov, I. et al. (2023) The curse of recursion: training on generated data makes models forget. arXiv. \\nhttps://arxiv.org/pdf/2305.17493v2 \\nSmith, A. et al. (2023) Hallucination or Confabulation? Neuroanatomy as metaphor in Large Language \\nModels. PLOS Digital Health. \\nhttps://journals.plos.org/digitalhealth/article?id=10.1371/journal.pdig.0000388 \\nSoice, E. et al. (2023) Can large language models democratize access to dual-use biotechnology? arXiv. \\nhttps://arxiv.org/abs/2306.03809 \\nSolaiman, I. et al. (2023) The Gradient of Generative AI Release: Methods and Considerations. arXiv. \\nhttps://arxiv.org/abs/2302.04844 \\nStaab, R. et al. (2023) Beyond Memorization: Violating Privacy via Inference With Large Language \\nModels. arXiv. https://arxiv.org/pdf/2310.07298 \\nStanford, S. et al. (2023) Whose Opinions Do Language Models Reﬂect? arXiv. \\nhttps://arxiv.org/pdf/2303.17548 \\nStrubell, E. et al. (2019) Energy and Policy Considerations for Deep Learning in NLP. arXiv. \\nhttps://arxiv.org/pdf/1906.02243 \\nThe White House (2016) Circular No. A-130, Managing Information as a Strategic Resource. \\nhttps://www.whitehouse.gov/wp-\\ncontent/uploads/legacy_drupal_ﬁles/omb/circulars/A130/a130revised.pdf \\nThe White House (2023) Executive Order on the Safe, Secure, and Trustworthy Development and Use of \\nArtiﬁcial Intelligence. https://www.whitehouse.gov/brieﬁng-room/presidential-\\nactions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-\\nartiﬁcial-intelligence/ \\nThe White House (2022) Roadmap for Researchers on Priorities Related to Information Integrity \\nResearch and Development. https://www.whitehouse.gov/wp-content/uploads/2022/12/Roadmap-\\nInformation-Integrity-RD-2022.pdf? \\nThiel, D. (2023) Investigation Finds AI Image Generation Models Trained on Child Abuse. Stanford Cyber \\nPolicy Center. https://cyber.fsi.stanford.edu/news/investigation-ﬁnds-ai-image-generation-models-\\ntrained-child-abuse \\n'),\n",
              " Document(metadata={'source': 'dataset/data2.pdf', 'file_path': 'dataset/data2.pdf', 'page': 62, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=' \\n59 \\nTirrell, L. (2017) Toxic Speech: Toward an Epidemiology of Discursive Harm. Philosophical Topics, 45(2), \\n139-162. https://www.jstor.org/stable/26529441  \\nTufekci, Z. (2015) Algorithmic Harms Beyond Facebook and Google: Emergent Challenges of \\nComputational Agency. Colorado Technology Law Journal. https://ctlj.colorado.edu/wp-\\ncontent/uploads/2015/08/Tufekci-ﬁnal.pdf \\nTurri, V. et al. (2023) Why We Need to Know More: Exploring the State of AI Incident Documentation \\nPractices. AAAI/ACM Conference on AI, Ethics, and Society. \\nhttps://dl.acm.org/doi/fullHtml/10.1145/3600211.3604700 \\nUrbina, F. et al. (2022) Dual use of artiﬁcial-intelligence-powered drug discovery. Nature Machine \\nIntelligence. https://www.nature.com/articles/s42256-022-00465-9 \\nWang, X. et al. (2023) Energy and Carbon Considerations of Fine-Tuning BERT. ACL Anthology. \\nhttps://aclanthology.org/2023.ﬁndings-emnlp.607.pdf \\nWang, Y. et al. (2023) Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs. arXiv. \\nhttps://arxiv.org/pdf/2308.13387 \\nWardle, C. et al. (2017) Information Disorder: Toward an interdisciplinary framework for research and \\npolicy making. Council of Europe. https://rm.coe.int/information-disorder-toward-an-interdisciplinary-\\nframework-for-researc/168076277c \\nWeatherbed, J. (2024) Trolls have ﬂooded X with graphic Taylor Swift AI fakes. The Verge. \\nhttps://www.theverge.com/2024/1/25/24050334/x-twitter-taylor-swift-ai-fake-images-trending \\nWei, J. et al. (2024) Long Form Factuality in Large Language Models. arXiv. \\nhttps://arxiv.org/pdf/2403.18802 \\nWeidinger, L. et al. (2021) Ethical and social risks of harm from Language Models. arXiv. \\nhttps://arxiv.org/pdf/2112.04359 \\nWeidinger, L. et al. (2023) Sociotechnical Safety Evaluation of Generative AI Systems. arXiv. \\nhttps://arxiv.org/pdf/2310.11986 \\nWeidinger, L. et al. (2022) Taxonomy of Risks posed by Language Models. FAccT ’22. \\nhttps://dl.acm.org/doi/pdf/10.1145/3531146.3533088 \\nWest, D. (2023) AI poses disproportionate risks to women. Brookings. \\nhttps://www.brookings.edu/articles/ai-poses-disproportionate-risks-to-women/ \\nWu, K. et al. (2024) How well do LLMs cite relevant medical references? An evaluation framework and \\nanalyses. arXiv. https://arxiv.org/pdf/2402.02008 \\nYin, L. et al. (2024) OpenAI’s GPT Is A Recruiter’s Dream Tool. Tests Show There’s Racial Bias. Bloomberg. \\nhttps://www.bloomberg.com/graphics/2024-openai-gpt-hiring-racial-discrimination/ \\nYu, Z. et al. (March 2024) Don’t Listen To Me: Understanding and Exploring Jailbreak Prompts of Large \\nLanguage Models. arXiv. https://arxiv.org/html/2403.17336v1 \\nZaugg, I. et al. (2022) Digitally-disadvantaged languages. Policy Review. \\nhttps://policyreview.info/pdf/policyreview-2022-2-1654.pdf \\n'),\n",
              " Document(metadata={'source': 'dataset/data2.pdf', 'file_path': 'dataset/data2.pdf', 'page': 63, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=' \\n60 \\nZhang, Y. et al. (2023) Human favoritism, not AI aversion: People’s perceptions (and bias) toward \\ngenerative AI, human experts, and human–GAI collaboration in persuasive content generation. Judgment \\nand Decision Making. https://www.cambridge.org/core/journals/judgment-and-decision-\\nmaking/article/human-favoritism-not-ai-aversion-peoples-perceptions-and-bias-toward-generative-ai-\\nhuman-experts-and-humangai-collaboration-in-persuasive-content-\\ngeneration/419C4BD9CE82673EAF1D8F6C350C4FA8 \\nZhang, Y. et al. (2023) Siren’s Song in the AI Ocean: A Survey on Hallucination in Large Language Models. \\narXiv. https://arxiv.org/pdf/2309.01219 \\nZhao, X. et al. (2023) Provable Robust Watermarking for AI-Generated Text. Semantic Scholar. \\nhttps://www.semanticscholar.org/paper/Provable-Robust-Watermarking-for-AI-Generated-Text-Zhao-\\nAnanth/75b68d0903af9d9f6e47ce3cf7e1a7d27ec811dc \\n')]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WM3b1Nelmx2"
      },
      "source": [
        "## Task 3: Generate Synthetic Data\n",
        "\n",
        "Let's first take a peek under the RAGAS hood to see what's happening when we generate a single example.\n",
        "\n",
        "For simplicities sake - we'll look at a flow that results in a reasoning question."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6W8Q6yr5maq"
      },
      "source": [
        "Actually creating our Synthetic Dataset is as simple as running the following cell!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "YxQrHsmvVU-9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\juanm\\anaconda3\\envs\\llms\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from ragas.testset.generator import TestsetGenerator\n",
        "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "\n",
        "generator_llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
        "critic_llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "generator = TestsetGenerator.from_langchain(\n",
        "    generator_llm,\n",
        "    critic_llm,\n",
        "    embeddings\n",
        ")\n",
        "\n",
        "distributions = {\n",
        "    simple: 0.5,\n",
        "    multi_context: 0.25,\n",
        "    reasoning: 0.25\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "07ab3dc0790241bbb85a7f488a42ef8c",
            "7710c7377cbc4c30b55b28b4bc99e88f",
            "41bdd49fab5f4826959d0d50663ff539",
            "60168d85131d4afc99d55d61ab954ee6",
            "9edf898aeeab40dda9b9475395776521",
            "5c2fda99d4204d85b1bf7ad354fd58d4",
            "93cd4d35c5fd41f5904ca1d52d1f52a8",
            "6eb8b2e3262c45248708a2082c366f0a",
            "095f680d37a3430fb82d223615662db5",
            "3a8537e37fc14fd9b16ca0ceee4fede6",
            "1160a44dc18e47b0890f70c40eaa7eb0",
            "33f063017b7c4c7fa8cbafc89674350b",
            "6864c81e2bcf459bbaf5acbb36bdfcbe",
            "59d6e269eadf429a924f6f79bc8ba4ba",
            "ca791fc471e34b9da2f9070fc1053c0f",
            "8baf0ed3d0f743f294e07f2b5407e820",
            "10df31709059484c99f102453d780473",
            "2508d229935744cbb5fc340222e2d660",
            "890e0dd7fa524ceca1e805cb6253ee71",
            "61b52ff459214129b8f7e6d67b192b78",
            "23863bc37a8645029934b8c106622c51",
            "5ab5f08afa5841709aedb2f78a52a11c"
          ]
        },
        "id": "ak_MfUrSWHpZ",
        "outputId": "4306cc9a-e240-4674-a33f-0331f94f4916"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Filename and doc_id are the same for all nodes.                   \n",
            "Generating:   0%|          | 0/16 [00:00<?, ?it/s][ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
            "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['AI system', 'Safety risks', 'Negative risk', 'System reliability', 'Real-time monitoring']\n",
            "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 2, 'structure': 2, 'relevance': 3, 'score': 2.25}\n",
            "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['OSTP conducted meetings', 'Private sector and civil society stakeholders', 'AI Bill of Rights', 'Positive use cases', 'Oversight possibilities']\n",
            "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 2, 'structure': 1, 'relevance': 2, 'score': 1.5}\n",
            "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Racial equity', 'Supreme Court Decision', 'Automated society', 'Privacy protection', 'Crime prediction software']\n",
            "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
            "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Continuous monitoring', 'GAI system impacts', 'Structured feedback mechanisms', 'Harmful Bias and Homogenization', 'Information Integrity']\n",
            "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
            "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Healthcare navigators', 'Automated customer service', 'Ballot curing laws', 'Fallback system', 'Voter signature matching algorithm']\n",
            "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
            "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Evaluations involving human subjects', 'Content provenance data', 'Data privacy', 'Anonymization', 'AI system performance']\n",
            "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 2, 'structure': 1, 'relevance': 2, 'score': 1.5}\n",
            "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['AI Risk Management Framework', 'Bias in Artificial Intelligence', 'Trustworthy AI', 'Language models', 'Synthetic media transparency']\n",
            "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
            "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Monitoring system capabilities', 'GAI content', 'Provenance data', 'Content provenance', 'Incident disclosure']\n",
            "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 3, 'depth': 3, 'structure': 3, 'relevance': 3, 'score': 3.0}\n",
            "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Automated systems', 'Protect the public from harm', 'Consultation', 'Testing', 'Risk identification and mitigation']\n",
            "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
            "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['GAI models', 'Synthetic NCII and CSAM', 'Value chain and component integration', 'Trustworthy AI characteristics', 'Suggested actions to manage GAI risks']\n",
            "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
            "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Monitoring system capabilities', 'GAI content', 'Provenance data', 'Content provenance', 'Incident disclosure']\n",
            "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 1, 'structure': 2, 'relevance': 2, 'score': 1.5}\n",
            "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Technical companion', 'AI Bill of Rights', 'Algorithmic discrimination protections', 'Data privacy', 'Human alternatives']\n",
            "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
            "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['AI system', 'Safety risks', 'Negative risk', 'System reliability', 'Real-time monitoring']\n",
            "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
            "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['AI system', 'Safety risks', 'Negative risk', 'System reliability', 'Real-time monitoring']\n",
            "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 1, 'structure': 2, 'relevance': 2, 'score': 1.5}\n",
            "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Technical companion', 'AI Bill of Rights', 'Algorithmic discrimination protections', 'Data privacy', 'Human alternatives']\n",
            "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
            "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['GAI models', 'Synthetic NCII and CSAM', 'Value chain and component integration', 'Trustworthy AI characteristics', 'Suggested actions to manage GAI risks']\n",
            "[ragas.testset.evolutions.INFO] seed question generated: \"What factors are considered when evaluating the negative risk of the AI system in the context provided?\"\n",
            "[ragas.testset.evolutions.INFO] seed question generated: How can structured feedback mechanisms be used to monitor and improve GAI system outputs?\n",
            "[ragas.testset.evolutions.INFO] seed question generated: \"How do ballot curing laws in at least 24 states address issues with the voter signature matching algorithm?\"\n",
            "[ragas.testset.evolutions.INFO] seed question generated: \"How are AI incidents defined and tracked in the absence of formal reporting channels?\"\n",
            "[ragas.testset.evolutions.INFO] seed question generated: \"How can organizations enhance content provenance through structured public feedback in monitoring system capabilities?\"\n",
            "[ragas.testset.evolutions.INFO] seed question generated: \"What are the characteristics of Trustworthy AI as mentioned in the given context?\"\n",
            "[ragas.testset.evolutions.INFO] seed question generated: What measures should be taken to protect the public from harm when developing automated systems?\n",
            "[ragas.testset.evolutions.INFO] seed question generated: \"What is the significance of the Executive Order On Advancing Racial Equity and Support for Underserved Communities Through the Federal Government?\"\n",
            "[ragas.testset.evolutions.INFO] seed question generated: What is the importance of building a glossary for synthetic media transparency methods, particularly in relation to indirect disclosure?\n",
            "[ragas.testset.evolutions.INFO] seed question generated: \"What is discussed in the section on data privacy in the technical companion to the Blueprint for an AI Bill of Rights?\"\n",
            "[ragas.testset.evolutions.INFO] seed question generated: How do safety metrics reflect system reliability in the evaluation of AI systems?\n",
            "[ragas.testset.evolutions.INFO] seed question generated: What is the purpose of the technical companion to the Blueprint for an AI Bill of Rights?\n",
            "[ragas.testset.evolutions.INFO] seed question generated: \"Which organizations were involved as private sector and civil society stakeholders in the meetings conducted by OSTP for the development of the Blueprint for an AI Bill of Rights?\"\n",
            "[ragas.testset.evolutions.INFO] seed question generated: How can evaluations involving human subjects meet applicable requirements and be representative of the relevant population?\n",
            "[ragas.testset.evolutions.INFO] seed question generated: How is the AI system evaluated for safety risks according to the given context?\n",
            "[ragas.testset.evolutions.INFO] seed question generated: What are some suggested actions to manage GAI risks?\n",
            "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the factors considered when evaluating the negative risk of an AI system, referencing 'the context provided' without actually including or describing this context within the question. This reliance on unspecified context makes the question unclear and potentially unanswerable for those who do not have access to that context. To improve clarity and answerability, the question should either include the relevant context directly or specify the type of AI system and the criteria for evaluating negative risk. Additionally, clarifying what is meant by 'negative risk' could enhance understanding.\", 'verdict': 0}\n",
            "[ragas.testset.evolutions.INFO] rewritten question: \"What factors are considered when evaluating the negative risk of the AI system in the context provided?\"\n",
            "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the importance of creating a glossary for synthetic media transparency methods, specifically focusing on indirect disclosure. It is clear in its intent and specifies the topic of interest, making it understandable. However, the term 'indirect disclosure' may not be universally understood without additional context. To improve clarity and answerability, the question could briefly define 'indirect disclosure' or explain its relevance to synthetic media transparency methods. This would help ensure that the question is accessible to a wider audience.\", 'verdict': 1}\n",
            "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the significance of a specific Executive Order related to racial equity and support for underserved communities. It is clear in its intent, seeking to understand the importance or implications of the Executive Order. However, the question could be improved by providing a bit more context about the Executive Order itself, such as its main goals or provisions, to ensure that the answer can be more focused and relevant. Additionally, specifying whether the inquiry is about its historical significance, practical implications, or societal impact could enhance clarity.', 'verdict': 1}\n",
            "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: \"What is the significance of the Executive Order On Advancing Racial Equity and Support for Underserved Communities Through the Federal Government?\"\n",
            "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question is specific and seeks information about the organizations involved as private sector and civil society stakeholders in meetings conducted by the OSTP regarding the AI Bill of Rights. It is clear in its intent and does not rely on external references, making it understandable and answerable based on the details provided. However, it could be improved by specifying the time frame of the meetings or the context in which these organizations were involved, as this would provide a clearer scope for the answer.', 'verdict': 1}\n",
            "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks how organizations can improve content provenance by utilizing structured public feedback in monitoring system capabilities. It is specific in its focus on organizations, content provenance, structured public feedback, and monitoring system capabilities, which makes the intent clear. However, the question may still be somewhat complex and could benefit from simplification or clarification of terms like 'content provenance' and 'monitoring system capabilities' for a broader audience. To enhance clarity and answerability, the question could be rephrased to define these terms or provide examples of what is meant by structured public feedback in this context.\", 'verdict': 1}\n",
            "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks for the characteristics of Trustworthy AI as mentioned in 'the given context', but it does not provide any details about what that context is. This reliance on unspecified external information makes the question unclear and unanswerable for those who do not have access to the context. To improve clarity and answerability, the question should either include a brief description of the context or specify the characteristics of Trustworthy AI that are being referred to. Additionally, it could clarify what aspects of Trustworthy AI (e.g., ethical considerations, transparency, accountability) are of interest.\", 'verdict': 0}\n",
            "[ragas.testset.evolutions.INFO] rewritten question: \"What are the characteristics of Trustworthy AI as mentioned in the given context?\"\n",
            "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about measures to protect the public from harm during the development of automated systems. It is clear in its intent, specifying the focus on public safety in the context of automated systems. However, it could benefit from being more specific about the type of automated systems (e.g., AI, robotics, self-driving cars) or the context in which these measures are to be applied (e.g., regulatory frameworks, ethical guidelines). This additional detail would enhance clarity and allow for a more targeted response. Overall, the question is understandable and answerable based on the details provided.', 'verdict': 1}\n",
            "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: What measures should be taken to protect the public from harm when developing automated systems?\n",
            "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about ballot curing laws in at least 24 states and their relation to the voter signature matching algorithm. It is specific in its focus on ballot curing laws and the signature matching algorithm, which provides a clear intent. However, the question assumes knowledge of what ballot curing laws entail and how they relate to the signature matching algorithm without providing any context or definitions. To improve clarity and answerability, the question could briefly define 'ballot curing laws' and explain the significance of the 'voter signature matching algorithm' in this context. This would help ensure that the question is understandable to a broader audience.\", 'verdict': 1}\n",
            "[ragas.testset.evolutions.DEBUG] [ReasoningEvolution] simple question generated: \"How do ballot curing laws in at least 24 states address issues with the voter signature matching algorithm?\"\n",
            "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the use of structured feedback mechanisms to monitor and improve outputs from Generative AI (GAI) systems. It is specific in its focus on structured feedback mechanisms and their application to GAI systems, making the intent clear. However, the term 'structured feedback mechanisms' could be interpreted in various ways, and the question does not provide any context or examples of what these mechanisms might entail. To enhance clarity and answerability, the question could specify what types of structured feedback mechanisms are being considered (e.g., user feedback, automated evaluation metrics) or provide a brief context about the GAI systems in question.\", 'verdict': 1}\n",
            "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks for the purpose of a specific document, the technical companion to the Blueprint for an AI Bill of Rights. It is clear in its intent, specifying the document in question and what information is being sought (its purpose). However, it may require some prior knowledge about the Blueprint for an AI Bill of Rights to fully understand the context and significance of the technical companion. To enhance clarity and answerability for a broader audience, the question could briefly explain what the Blueprint for an AI Bill of Rights is or its relevance in the context of AI governance.', 'verdict': 1}\n",
            "[ragas.testset.evolutions.DEBUG] [ReasoningEvolution] simple question generated: What is the purpose of the technical companion to the Blueprint for an AI Bill of Rights?\n",
            "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the definition and tracking of AI incidents without formal reporting channels. It is specific in its focus on AI incidents and the context of lacking formal reporting mechanisms, making it relatively clear. However, the term 'AI incidents' could benefit from further clarification, as it may encompass a range of issues from ethical concerns to technical failures. To improve clarity and answerability, the question could specify what types of AI incidents are being referred to (e.g., ethical breaches, system failures) or provide examples of how tracking might occur in such scenarios. Overall, the question is understandable and answerable based on the details provided.\", 'verdict': 1}\n",
            "[ragas.testset.evolutions.DEBUG] [ReasoningEvolution] simple question generated: \"How are AI incidents defined and tracked in the absence of formal reporting channels?\"\n",
            "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the relationship between safety metrics and system reliability in the evaluation of AI systems. It is clear in its intent, specifying the focus on safety metrics and their role in assessing reliability. However, the question could benefit from being more specific about which safety metrics are being referred to, as well as the context of the AI systems in question (e.g., autonomous vehicles, healthcare applications). Providing examples or specifying the type of AI systems would enhance clarity and answerability. Overall, while the question is understandable, it could be improved by narrowing down the scope.', 'verdict': 1}\n",
            "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the content of a specific section on data privacy in a document referred to as the 'technical companion to the Blueprint for an AI Bill of Rights'. While it specifies the topic of interest (data privacy) and the document, it assumes familiarity with the document's content without providing any details or context. This reliance on external references makes the question less clear and answerable for those who do not have access to or knowledge of the document. To improve clarity and answerability, the question could include a brief summary of the document or specify what aspects of data privacy are of interest (e.g., principles, regulations, implications).\", 'verdict': 0}\n",
            "[ragas.testset.evolutions.INFO] rewritten question: \"What is discussed in the section on data privacy in the technical companion to the Blueprint for an AI Bill of Rights?\"\n",
            "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the evaluation of an AI system for safety risks, referencing 'the given context' without providing any details about what that context entails. This reliance on unspecified external information makes the question unclear and potentially unanswerable for those who do not have access to that context. To improve clarity and answerability, the question could either include a brief description of the context or specify the criteria or methods used for evaluating safety risks in AI systems. This would help ensure that the question is self-contained and understandable.\", 'verdict': 0}\n",
            "[ragas.testset.evolutions.INFO] rewritten question: How is the AI system evaluated for safety risks according to the given context?\n",
            "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: \"What steps should be taken to ensure the safety and effectiveness of automated systems in protecting the public from harm?\"\n",
            "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks for suggested actions to manage risks associated with Generative AI (GAI). It is clear in its intent and does not rely on external references, making it understandable and answerable. However, it could be improved by specifying the types of risks being referred to (e.g., ethical, security, operational) or the context in which these actions are to be applied (e.g., in a corporate setting, for developers, etc.). This would help narrow down the responses and provide more targeted suggestions.', 'verdict': 1}\n",
            "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: What are some suggested actions to manage GAI risks?\n",
            "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: \"What implications does the Executive Order On Advancing Racial Equity and Support for Underserved Communities Through the Federal Government have on algorithmic bias and public interest assessments?\"\n",
            "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks how evaluations involving human subjects can meet applicable requirements and be representative of the relevant population. It is clear in its intent, focusing on the standards and representativeness of evaluations. However, it lacks specificity regarding which applicable requirements are being referred to (e.g., ethical guidelines, regulatory standards) and what type of evaluations are being discussed (e.g., clinical trials, surveys). To improve clarity and answerability, the question could specify the context of the evaluations (e.g., medical research, social science studies) and the particular requirements of interest, which would help in providing a more focused and relevant response.', 'verdict': 0}\n",
            "[ragas.testset.evolutions.INFO] rewritten question: How can evaluations involving human subjects meet applicable requirements and be representative of the relevant population?\n",
            "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the factors considered when evaluating the negative risk of an AI system, referencing 'the context provided' without actually including or describing this context within the question. This reliance on unspecified context makes the question unclear and potentially unanswerable for those who do not have access to that context. To improve clarity and answerability, the question should either include the relevant context directly or specify the types of factors being considered (e.g., ethical implications, technical vulnerabilities) to provide a clearer framework for the inquiry.\", 'verdict': 0}\n",
            "[ragas.testset.evolutions.INFO] retrying evolution: 1 times\n",
            "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: \"What actions can be taken to manage GAI risks considering the potential for misuse and the integration of various components in the value chain?\"\n",
            "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks for the characteristics of Trustworthy AI as mentioned in 'the given context', but it does not provide any details about what that context is. This reliance on unspecified external information makes the question unclear and unanswerable for those who do not have access to the context. To improve clarity and answerability, the question should either include a brief description of the context or specify the characteristics of Trustworthy AI that are being referred to. Additionally, it could clarify what aspects of Trustworthy AI (e.g., ethical considerations, transparency, accountability) are of interest.\", 'verdict': 0}\n",
            "[ragas.testset.evolutions.INFO] retrying evolution: 1 times\n",
            "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
            "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Primary GAI Considerations', 'Governance', 'Pre-Deployment Testing', 'Content Provenance', 'Incident Disclosure']\n",
            "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question is specific and seeks to understand how organizations can improve awareness of AI incidents through external feedback and provenance data, without relying on formal reporting channels. It is clear in its intent and does not require additional context or references to be understood. However, it could be improved by specifying what types of organizations are being referred to (e.g., tech companies, healthcare organizations) or what kind of AI incidents are of interest. This would help narrow down the focus and make the question even more answerable.', 'verdict': 1}\n",
            "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'Organizations can enhance content provenance through structured public feedback by integrating pre- and post-deployment external feedback into the monitoring process for GAI models and corresponding applications. This helps enhance awareness of performance changes and mitigate potential risks and harms from outputs. By capturing input from external sources such as through AI red-teaming, organizations can gain insights about authentication efficacy, vulnerabilities, impacts of adversarial threats, and unintended consequences resulting from the utilization of content provenance approaches on users and communities.', 'verdict': 1}\n",
            "Generating:   6%|▋         | 1/16 [00:05<01:18,  5.25s/it][ragas.testset.evolutions.DEBUG] answer generated: {'answer': \"Safety metrics in the evaluation of AI systems reflect system reliability by assessing the system's ability to operate safely, its resilience to failures, real-time monitoring capabilities, and response times in case of AI system failures. These metrics ensure that the AI system is evaluated regularly for safety risks, with a focus on reliability and robustness.\", 'verdict': 1}\n",
            "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question is specific and seeks to understand how ballot curing laws in at least 24 states facilitate voters in correcting issues related to the voter signature matching algorithm. It clearly identifies the topic (ballot curing laws) and the context (voter signature matching algorithm), making the intent of the question understandable. However, it assumes familiarity with the concept of ballot curing laws and the signature matching algorithm without providing any context or definitions. To improve clarity and answerability, the question could briefly define what ballot curing laws are and how the signature matching algorithm works, or specify the types of issues voters might encounter.', 'verdict': 1}\n",
            "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks for steps to ensure the safety and effectiveness of automated systems in protecting the public from harm. It is clear in its intent, specifying the focus on automated systems and their role in public safety. However, the question is somewhat broad and could benefit from more specificity regarding the type of automated systems (e.g., self-driving cars, industrial robots, AI systems) or the context in which they are being used. To improve clarity and answerability, the question could specify the domain of automated systems being referred to or the particular aspects of safety and effectiveness that are of interest (e.g., regulatory measures, design principles, testing protocols).', 'verdict': 1}\n",
            "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the implications of a specific Executive Order on algorithmic bias and public interest assessments. It is clear in its intent, specifying the subject (the Executive Order) and the areas of impact (algorithmic bias and public interest assessments). However, the question may require some background knowledge about the Executive Order itself and its contents, which are not provided in the question. To improve clarity and answerability, the question could briefly summarize the key points of the Executive Order or specify which aspects of algorithmic bias and public interest assessments are being referred to. This would make it more accessible to those unfamiliar with the document.', 'verdict': 0}\n",
            "[ragas.testset.evolutions.INFO] rewritten question: \"What implications does the Executive Order On Advancing Racial Equity and Support for Underserved Communities Through the Federal Government have on algorithmic bias and public interest assessments?\"\n",
            "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the content of a specific section on data privacy in a document referred to as the 'technical companion to the Blueprint for an AI Bill of Rights'. While it specifies the topic of interest (data privacy) and the document, it assumes familiarity with the document itself without providing any context or details about its content. This reliance on external references makes the question less clear and answerable for those who may not have access to or knowledge of the document. To improve clarity and answerability, the question could include a brief summary of the document or specify what aspects of data privacy are of interest (e.g., regulations, ethical considerations).\", 'verdict': 0}\n",
            "[ragas.testset.evolutions.INFO] retrying evolution: 1 times\n",
            "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
            "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Purpose-built testing environment', 'NIST Dioptra', 'GAI trustworthy characteristics', 'AI system deployment', 'Human-AI Configuration']\n",
            "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the evaluation of an AI system for safety risks, referencing 'the given context' without providing any specific details about what that context entails. This reliance on unspecified external information makes the question unclear and potentially unanswerable for those who do not have access to the context. To improve clarity and answerability, the question could either include a brief description of the context or specify the criteria or methods used for evaluating safety risks in AI systems. Additionally, clarifying what aspects of safety risks are being considered (e.g., ethical implications, technical vulnerabilities) would enhance the question's specificity.\", 'verdict': 0}\n",
            "[ragas.testset.evolutions.INFO] retrying evolution: 1 times\n",
            "[ragas.testset.evolutions.INFO] seed question generated: \"What considerations are important for organizations in managing content provenance in generative AI systems?\"\n",
            "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks how evaluations involving human subjects can meet applicable requirements and be representative of the relevant population. It is clear in its intent, focusing on the standards and representativeness of evaluations. However, it lacks specificity regarding which applicable requirements are being referred to (e.g., ethical guidelines, regulatory standards) and what type of evaluations are being discussed (e.g., clinical trials, surveys). To improve clarity and answerability, the question could specify the context of the evaluations (e.g., medical, psychological) and the particular requirements of interest, which would help in providing a more focused and relevant response.', 'verdict': 0}\n",
            "[ragas.testset.evolutions.INFO] retrying evolution: 1 times\n",
            "[ragas.testset.evolutions.DEBUG] [ReasoningEvolution] question compressed: \"How can organizations enhance awareness of AI incidents without formal reporting channels by leveraging external feedback and provenance data?\"\n",
            "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'Structured feedback mechanisms can be used to monitor and improve GAI system outputs by implementing continuous monitoring of GAI system impacts to ensure equitable outputs across various sub-populations. This can involve seeking active and direct feedback from affected communities through mechanisms such as red-teaming exercises to identify and address any harmful bias, homogenization, or information integrity issues. Additionally, evaluating the quality and integrity of data used in training, tracking and documenting risks or opportunities related to unmeasured GAI risks, and engaging in internal and external evaluations with representative AI actors can help improve GAI system outputs.', 'verdict': 1}\n",
            "Generating:  19%|█▉        | 3/16 [00:05<00:20,  1.60s/it][ragas.testset.evolutions.DEBUG] [ReasoningEvolution] question compressed: \"How do ballot curing laws in at least 24 states ensure that voters can rectify issues with the voter signature matching algorithm to have their ballot counted?\"\n",
            "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: \"How can automated systems be made safe and effective in protecting the public?\"\n",
            "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about actions to manage risks associated with Generative AI (GAI), specifically considering misuse and the integration of various components in the value chain. It is clear in its intent to seek actionable strategies for risk management. However, the question is somewhat broad and could benefit from more specificity regarding the types of risks (e.g., ethical, operational, security) or components in the value chain being referred to. To improve clarity and answerability, the question could specify which aspects of GAI risks are of most concern or provide examples of components in the value chain that should be considered.', 'verdict': 1}\n",
            "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question is specific and seeks to understand the contributions of the technical companion to the Blueprint for an AI Bill of Rights in several key areas: safety and effectiveness of systems, protection against algorithmic discrimination, data privacy, notice and explanation, and promotion of human alternatives. However, it is somewhat complex and may benefit from simplification or breaking down into smaller parts for clarity. While the intent is clear, the length and multiple components could make it challenging for some to parse. To improve clarity and answerability, the question could be rephrased to focus on one or two specific contributions at a time or to clarify how these contributions are assessed or measured.', 'verdict': 1}\n",
            "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 2, 'structure': 1, 'relevance': 2, 'score': 1.5}\n",
            "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['National Science Foundation', 'Artificial Intelligence Research Institutes', 'Cyber-Physical Systems', 'Secure and Trustworthy Cyberspace', 'Formal Methods in the Field']\n",
            "[ragas.testset.evolutions.INFO] seed question generated: What is the role of a purpose-built testing environment, such as NIST Dioptra, in empirically evaluating GAI trustworthy characteristics?\n",
            "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 3, 'depth': 3, 'structure': 3, 'relevance': 3, 'score': 3.0}\n",
            "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Data privacy', 'Consent requests', 'Surveillance technologies', 'Automated systems', 'Notice and explanation']\n",
            "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
            "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Trustworthy Artificial Intelligence', 'Executive Order 13960', 'AI Bill of Rights', 'National Highway Traffic Safety Administration', 'Risk Management Framework']\n",
            "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: \"How to manage GAI risks with potential misuse and integrated components?\"\n",
            "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'The importance of building a glossary for synthetic media transparency methods, especially in relation to indirect disclosure, lies in enhancing understanding and awareness of the techniques and implications of synthetic media. By defining terms and concepts clearly, stakeholders can navigate the complexities of synthetic media, identify potential risks, and develop strategies to promote transparency and accountability in its creation and dissemination.', 'verdict': 1}\n",
            "Generating:  25%|██▌       | 4/16 [00:06<00:15,  1.32s/it][ragas.testset.evolutions.INFO] seed question generated: What is the focus of the National Science Foundation's program on Secure and Trustworthy Cyberspace?\n",
            "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'Both questions address the safety and effectiveness of automated systems in relation to public protection, sharing similar constraints and depth of inquiry.', 'verdict': 1}\n",
            "[ragas.testset.evolutions.INFO] retrying evolution: 1 times\n",
            "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'The first question focuses on the definition and tracking of AI incidents without formal reporting channels, while the second question emphasizes improving awareness of AI incidents through external feedback and provenance data. They differ in their specific inquiries and requirements, leading to different depths and breadths of inquiry.', 'verdict': 0}\n",
            "[ragas.testset.evolutions.INFO] seed question generated: What is the purpose of the NIST AI Risk Management Framework and how is it being developed?\n",
            "[ragas.testset.evolutions.DEBUG] [ReasoningEvolution] question compressed: \"How does the technical companion to the Blueprint for an AI Bill of Rights contribute to ensuring safe and effective systems, protecting against algorithmic discrimination, safeguarding data privacy, providing notice and explanation, and promoting human alternatives, consideration, and fallback?\"\n",
            "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'Both questions inquire about the role of ballot curing laws in relation to voter signature issues in 24 states, focusing on similar constraints and requirements regarding the signature matching algorithm.', 'verdict': 1}\n",
            "[ragas.testset.evolutions.DEBUG] evolution_filter failed, retrying with 1\n",
            "[ragas.testset.evolutions.INFO] retrying evolution: 1 times\n",
            "[ragas.testset.evolutions.INFO] seed question generated: \"How should designers, developers, and deployers of automated systems ensure that individuals have agency over their data and understand how automated systems impact them?\"\n",
            "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the implications of a specific Executive Order on algorithmic bias and public interest assessments. It is clear in its intent, specifying the subject (the Executive Order) and the areas of concern (algorithmic bias and public interest assessments). However, the question may require some background knowledge about the Executive Order itself and its contents, which are not provided in the question. To improve clarity and answerability, the question could briefly summarize the key points of the Executive Order or specify which aspects of algorithmic bias and public interest assessments are being referred to. This would make it more accessible to those unfamiliar with the document.', 'verdict': 0}\n",
            "[ragas.testset.evolutions.INFO] retrying evolution: 1 times\n",
            "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 1, 'structure': 1, 'relevance': 1, 'score': 1.0}\n",
            "[ragas.testset.evolutions.INFO] retrying evolution: 1 times\n",
            "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'The first question asks for suggested actions to manage GAI risks, while the second question focuses on managing GAI risks specifically related to potential misuse and integrated components. This introduces different constraints and requirements, leading to a difference in depth and breadth of inquiry.', 'verdict': 0}\n",
            "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
            "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Healthcare navigators', 'Automated customer service', 'Ballot curing laws', 'Fallback system', 'Voter signature matching algorithm']\n",
            "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the important considerations for organizations regarding content provenance in generative AI systems. It is specific in its focus on organizations and the topic of content provenance, making the intent clear. However, the question could benefit from further clarification on what aspects of content provenance are of interest (e.g., ethical implications, legal compliance, data integrity) to provide a more targeted response. Additionally, defining 'content provenance' in the context of generative AI could enhance understanding for those less familiar with the term. Overall, while the question is relatively clear, it could be improved by specifying the particular considerations or challenges organizations face.\", 'verdict': 1}\n",
            "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: \"What considerations are important for organizations in managing content provenance in generative AI systems?\"\n",
            "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': \"Adobe, American Civil Liberties Union (ACLU), The Aspen Commission on Information Disorder, The Awood Center, The Australian Human Rights Commission, Biometrics Institute, The Brookings Institute, BSA | The Software Alliance, Cantellus Group, Center for American Progress, Center for Democracy and Technology, Center on Privacy and Technology at Georgetown Law, Christiana Care, Color of Change, Coworker, Data Robot, Data Trust Alliance, Data and Society Research Institute, Deepmind, EdSAFE AI Alliance, Electronic Privacy Information Center (EPIC), Encode Justice, Equal AI, Google, Hitachi's AI Policy Committee, The Innocence Project, Institute of Electrical and Electronics Engineers (IEEE), Intuit, Lawyers Committee for Civil Rights Under Law, Legal Aid Society, The Leadership Conference on Civil and Human Rights, Meta, Microsoft, The MIT AI Policy Forum, Movement Alliance Project, The National Association of Criminal Defense Lawyers, O'Neil Risk Consulting & Algorithmic Auditing, The Partnership on AI, Pinterest, The Plaintext Group, pymetrics, SAP, The Security Industry Association, Software and Information Industry Association (SIIA), Special Competitive Studies Project, Thorn, United for Respect, University of California at Berkeley Citris Policy Lab, University of California at Berkeley Labor Center, Unfinished/Project Liberty, Upturn, US Chamber of Commerce, US Chamber of Commerce Technology Engagement Center, A.I. Working Group, Vibrent Health, Warehouse Worker Resource Center, Waymap\", 'verdict': 1}\n",
            "Generating:  31%|███▏      | 5/16 [00:07<00:13,  1.26s/it][ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
            "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Biometric Information Privacy Act', 'Fair Credit Reporting Act', 'Equal Credit Opportunity Act', 'California law', 'Explainable AI systems']\n",
            "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 2, 'structure': 1, 'relevance': 2, 'score': 1.5}\n",
            "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['National Science Foundation', 'Artificial Intelligence Research Institutes', 'Cyber-Physical Systems', 'Secure and Trustworthy Cyberspace', 'Formal Methods in the Field']\n",
            "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the role of a specific testing environment (NIST Dioptra) in evaluating trustworthy characteristics of Generative AI (GAI). It is clear in its intent and specifies the context (purpose-built testing environment and GAI trustworthy characteristics), making it understandable. However, it assumes familiarity with both NIST Dioptra and the concept of trustworthy characteristics in GAI, which may not be universally known. To enhance clarity and answerability, the question could briefly define what NIST Dioptra is and outline what is meant by 'trustworthy characteristics' in the context of GAI.\", 'verdict': 1}\n",
            "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'The first question asks about the purpose of the technical companion, while the second question inquires about its contributions to various aspects of AI rights. They differ in focus and depth, with the second question requiring a more detailed response.', 'verdict': 0}\n",
            "[ragas.testset.evolutions.INFO] seed question generated: \"How do ballot curing laws in at least 24 states require a fallback system for voters to correct their ballot?\"\n",
            "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: \"What key factors should organizations consider when managing the origin of content in generative AI systems?\"\n",
            "[ragas.testset.evolutions.INFO] seed question generated: \"What requirements does California law impose on warehouse employers regarding notice and explanation of quotas for employees?\"\n",
            "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the focus of the National Science Foundation's program on Secure and Trustworthy Cyberspace. It is specific and clear in its intent, seeking information about a particular program and its objectives. The question is independent, as it does not rely on external references or additional context to be understood. Therefore, it meets the criteria for clarity and answerability.\", 'verdict': 1}\n",
            "[ragas.testset.evolutions.INFO] seed question generated: What are some funding opportunities provided by the National Science Foundation in the field of artificial intelligence and cyber-physical systems?\n",
            "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the purpose of the NIST AI Risk Management Framework and its development process. It is clear in its intent, specifying two distinct aspects: the purpose and the development of the framework. However, the question could benefit from additional context regarding what specific aspects of the framework's purpose or development are of interest (e.g., intended audience, specific goals, or methodologies used in development). This would enhance clarity and allow for a more focused response. Overall, the question is understandable and answerable based on the details provided, but could be improved with more specificity.\", 'verdict': 1}\n",
            "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question is clear and specific, asking how various stakeholders (designers, developers, and deployers) of automated systems can ensure individuals maintain agency over their data and comprehend the impact of these systems. It does not rely on external references and conveys a clear intent regarding the responsibilities of these stakeholders. However, it could be improved by specifying what aspects of 'agency' and 'understanding' are being referred to, as well as the context in which these automated systems operate (e.g., privacy, decision-making). This would help in providing a more focused and detailed response.\", 'verdict': 1}\n",
            "[ragas.testset.evolutions.DEBUG] [ReasoningEvolution] simple question generated: \"How should designers, developers, and deployers of automated systems ensure that individuals have agency over their data and understand how automated systems impact them?\"\n",
            "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'Organizations can improve AI incident awareness without formal reporting by integrating pre- and post-deployment external feedback into the monitoring process for GAI models and corresponding applications. This can help enhance awareness of performance changes and mitigate potential risks and harms from outputs. Additionally, organizations can track and document the provenance of datasets to identify instances where AI-generated data may be a root cause of performance issues with the GAI system.', 'verdict': 1}\n",
            "Generating:  38%|███▊      | 6/16 [00:08<00:12,  1.22s/it][ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'The answer to given question is not present in context', 'verdict': -1}\n",
            "Generating:  44%|████▍     | 7/16 [00:09<00:08,  1.05it/s][ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the key factors organizations should consider regarding the management of content origin in generative AI systems. It is specific and has a clear intent, focusing on organizational considerations in a particular domain (generative AI). The question is independent and does not rely on external references or context, making it understandable and answerable based on the details provided. Therefore, it meets the criteria for clarity and answerability.', 'verdict': 1}\n",
            "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question is clear and specific, asking for information about funding opportunities from the National Science Foundation (NSF) specifically in the fields of artificial intelligence and cyber-physical systems. It does not rely on external references or additional context, making it independent and self-contained. The intent is also clear, as it seeks to identify specific funding opportunities. Therefore, it meets the criteria for clarity and answerability.', 'verdict': 1}\n",
            "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: What are some funding opportunities provided by the National Science Foundation in the field of artificial intelligence and cyber-physical systems?\n",
            "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': \"The focus of the National Science Foundation's program on Secure and Trustworthy Cyberspace is to ensure the security and reliability of cyberspace through research and funding opportunities.\", 'verdict': 1}\n",
            "Generating:  50%|█████     | 8/16 [00:10<00:06,  1.15it/s][ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question is specific and clear, asking about the requirements imposed by California law on warehouse employers concerning notice and explanation of quotas for employees. It does not rely on external references and can be understood independently. The intent is clear, as it seeks specific legal information regarding employer obligations. Therefore, it meets the criteria for clarity and answerability.', 'verdict': 1}\n",
            "[ragas.testset.evolutions.DEBUG] [ReasoningEvolution] simple question generated: \"What requirements does California law impose on warehouse employers regarding notice and explanation of quotas for employees?\"\n",
            "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about ballot curing laws in at least 24 states and their requirement for a fallback system for voters to correct their ballots. It is specific in its focus on ballot curing laws and the number of states involved, which contributes to its clarity. However, the phrase 'fallback system' could be interpreted in various ways, and the question does not provide any context or examples of what such a system might entail. To improve clarity and answerability, the question could specify what is meant by 'fallback system' or provide examples of how these laws are implemented in different states. Additionally, it could clarify whether the inquiry is about the legal text, practical implementation, or voter experiences.\", 'verdict': 1}\n",
            "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: \"How do ballot curing laws in at least 24 states require a fallback system for voters to correct their ballot?\"\n",
            "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: \"What factors to consider when managing content in generative AI systems?\"\n",
            "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'A purpose-built testing environment like NIST Dioptra is utilized to empirically evaluate the trustworthy characteristics of GAI. This involves assessing aspects such as CBRN information or capabilities, data privacy, confabulation, information integrity, information security, dangerous, violent, or hateful content, harmful bias, and homogenization. The testing environment helps in ensuring that the AI system to be deployed is valid, reliable, and its limitations in generalizability are documented.', 'verdict': 1}\n",
            "Generating:  56%|█████▋    | 9/16 [00:10<00:05,  1.36it/s][ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: \"What NSF funding opportunities support the development of safe, trustworthy, and effective AI systems with legal compliance in mind?\"\n",
            "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'The answer to given question is not present in context', 'verdict': -1}\n",
            "Generating:  62%|██████▎   | 10/16 [00:10<00:03,  1.66it/s][ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: \"How do ballot curing laws in at least 24 states ensure a human fallback system is in place for voters to rectify their ballot discrepancies?\"\n",
            "[ragas.testset.filters.DEBUG] evolution filter: {'reason': \"Both questions inquire about the management of content in generative AI systems, but the first question specifically emphasizes 'content provenance' and the considerations for organizations, which adds a layer of depth and specificity not present in the second question.\", 'verdict': 0}\n",
            "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'The NIST AI Risk Management Framework is being developed to better manage risks posed to individuals, organizations, and society by AI. It is intended for voluntary use to help incorporate trustworthiness considerations into the design, development, use, and evaluation of AI products, services, and systems. The framework is being developed through a consensus-driven, open, transparent, and collaborative process that includes workshops and other opportunities for input. It aims to foster innovative approaches to address characteristics of trustworthiness such as accuracy, explainability, reliability, privacy, robustness, safety, security, and mitigation of unintended or harmful bias.', 'verdict': 1}\n",
            "Generating:  69%|██████▉   | 11/16 [00:11<00:03,  1.67it/s][ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question addresses the roles of designers, developers, and deployers in ensuring that individuals have control over their data and understand the impact of automated systems. It is clear in its intent and specifies the stakeholders involved, making it understandable. However, the question is somewhat broad and could benefit from more specificity regarding the context or type of automated systems being referred to. To improve clarity and answerability, the question could specify particular domains (e.g., social media, healthcare, finance) or types of automated systems (e.g., recommendation algorithms, decision-making systems) to provide a clearer framework for the response.', 'verdict': 1}\n",
            "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question is specific and clear, asking about a particular California law that requires written descriptions of quotas for warehouse employees and mentions potential adverse actions for non-compliance. It does not rely on external references or additional context, making it independent and understandable. The intent is clear, as it seeks information about a specific legal requirement. Therefore, it meets the criteria for clarity and answerability.', 'verdict': 1}\n",
            "[ragas.testset.evolutions.DEBUG] [ReasoningEvolution] question compressed: \"How can designers, developers, and deployers ensure individuals have control over their data and comprehend the impact of automated systems on them?\"\n",
            "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question is specific and seeks information about NSF funding opportunities related to the development of AI systems that are safe, trustworthy, effective, and legally compliant. It is clear in its intent and does not rely on external references, making it understandable and answerable based on the details provided. However, it could be improved by specifying the type of AI systems or the particular aspects of legal compliance being considered, which would help narrow down the response further. Overall, the question is well-structured and meets the criteria for clarity and answerability.', 'verdict': 1}\n",
            "[ragas.testset.evolutions.DEBUG] [ReasoningEvolution] question compressed: \"What California law mandates written descriptions of quotas for warehouse employees, including potential adverse actions for non-compliance?\"\n",
            "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'The answer to given question is not present in context', 'verdict': -1}\n",
            "Generating:  75%|███████▌  | 12/16 [00:12<00:03,  1.24it/s][ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: \"What NSF funding opportunities are available for AI systems with legal compliance?\"\n",
            "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'The first question specifically addresses the roles of designers, developers, and deployers in ensuring individual agency over data, while the second question is more general and does not specify these roles. This leads to a difference in depth and breadth of inquiry.', 'verdict': 0}\n",
            "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about ballot curing laws in at least 24 states and their role in ensuring a human fallback system for voters to address ballot discrepancies. It is specific in its focus on ballot curing laws and the mechanism of a human fallback system, which provides a clear intent. However, the phrase 'ensure a human fallback system is in place' could be interpreted in various ways, leading to potential ambiguity in what specific aspects of the laws are being referenced. To improve clarity, the question could specify what is meant by 'human fallback system' and what types of discrepancies are being considered. Additionally, it could clarify whether the inquiry is about the effectiveness, implementation, or legal framework of these laws.\", 'verdict': 1}\n",
            "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'The first question is broader, asking about funding opportunities in artificial intelligence and cyber-physical systems, while the second question narrows the focus to AI systems specifically related to legal compliance. This difference in scope affects the depth and breadth of inquiry.', 'verdict': 0}\n",
            "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'Both questions address California law related to warehouse employers and employee quotas, but they focus on different aspects: one emphasizes the requirements for notice and explanation, while the other highlights the law itself and consequences for non-compliance, indicating a difference in depth and breadth of inquiry.', 'verdict': 0}\n",
            "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: \"How do ballot curing laws in 24+ states provide a way for voters to fix ballot errors?\"\n",
            "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'Both questions inquire about the mechanisms of ballot curing laws in a similar number of states, focusing on how these laws assist voters in correcting ballot errors. They share the same constraints and requirements, as well as depth and breadth of inquiry.', 'verdict': 1}\n",
            "[ragas.testset.evolutions.INFO] retrying evolution: 2 times\n",
            "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'The answer to given question is not present in context', 'verdict': -1}\n",
            "Generating:  81%|████████▏ | 13/16 [00:15<00:03,  1.32s/it][ragas.testset.filters.DEBUG] context scoring: {'clarity': 3, 'depth': 3, 'structure': 3, 'relevance': 3, 'score': 3.0}\n",
            "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Automated systems', 'Protect the public from harm', 'Consultation', 'Testing', 'Risk identification and mitigation']\n",
            "[ragas.testset.evolutions.INFO] seed question generated: \"What are the key components of testing automated systems before deployment?\"\n",
            "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'A California law requires that warehouse employees are provided with notice and explanation about quotas, potentially facilitated by automated systems, that apply to them. Warehousing employers in California that use quota systems (often facilitated by algorithmic monitoring systems) are required to provide employees with a written description of each quota that applies to the employee, including quantified number of tasks to be performed or materials to be produced or handled, within the defined time period, and any potential adverse employment action that could result from failure to meet the quota.', 'verdict': 1}\n",
            "Generating:  88%|████████▊ | 14/16 [00:16<00:02,  1.39s/it][ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'Stakeholders can ensure data control and understanding of automated systems by implementing built-in protections that safeguard against abusive data practices. This includes ensuring that individuals have agency over how their data is used, that data collection is limited to what is strictly necessary for the specific context, and that permission is sought and respected for data collection, use, access, transfer, and deletion. Design choices should prioritize user privacy and avoid obfuscating user choice or burdening users with privacy-invasive defaults. Consent should be clear, brief, and understandable, giving individuals control over their data and its specific context of use. Enhanced protections and restrictions should be in place for sensitive domains like health, work, education, criminal justice, and finance, with a focus on putting individuals first. Unchecked surveillance should be avoided, with surveillance technologies subject to oversight and limits to protect privacy and civil liberties. Individuals should have access to reporting confirming that their data decisions have been respected and assessing the potential impact of surveillance technologies on their rights, opportunities, or access.', 'verdict': 1}\n",
            "Generating:  94%|█████████▍| 15/16 [00:17<00:01,  1.19s/it][ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks for the key components involved in testing automated systems prior to their deployment. It is specific and independent, as it does not rely on external references or additional context to be understood. The intent is clear, seeking a list or explanation of important elements in the testing process. Therefore, it meets the criteria for clarity and answerability.', 'verdict': 1}\n",
            "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: \"What are the key components of testing automated systems before deployment?\"\n",
            "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: \"What measures should be taken to ensure the safety and effectiveness of automated systems before deployment?\"\n",
            "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the measures necessary to ensure the safety and effectiveness of automated systems prior to their deployment. It is clear in its intent, specifying the focus on safety and effectiveness, and does not rely on external references or context. However, it could be improved by specifying the type of automated systems in question (e.g., industrial, transportation, healthcare) or the specific aspects of safety and effectiveness being considered (e.g., testing protocols, regulatory compliance). This would help narrow down the scope and provide a more targeted response.', 'verdict': 1}\n",
            "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: \"What steps are needed for safe and effective automated system deployment?\"\n",
            "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'The first question focuses on the components necessary for testing automated systems, while the second question addresses the steps for deployment. They have different scopes and requirements, leading to different depths of inquiry.', 'verdict': 0}\n",
            "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'The answer to given question is not present in context', 'verdict': -1}\n",
            "Generating: 100%|██████████| 16/16 [00:22<00:00,  1.41s/it]\n"
          ]
        }
      ],
      "source": [
        "testset = generator.generate_with_langchain_docs(documents, 15, distributions, with_debugging_logs=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 926
        },
        "id": "mOUdh4bUp63p",
        "outputId": "35d04edd-631f-4ae8-fcf3-cf90e039cbc6"
      },
      "outputs": [],
      "source": [
        "dataset=testset.to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>contexts</th>\n",
              "      <th>ground_truth</th>\n",
              "      <th>evolution_type</th>\n",
              "      <th>metadata</th>\n",
              "      <th>episode_done</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Which organizations were involved as private s...</td>\n",
              "      <td>[APPENDIX\\n• OSTP conducted meetings with a va...</td>\n",
              "      <td>Adobe, American Civil Liberties Union (ACLU), ...</td>\n",
              "      <td>simple</td>\n",
              "      <td>[{'source': 'dataset/data1.pdf', 'file_path': ...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What is the importance of building a glossary ...</td>\n",
              "      <td>[ \\n57 \\nNational Institute of Standards and T...</td>\n",
              "      <td>The importance of building a glossary for synt...</td>\n",
              "      <td>simple</td>\n",
              "      <td>[{'source': 'dataset/data2.pdf', 'file_path': ...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How can organizations enhance content provenan...</td>\n",
              "      <td>[ \\n52 \\n• \\nMonitoring system capabilities an...</td>\n",
              "      <td>Organizations can enhance content provenance t...</td>\n",
              "      <td>simple</td>\n",
              "      <td>[{'source': 'dataset/data2.pdf', 'file_path': ...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What is the purpose of the NIST AI Risk Manage...</td>\n",
              "      <td>[ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nSAF...</td>\n",
              "      <td>The NIST AI Risk Management Framework is being...</td>\n",
              "      <td>simple</td>\n",
              "      <td>[{'source': 'dataset/data1.pdf', 'file_path': ...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How can structured feedback mechanisms be used...</td>\n",
              "      <td>[ \\n29 \\nMS-1.1-006 \\nImplement continuous mon...</td>\n",
              "      <td>Structured feedback mechanisms can be used to ...</td>\n",
              "      <td>simple</td>\n",
              "      <td>[{'source': 'dataset/data2.pdf', 'file_path': ...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            question  \\\n",
              "0  Which organizations were involved as private s...   \n",
              "1  What is the importance of building a glossary ...   \n",
              "2  How can organizations enhance content provenan...   \n",
              "3  What is the purpose of the NIST AI Risk Manage...   \n",
              "4  How can structured feedback mechanisms be used...   \n",
              "\n",
              "                                            contexts  \\\n",
              "0  [APPENDIX\\n• OSTP conducted meetings with a va...   \n",
              "1  [ \\n57 \\nNational Institute of Standards and T...   \n",
              "2  [ \\n52 \\n• \\nMonitoring system capabilities an...   \n",
              "3  [ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nSAF...   \n",
              "4  [ \\n29 \\nMS-1.1-006 \\nImplement continuous mon...   \n",
              "\n",
              "                                        ground_truth evolution_type  \\\n",
              "0  Adobe, American Civil Liberties Union (ACLU), ...         simple   \n",
              "1  The importance of building a glossary for synt...         simple   \n",
              "2  Organizations can enhance content provenance t...         simple   \n",
              "3  The NIST AI Risk Management Framework is being...         simple   \n",
              "4  Structured feedback mechanisms can be used to ...         simple   \n",
              "\n",
              "                                            metadata  episode_done  \n",
              "0  [{'source': 'dataset/data1.pdf', 'file_path': ...          True  \n",
              "1  [{'source': 'dataset/data2.pdf', 'file_path': ...          True  \n",
              "2  [{'source': 'dataset/data2.pdf', 'file_path': ...          True  \n",
              "3  [{'source': 'dataset/data1.pdf', 'file_path': ...          True  \n",
              "4  [{'source': 'dataset/data2.pdf', 'file_path': ...          True  "
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SLtk1GtnyoY"
      },
      "source": [
        "## LangSmith Dataset\n",
        "\n",
        "Now we can move on to creating a dataset for LangSmith!\n",
        "\n",
        "First, we'll need to create a dataset on LangSmith using the `Client`!\n",
        "\n",
        "We'll name our Dataset to make it easy to work with later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "TLgm6OjvYSsm"
      },
      "outputs": [],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()\n",
        "\n",
        "dataset_name = \"Etical_ai\"\n",
        "\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    description=\"Questions about etical AI\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "8nFQ6di_XnY7"
      },
      "outputs": [],
      "source": [
        "for test in testset.to_pandas().iterrows():\n",
        "  client.create_example(\n",
        "      inputs={\n",
        "          \"question\": test[1][\"question\"]\n",
        "      },\n",
        "      outputs={\n",
        "          \"answer\": test[1][\"ground_truth\"]\n",
        "      },\n",
        "      metadata={\n",
        "          \"context\": test[0]\n",
        "      },\n",
        "      dataset_id=dataset.id\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6EbQVyZq-2j"
      },
      "source": [
        "## Evaluation Pipeline Chunk stragegy and  embedding model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQorBy8H1AZR"
      },
      "source": [
        "Recursive character text splitter!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     C:\\nltk_data...\n",
            "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Firs TIME\n",
        "#import nltk\n",
        "#nltk.download('punkt_tab')\n",
        "#import nltk\n",
        "#nltk.download('averaged_perceptron_tagger_eng')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You have 1 document(s) in your data\n",
            "There are 219681 characters in your document\n",
            "You have 465 pages\n"
          ]
        }
      ],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "print(f\"You have {len(data)} document(s) in your data\")\n",
        "print(f\"There are {len(data[0].page_content)} characters in your document\")\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 50)\n",
        "rag_documents = text_splitter.split_documents(documents)\n",
        "\n",
        "print(f\"You have {len(rag_documents)} pages\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "UwfJCzP3aqKI"
      },
      "outputs": [],
      "source": [
        "from langchain_ollama import OllamaEmbeddings\n",
        "embeddings = OllamaEmbeddings(\n",
        "    model=\"bge-large\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "OllamaEmbeddings(model='bge-large')"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "58Ypj_NgbEsi"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Qdrant\n",
        "\n",
        "vectorstore = Qdrant.from_documents(\n",
        "    documents=rag_documents,\n",
        "    embedding=embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"EticalAI\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_qdrant import QdrantVectorStore\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http.models import Distance, VectorParams\n",
        "client = QdrantClient(\":memory:\")\n",
        "\n",
        "client.create_collection(\n",
        "    collection_name=\"EticalAI\",\n",
        "    vectors_config=VectorParams(size=1024 , distance=Distance.COSINE),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_qdrant import RetrievalMode\n",
        "\n",
        "qdrant = QdrantVectorStore.from_documents(\n",
        "    rag_documents,\n",
        "    embedding=embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"EticalAI\",\n",
        "    retrieval_mode=RetrievalMode.DENSE,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 5, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': '', '_id': '1ddd09fdfddd43fba86cd857f26372ea', '_collection_name': 'EticalAI'}, page_content='SECTION TITLE\\nDATA PRIVACY\\nYou should be protected from abusive data practices via built-in protections and you \\nshould have agency over how data about you is used. You should be protected from violations of \\nprivacy through design choices that ensure such protections are included by default, including ensuring that \\ndata collection conforms to reasonable expectations and that only data strictly necessary for the specific \\ncontext is collected. Designers, developers, and deployers of automated systems should seek your permission \\nand respect your decisions regarding collection, use, access, transfer, and deletion of your data in appropriate \\nways and to the greatest extent possible; where not possible, alternative privacy by design safeguards should be \\nused. Systems should not employ user experience and design decisions that obfuscate user choice or burden \\nusers with defaults that are privacy invasive. Consent should only be used to justify collection of data in cases'),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 32, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': '', '_id': '941d0d9b41af43a78d8905aa505af965', '_collection_name': 'EticalAI'}, page_content='fied. \\nRisk identification and mitigation. Entities that collect, use, share, or store sensitive data should \\nattempt to proactively identify harms and seek to manage them so as to avoid, mitigate, and respond appropri\\xad\\nately to identified risks. Appropriate responses include determining not to process data when the privacy risks \\noutweigh the benefits or implementing measures to mitigate acceptable risks. Appropriate responses do not \\ninclude sharing or transferring the privacy risks to users via notice or consent requests where users could not \\nreasonably be expected to understand the risks without further support. \\nPrivacy-preserving security. Entities creating, using, or governing automated systems should follow \\nprivacy and security best practices designed to ensure data and metadata do not leak beyond the specific \\nconsented use case. Best practices could include using privacy-enhancing cryptography or other types of'),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 29, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': '', '_id': 'f4215d247fc94dc3bb135ac532d35c52', '_collection_name': 'EticalAI'}, page_content='You should be protected from abusive data practices via built-in \\nprotections and you should have agency over how data about \\nyou is used. You should be protected from violations of privacy through \\ndesign choices that ensure such protections are included by default, including \\nensuring that data collection conforms to reasonable expectations and that \\nonly data strictly necessary for the specific context is collected. Designers, de\\xad\\nvelopers, and deployers of automated systems should seek your permission \\nand respect your decisions regarding collection, use, access, transfer, and de\\xad\\nletion of your data in appropriate ways and to the greatest extent possible; \\nwhere not possible, alternative privacy by design safeguards should be used. \\nSystems should not employ user experience and design decisions that obfus\\xad\\ncate user choice or burden users with defaults that are privacy invasive. Con\\xad\\nsent should only be used to justify collection of data in cases where it can be'),\n",
              " Document(metadata={'source': 'dataset/data1.pdf', 'file_path': 'dataset/data1.pdf', 'page': 30, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': '', '_id': '8bac664a43c54b7e8ea20b13e472eabe', '_collection_name': 'EticalAI'}, page_content='in some cases. Many states have also enacted consumer data privacy protection regimes to address some of these \\nharms. \\nHowever, these are not yet standard practices, and the United States lacks a comprehensive statutory or regulatory \\nframework governing the rights of the public when it comes to personal data. While a patchwork of laws exists to \\nguide the collection and use of personal data in specific contexts, including health, employment, education, and credit, \\nit can be unclear how these laws apply in other contexts and in an increasingly automated society. Additional protec\\xad\\ntions would assure the American public that the automated systems they use are not monitoring their activities, \\ncollecting information on their lives, or otherwise surveilling them without context-specific consent or legal authori\\xad\\nty. \\n31')]"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "query = \"What data privacy concerns are mentioned?\"\n",
        "found_docs = qdrant.similarity_search(query)\n",
        "found_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "SbKSjfSkbTYo"
      },
      "outputs": [],
      "source": [
        "retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxUOMaQX1K2N"
      },
      "source": [
        "To get the \"A\" in RAG, we'll provide a prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "1sLeY1oWbVqO"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_PROMPT = \"\"\"\\\n",
        "Given a provided context and question, you must answer the question based only on context.\n",
        "\n",
        "If you cannot answer the question based on the context - you must say \"I don't know\".\n",
        "\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "6nx-ue1XbciV"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmTL6-pc1ZGz"
      },
      "source": [
        "Finally, we can set-up our RAG LCEL chain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "TjWj0OLIbbFc"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
        "from langchain.schema import StrOutputParser\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
        "    | rag_prompt | llm | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "WQ7bEweo4IIb",
        "outputId": "d161b269-f799-4920-d6ce-c202f6e783aa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The data privacy concerns mentioned include:\\n\\n1. Protection from abusive data practices and the need for built-in protections.\\n2. Agency over how data about individuals is used.\\n3. Violations of privacy through design choices that should ensure protections are included by default.\\n4. Data collection should conform to reasonable expectations and only collect data strictly necessary for the specific context.\\n5. The need for designers, developers, and deployers of automated systems to seek permission and respect user decisions regarding data collection, use, access, transfer, and deletion.\\n6. User experience and design decisions should not obfuscate user choice or impose privacy-invasive defaults.\\n7. Consent should only be used justifiably for data collection in certain cases.\\n8. Lack of a comprehensive statutory or regulatory framework governing public rights concerning personal data, leading to unclear application of existing laws in various contexts. \\n9. Concerns about automated systems monitoring activities and collecting information without context-specific consent or legal authority.'"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rag_chain.invoke({\"question\" : \"What data privacy concerns are mentioned?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SemanticChunker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "semantic_chunker = SemanticChunker(embeddings, breakpoint_threshold_type=\"percentile\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "semantic_chunks = semantic_chunker.create_documents([d.page_content for d in documents])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "vectorstore = Qdrant.from_documents(\n",
        "    documents=semantic_chunks,\n",
        "    embedding=embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"EticalAI2\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "retriever2 = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {},
      "outputs": [],
      "source": [
        "rag_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
        "    | rag_prompt | llm | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The data privacy concerns mentioned include:\\n\\n1. The use of personal data for GAI training raises risks to privacy principles such as transparency, individual participation (including consent), and purpose specification.\\n2. Lack of disclosure by model developers regarding specific data sources used for training, limiting user awareness of whether personally identifiable information (PII) was included.\\n3. The potential for models to leak, generate, or infer sensitive information about individuals, leading to privacy risks.\\n4. Data memorization, where models may reveal sensitive information that was present in their training data.\\n5. Inferences made by models can negatively impact individuals, even if those inferences are not accurate.\\n6. Wrong or inappropriate inferences of PII can lead to downstream harmful impacts, such as adverse decisions based on predictive inferences.\\n7. Concerns about mission creep in data collection and use, requiring that data collection be minimized and clearly communicated to users.\\n8. The importance of obtaining user consent and ensuring that users understand how their data will be used.\\n9. The need for risk identification and mitigation measures for entities that collect, use, share, or store sensitive data.\\n10. The necessity of protections against abusive data practices and ensuring user agency over data usage.'"
            ]
          },
          "execution_count": 119,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rag_chain.invoke({\"question\" : \"What data privacy concerns are mentioned?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# According to the answer retrieval, the semantic chunk is providing better responses, so let's evaluate it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_PROMPT = \"\"\"\\\n",
        "Given a provided context and question, you must answer the question based only on context.\n",
        "\n",
        "If you cannot answer the question based on the context - you must say \"I don't know\".\n",
        "\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "primary_qa_llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "retrieval_augmented_qa_chain = (\n",
        "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
        "    # \"question\" : populated by getting the value of the \"question\" key\n",
        "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
        "    {\"context\": itemgetter(\"question\") | retriever2, \"question\": itemgetter(\"question\")}\n",
        "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
        "    #              by getting the value of the \"context\" key from the previous step\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
        "    #              into the LLM and stored in a key called \"response\"\n",
        "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
        "    | {\"response\": rag_prompt | primary_qa_llm, \"context\": itemgetter(\"context\")})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>contexts</th>\n",
              "      <th>ground_truth</th>\n",
              "      <th>evolution_type</th>\n",
              "      <th>metadata</th>\n",
              "      <th>episode_done</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Which organizations were involved as private s...</td>\n",
              "      <td>[APPENDIX\\n• OSTP conducted meetings with a va...</td>\n",
              "      <td>Adobe, American Civil Liberties Union (ACLU), ...</td>\n",
              "      <td>simple</td>\n",
              "      <td>[{'source': 'dataset/data1.pdf', 'file_path': ...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What is the importance of building a glossary ...</td>\n",
              "      <td>[ \\n57 \\nNational Institute of Standards and T...</td>\n",
              "      <td>The importance of building a glossary for synt...</td>\n",
              "      <td>simple</td>\n",
              "      <td>[{'source': 'dataset/data2.pdf', 'file_path': ...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How can organizations enhance content provenan...</td>\n",
              "      <td>[ \\n52 \\n• \\nMonitoring system capabilities an...</td>\n",
              "      <td>Organizations can enhance content provenance t...</td>\n",
              "      <td>simple</td>\n",
              "      <td>[{'source': 'dataset/data2.pdf', 'file_path': ...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What is the purpose of the NIST AI Risk Manage...</td>\n",
              "      <td>[ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nSAF...</td>\n",
              "      <td>The NIST AI Risk Management Framework is being...</td>\n",
              "      <td>simple</td>\n",
              "      <td>[{'source': 'dataset/data1.pdf', 'file_path': ...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How can structured feedback mechanisms be used...</td>\n",
              "      <td>[ \\n29 \\nMS-1.1-006 \\nImplement continuous mon...</td>\n",
              "      <td>Structured feedback mechanisms can be used to ...</td>\n",
              "      <td>simple</td>\n",
              "      <td>[{'source': 'dataset/data2.pdf', 'file_path': ...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            question  \\\n",
              "0  Which organizations were involved as private s...   \n",
              "1  What is the importance of building a glossary ...   \n",
              "2  How can organizations enhance content provenan...   \n",
              "3  What is the purpose of the NIST AI Risk Manage...   \n",
              "4  How can structured feedback mechanisms be used...   \n",
              "\n",
              "                                            contexts  \\\n",
              "0  [APPENDIX\\n• OSTP conducted meetings with a va...   \n",
              "1  [ \\n57 \\nNational Institute of Standards and T...   \n",
              "2  [ \\n52 \\n• \\nMonitoring system capabilities an...   \n",
              "3  [ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nSAF...   \n",
              "4  [ \\n29 \\nMS-1.1-006 \\nImplement continuous mon...   \n",
              "\n",
              "                                        ground_truth evolution_type  \\\n",
              "0  Adobe, American Civil Liberties Union (ACLU), ...         simple   \n",
              "1  The importance of building a glossary for synt...         simple   \n",
              "2  Organizations can enhance content provenance t...         simple   \n",
              "3  The NIST AI Risk Management Framework is being...         simple   \n",
              "4  Structured feedback mechanisms can be used to ...         simple   \n",
              "\n",
              "                                            metadata  episode_done  \n",
              "0  [{'source': 'dataset/data1.pdf', 'file_path': ...          True  \n",
              "1  [{'source': 'dataset/data2.pdf', 'file_path': ...          True  \n",
              "2  [{'source': 'dataset/data2.pdf', 'file_path': ...          True  \n",
              "3  [{'source': 'dataset/data1.pdf', 'file_path': ...          True  \n",
              "4  [{'source': 'dataset/data2.pdf', 'file_path': ...          True  "
            ]
          },
          "execution_count": 117,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_questions = dataset[\"question\"].values.tolist()\n",
        "test_groundtruths = dataset[\"ground_truth\"].values.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [],
      "source": [
        "answers = []\n",
        "contexts = []\n",
        "\n",
        "for question in test_questions:\n",
        "  response = retrieval_augmented_qa_chain.invoke({\"question\" : question})\n",
        "  answers.append(response[\"response\"].content)\n",
        "  contexts.append([context.page_content for context in response[\"context\"]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "response_dataset = Dataset.from_dict({\n",
        "    \"question\" : test_questions,\n",
        "    \"answer\" : answers,\n",
        "    \"contexts\" : contexts,\n",
        "    \"ground_truth\" : test_groundtruths\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'question': 'What is the role of a purpose-built testing environment, such as NIST Dioptra, in empirically evaluating GAI trustworthy characteristics?',\n",
              " 'answer': 'The role of a purpose-built testing environment, such as NIST Dioptra, in empirically evaluating GAI trustworthy characteristics is to facilitate the assessment of various factors including CBRN Information or Capabilities, Data Privacy, Confabulation, Information Integrity, Information Security, Dangerous, Violent, or Hateful Content, and Harmful Bias and Homogenization. It is utilized to ensure that the AI system being deployed is valid and reliable, and to document limitations of generalizability beyond the conditions under which the technology was developed.',\n",
              " 'contexts': [' \\n31 \\nMS-2.3-004 \\nUtilize a purpose-built testing environment such as NIST Dioptra to empirically \\nevaluate GAI trustworthy characteristics. CBRN Information or Capabilities; \\nData Privacy; Confabulation; \\nInformation Integrity; Information \\nSecurity; Dangerous, Violent, or \\nHateful Content; Harmful Bias and \\nHomogenization \\nAI Actor Tasks: AI Deployment, TEVV \\n \\nMEASURE 2.5: The AI system to be deployed is demonstrated to be valid and reliable. Limitations of the generalizability beyond the \\nconditions under which the technology was developed are documented.',\n",
              "  \"Assess the general awareness among end users and impacted communities \\nabout the availability of these feedback channels. Human-AI Conﬁguration; \\nInformation Integrity; Harmful Bias \\nand Homogenization \\nAI Actor Tasks: AI Deployment, Aﬀected Individuals and Communities, End-Users, Operation and Monitoring, TEVV \\n \\nMEASURE 4.2: Measurement results regarding AI system trustworthiness in deployment context(s) and across the AI lifecycle are \\ninformed by input from domain experts and relevant AI Actors to validate whether the system is performing consistently as \\nintended. Results are documented. Action ID \\nSuggested Action \\nGAI Risks \\nMS-4.2-001 \\nConduct adversarial testing at a regular cadence to map and measure GAI risks, \\nincluding tests to address attempts to deceive or manipulate the application of \\nprovenance techniques or other misuses. Identify vulnerabilities and \\nunderstand potential misuse scenarios and unintended outputs. Information Integrity; Information \\nSecurity \\nMS-4.2-002 \\nEvaluate GAI system performance in real-world scenarios to observe its \\nbehavior in practical environments and reveal issues that might not surface in \\ncontrolled and optimized testing environments. Human-AI Conﬁguration; \\nConfabulation; Information \\nSecurity \\nMS-4.2-003 \\nImplement interpretability and explainability methods to evaluate GAI system \\ndecisions and verify alignment with intended purpose. Information Integrity; Harmful Bias \\nand Homogenization \\nMS-4.2-004 \\nMonitor and document instances where human operators or other systems \\noverride the GAI's decisions. Evaluate these cases to understand if the overrides \\nare linked to issues related to content provenance. Information Integrity \\nMS-4.2-005 \\nVerify and document the incorporation of results of structured public feedback \\nexercises into design, implementation, deployment approval (“go”/“no-go” \\ndecisions), monitoring, and decommission decisions. Human-AI Conﬁguration; \\nInformation Security \\nAI Actor Tasks: AI Deployment, Domain Experts, End-Users, Operation and Monitoring, TEVV \\n \\n\",\n",
              "  ' \\n2 \\nThis work was informed by public feedback and consultations with diverse stakeholder groups as part of NIST’s \\nGenerative AI Public Working Group (GAI PWG). The GAI PWG was an open, transparent, and collaborative \\nprocess, facilitated via a virtual workspace, to obtain multistakeholder input on GAI risk management and to \\ninform NIST’s approach. The focus of the GAI PWG was limited to four primary considerations relevant to GAI: Governance, Content \\nProvenance, Pre-deployment Testing, and Incident Disclosure (further described in Appendix A). As such, the \\nsuggested actions in this document primarily address these considerations. Future revisions of this proﬁle will include additional AI RMF subcategories, risks, and suggested actions based \\non additional considerations of GAI as the space evolves and empirical evidence indicates additional risks. A \\nglossary of terms pertinent to GAI risk management will be developed and hosted on NIST’s Trustworthy & \\nResponsible AI Resource Center (AIRC), and added to The Language of Trustworthy AI: An In-Depth Glossary of \\nTerms. This document was also informed by public comments and consultations from several Requests for Information.',\n",
              "  ' \\n52 \\n• \\nMonitoring system capabilities and limitations in deployment through rigorous TEVV processes; \\n• \\nEvaluating how humans engage, interact with, or adapt to GAI content (especially in decision \\nmaking tasks informed by GAI content), and how they react to applied provenance techniques \\nsuch as overt disclosures. Organizations can document and delineate GAI system objectives and limitations to identify gaps where \\nprovenance data may be most useful. For instance, GAI systems used for content creation may require \\nrobust watermarking techniques and corresponding detectors to identify the source of content or \\nmetadata recording techniques and metadata management tools and repositories to trace content \\norigins and modiﬁcations. Further narrowing of GAI task deﬁnitions to include provenance data can \\nenable organizations to maximize the utility of provenance data and risk management eﬀorts. A.1.7. Enhancing Content Provenance through Structured Public Feedback \\nWhile indirect feedback methods such as automated error collection systems are useful, they often lack \\nthe context and depth that direct input from end users can provide. Organizations can leverage feedback \\napproaches described in the Pre-Deployment Testing section to capture input from external sources such \\nas through AI red-teaming. Integrating pre- and post-deployment external feedback into the monitoring process for GAI models and \\ncorresponding applications can help enhance awareness of performance changes and mitigate potential \\nrisks and harms from outputs. There are many ways to capture and make use of user feedback – before \\nand after GAI systems and digital content transparency approaches are deployed – to gain insights about \\nauthentication eﬃcacy and vulnerabilities, impacts of adversarial threats on techniques, and unintended \\nconsequences resulting from the utilization of content provenance approaches on users and \\ncommunities. Furthermore, organizations can track and document the provenance of datasets to identify \\ninstances in which AI-generated data is a potential root cause of performance issues with the GAI \\nsystem.'],\n",
              " 'ground_truth': 'A purpose-built testing environment like NIST Dioptra is utilized to empirically evaluate the trustworthy characteristics of GAI. This involves assessing aspects such as CBRN information or capabilities, data privacy, confabulation, information integrity, information security, dangerous, violent, or hateful content, harmful bias, and homogenization. The testing environment helps in ensuring that the AI system to be deployed is valid, reliable, and its limitations in generalizability are documented.'}"
            ]
          },
          "execution_count": 108,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response_dataset[7]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas import evaluate\n",
        "from ragas.metrics import (\n",
        "    faithfulness,\n",
        "    answer_relevancy,\n",
        "    answer_correctness,\n",
        "    context_recall,\n",
        "    context_precision,\n",
        ")\n",
        "\n",
        "metrics = [\n",
        "    faithfulness,\n",
        "    answer_relevancy,\n",
        "    context_recall,\n",
        "    context_precision,\n",
        "    answer_correctness,\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating:  79%|███████▉  | 63/80 [00:24<00:06,  2.63it/s]No statements were generated from the answer.\n",
            "Evaluating: 100%|██████████| 80/80 [00:43<00:00,  1.83it/s]\n"
          ]
        }
      ],
      "source": [
        "results = evaluate(response_dataset, metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_df2 = results.to_pandas()\n",
        "results_df2.to_csv(\"results.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0     0.983049\n",
              "1     0.177262\n",
              "2     0.909787\n",
              "3     0.525703\n",
              "4     0.813576\n",
              "5     0.178610\n",
              "6     0.672686\n",
              "7     0.997191\n",
              "8     0.180106\n",
              "9     0.173743\n",
              "10    0.175215\n",
              "11    0.176218\n",
              "12    0.521829\n",
              "13    0.195204\n",
              "14    0.866048\n",
              "15    0.180756\n",
              "Name: answer_correctness, dtype: float64"
            ]
          },
          "execution_count": 127,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results_df.answer_correctness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0     0.968340\n",
              "1     0.000000\n",
              "2     0.972429\n",
              "3     0.969035\n",
              "4     0.979054\n",
              "5     0.000000\n",
              "6     1.000000\n",
              "7     0.991522\n",
              "8     0.997174\n",
              "9     0.997114\n",
              "10    0.933063\n",
              "11    0.918173\n",
              "12    1.000000\n",
              "13    0.000000\n",
              "14    0.971745\n",
              "15    0.000000\n",
              "Name: answer_relevancy, dtype: float64"
            ]
          },
          "execution_count": 114,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results_df2.answer_correctness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "df=pd.read_csv(\"results.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>contexts</th>\n",
              "      <th>ground_truth</th>\n",
              "      <th>faithfulness</th>\n",
              "      <th>answer_relevancy</th>\n",
              "      <th>context_recall</th>\n",
              "      <th>context_precision</th>\n",
              "      <th>answer_correctness</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Which organizations were involved as private s...</td>\n",
              "      <td>The organizations involved as private sector a...</td>\n",
              "      <td>['APPENDIX\\n• OSTP conducted meetings with a v...</td>\n",
              "      <td>Adobe, American Civil Liberties Union (ACLU), ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.968340</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.983049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>What is the importance of building a glossary ...</td>\n",
              "      <td>I don't know.</td>\n",
              "      <td>['al. (2024) AI deception: A survey of example...</td>\n",
              "      <td>The importance of building a glossary for synt...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.177262</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>How can organizations enhance content provenan...</td>\n",
              "      <td>Organizations can enhance content provenance t...</td>\n",
              "      <td>[' \\n52 \\n• \\nMonitoring system capabilities a...</td>\n",
              "      <td>Organizations can enhance content provenance t...</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.972429</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.806347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>What is the purpose of the NIST AI Risk Manage...</td>\n",
              "      <td>The purpose of the NIST AI Risk Management Fra...</td>\n",
              "      <td>[' \\n2 \\nThis work was informed by public feed...</td>\n",
              "      <td>The NIST AI Risk Management Framework is being...</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.969035</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.428933</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>How can structured feedback mechanisms be used...</td>\n",
              "      <td>Structured feedback mechanisms can be used to ...</td>\n",
              "      <td>[' \\n29 \\nMS-1.1-006 \\nImplement continuous mo...</td>\n",
              "      <td>Structured feedback mechanisms can be used to ...</td>\n",
              "      <td>0.625</td>\n",
              "      <td>0.979054</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.826538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>How do safety metrics reflect system reliabili...</td>\n",
              "      <td>I don't know.</td>\n",
              "      <td>[' \\n32 \\nMEASURE 2.6: The AI system is evalua...</td>\n",
              "      <td>Safety metrics in the evaluation of AI systems...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.178610</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>What is the focus of the National Science Foun...</td>\n",
              "      <td>The focus of the National Science Foundation's...</td>\n",
              "      <td>[\" \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\...</td>\n",
              "      <td>The focus of the National Science Foundation's...</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.619114</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>What is the role of a purpose-built testing en...</td>\n",
              "      <td>The role of a purpose-built testing environmen...</td>\n",
              "      <td>[' \\n31 \\nMS-2.3-004 \\nUtilize a purpose-built...</td>\n",
              "      <td>A purpose-built testing environment like NIST ...</td>\n",
              "      <td>0.800</td>\n",
              "      <td>0.991522</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.809734</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>What factors to consider when managing content...</td>\n",
              "      <td>The factors to consider when managing content ...</td>\n",
              "      <td>['et al. (2023) Human favoritism, not AI avers...</td>\n",
              "      <td>The answer to given question is not present in...</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.997174</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.180106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>What steps are needed for safe and effective a...</td>\n",
              "      <td>The steps needed for safe and effective automa...</td>\n",
              "      <td>['Systems should undergo extensive testing bef...</td>\n",
              "      <td>The answer to given question is not present in...</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.997114</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.173593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10</td>\n",
              "      <td>How to manage GAI risks with potential misuse ...</td>\n",
              "      <td>To manage GAI risks associated with potential ...</td>\n",
              "      <td>[' \\n48 \\n• Data protection \\n• Data retention...</td>\n",
              "      <td>The answer to given question is not present in...</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.933063</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.175215</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>11</td>\n",
              "      <td>What NSF funding opportunities are available f...</td>\n",
              "      <td>The National Science Foundation (NSF) has a pr...</td>\n",
              "      <td>[\" \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\...</td>\n",
              "      <td>The answer to given question is not present in...</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.918173</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.177309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>12</td>\n",
              "      <td>How can stakeholders ensure data control and u...</td>\n",
              "      <td>Stakeholders can ensure data control and under...</td>\n",
              "      <td>['Clear organizational oversight. Entities res...</td>\n",
              "      <td>Stakeholders can ensure data control and under...</td>\n",
              "      <td>0.750</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.577092</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>13</td>\n",
              "      <td>How does the technical companion to the AI Bil...</td>\n",
              "      <td>I don't know.</td>\n",
              "      <td>['TABLE OF CONTENTS\\nFROM PRINCIPLES TO PRACTI...</td>\n",
              "      <td>The answer to given question is not present in...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.195204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>14</td>\n",
              "      <td>How can organizations improve AI incident awar...</td>\n",
              "      <td>Organizations can improve AI incident awarenes...</td>\n",
              "      <td>[' \\n53 \\nDocumenting, reporting, and sharing ...</td>\n",
              "      <td>Organizations can improve AI incident awarenes...</td>\n",
              "      <td>0.800</td>\n",
              "      <td>0.971745</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.866048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>15</td>\n",
              "      <td>Which California law requires written descript...</td>\n",
              "      <td>I don't know.</td>\n",
              "      <td>['Warehous-\\ning employers in California that ...</td>\n",
              "      <td>A California law requires that warehouse emplo...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.180756</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Unnamed: 0                                           question  \\\n",
              "0            0  Which organizations were involved as private s...   \n",
              "1            1  What is the importance of building a glossary ...   \n",
              "2            2  How can organizations enhance content provenan...   \n",
              "3            3  What is the purpose of the NIST AI Risk Manage...   \n",
              "4            4  How can structured feedback mechanisms be used...   \n",
              "5            5  How do safety metrics reflect system reliabili...   \n",
              "6            6  What is the focus of the National Science Foun...   \n",
              "7            7  What is the role of a purpose-built testing en...   \n",
              "8            8  What factors to consider when managing content...   \n",
              "9            9  What steps are needed for safe and effective a...   \n",
              "10          10  How to manage GAI risks with potential misuse ...   \n",
              "11          11  What NSF funding opportunities are available f...   \n",
              "12          12  How can stakeholders ensure data control and u...   \n",
              "13          13  How does the technical companion to the AI Bil...   \n",
              "14          14  How can organizations improve AI incident awar...   \n",
              "15          15  Which California law requires written descript...   \n",
              "\n",
              "                                               answer  \\\n",
              "0   The organizations involved as private sector a...   \n",
              "1                                       I don't know.   \n",
              "2   Organizations can enhance content provenance t...   \n",
              "3   The purpose of the NIST AI Risk Management Fra...   \n",
              "4   Structured feedback mechanisms can be used to ...   \n",
              "5                                       I don't know.   \n",
              "6   The focus of the National Science Foundation's...   \n",
              "7   The role of a purpose-built testing environmen...   \n",
              "8   The factors to consider when managing content ...   \n",
              "9   The steps needed for safe and effective automa...   \n",
              "10  To manage GAI risks associated with potential ...   \n",
              "11  The National Science Foundation (NSF) has a pr...   \n",
              "12  Stakeholders can ensure data control and under...   \n",
              "13                                      I don't know.   \n",
              "14  Organizations can improve AI incident awarenes...   \n",
              "15                                      I don't know.   \n",
              "\n",
              "                                             contexts  \\\n",
              "0   ['APPENDIX\\n• OSTP conducted meetings with a v...   \n",
              "1   ['al. (2024) AI deception: A survey of example...   \n",
              "2   [' \\n52 \\n• \\nMonitoring system capabilities a...   \n",
              "3   [' \\n2 \\nThis work was informed by public feed...   \n",
              "4   [' \\n29 \\nMS-1.1-006 \\nImplement continuous mo...   \n",
              "5   [' \\n32 \\nMEASURE 2.6: The AI system is evalua...   \n",
              "6   [\" \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\...   \n",
              "7   [' \\n31 \\nMS-2.3-004 \\nUtilize a purpose-built...   \n",
              "8   ['et al. (2023) Human favoritism, not AI avers...   \n",
              "9   ['Systems should undergo extensive testing bef...   \n",
              "10  [' \\n48 \\n• Data protection \\n• Data retention...   \n",
              "11  [\" \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\...   \n",
              "12  ['Clear organizational oversight. Entities res...   \n",
              "13  ['TABLE OF CONTENTS\\nFROM PRINCIPLES TO PRACTI...   \n",
              "14  [' \\n53 \\nDocumenting, reporting, and sharing ...   \n",
              "15  ['Warehous-\\ning employers in California that ...   \n",
              "\n",
              "                                         ground_truth  faithfulness  \\\n",
              "0   Adobe, American Civil Liberties Union (ACLU), ...           NaN   \n",
              "1   The importance of building a glossary for synt...         0.000   \n",
              "2   Organizations can enhance content provenance t...         1.000   \n",
              "3   The NIST AI Risk Management Framework is being...         1.000   \n",
              "4   Structured feedback mechanisms can be used to ...         0.625   \n",
              "5   Safety metrics in the evaluation of AI systems...         0.000   \n",
              "6   The focus of the National Science Foundation's...         1.000   \n",
              "7   A purpose-built testing environment like NIST ...         0.800   \n",
              "8   The answer to given question is not present in...         1.000   \n",
              "9   The answer to given question is not present in...         1.000   \n",
              "10  The answer to given question is not present in...         1.000   \n",
              "11  The answer to given question is not present in...         1.000   \n",
              "12  Stakeholders can ensure data control and under...         0.750   \n",
              "13  The answer to given question is not present in...         0.000   \n",
              "14  Organizations can improve AI incident awarenes...         0.800   \n",
              "15  A California law requires that warehouse emplo...         0.000   \n",
              "\n",
              "    answer_relevancy  context_recall  context_precision  answer_correctness  \n",
              "0           0.968340             1.0           1.000000            0.983049  \n",
              "1           0.000000             1.0           0.833333            0.177262  \n",
              "2           0.972429             1.0           1.000000            0.806347  \n",
              "3           0.969035             0.5           0.750000            0.428933  \n",
              "4           0.979054             1.0           1.000000            0.826538  \n",
              "5           0.000000             1.0           1.000000            0.178610  \n",
              "6           1.000000             1.0           1.000000            0.619114  \n",
              "7           0.991522             1.0           1.000000            0.809734  \n",
              "8           0.997174             1.0           0.333333            0.180106  \n",
              "9           0.997114             1.0           0.000000            0.173593  \n",
              "10          0.933063             1.0           0.000000            0.175215  \n",
              "11          0.918173             1.0           1.000000            0.177309  \n",
              "12          1.000000             0.0           1.000000            0.577092  \n",
              "13          0.000000             1.0           0.000000            0.195204  \n",
              "14          0.971745             1.0           1.000000            0.866048  \n",
              "15          0.000000             1.0           1.000000            0.180756  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "07ab3dc0790241bbb85a7f488a42ef8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7710c7377cbc4c30b55b28b4bc99e88f",
              "IPY_MODEL_41bdd49fab5f4826959d0d50663ff539",
              "IPY_MODEL_60168d85131d4afc99d55d61ab954ee6"
            ],
            "layout": "IPY_MODEL_9edf898aeeab40dda9b9475395776521"
          }
        },
        "095f680d37a3430fb82d223615662db5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0b44cb0f8e34446c8dde668a75d3d8ad": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10df31709059484c99f102453d780473": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1160a44dc18e47b0890f70c40eaa7eb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "11c7f66acc1d45be9517d0addf49331e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "122b1bd1f0e9417a8dcb57d4eebe4d2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e0c233ad01604540a6c873f4a731982d",
              "IPY_MODEL_e9a01115c75b499884f7e0ef32e9e599",
              "IPY_MODEL_5faba4ad609448b2b49024add4ad3b8e"
            ],
            "layout": "IPY_MODEL_ef25efa751304e4699910f1fbc14345f"
          }
        },
        "158212a630f04cbd884c937f2f60f5c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1d54ccd56494c4d831f71b416a1f880",
            "placeholder": "​",
            "style": "IPY_MODEL_530f696feefe499da08c6312047379b2",
            "value": " 20/? [01:43&lt;00:00,  5.25s/it]"
          }
        },
        "23863bc37a8645029934b8c106622c51": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2508d229935744cbb5fc340222e2d660": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a0755d4476543feb4a64538e3e37213": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c20b539cd70b4ba99601ad1d69fd9cec",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a6d681eeafa44d18b933a4c5dec88382",
            "value": 1
          }
        },
        "33f063017b7c4c7fa8cbafc89674350b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6864c81e2bcf459bbaf5acbb36bdfcbe",
              "IPY_MODEL_59d6e269eadf429a924f6f79bc8ba4ba",
              "IPY_MODEL_ca791fc471e34b9da2f9070fc1053c0f"
            ],
            "layout": "IPY_MODEL_8baf0ed3d0f743f294e07f2b5407e820"
          }
        },
        "3a8537e37fc14fd9b16ca0ceee4fede6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41bdd49fab5f4826959d0d50663ff539": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6eb8b2e3262c45248708a2082c366f0a",
            "max": 64,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_095f680d37a3430fb82d223615662db5",
            "value": 64
          }
        },
        "530f696feefe499da08c6312047379b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "59d6e269eadf429a924f6f79bc8ba4ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_890e0dd7fa524ceca1e805cb6253ee71",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_61b52ff459214129b8f7e6d67b192b78",
            "value": 20
          }
        },
        "5ab5f08afa5841709aedb2f78a52a11c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5c2fda99d4204d85b1bf7ad354fd58d4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5faba4ad609448b2b49024add4ad3b8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_849b5c95008541d49f1ceedf0a59ac60",
            "placeholder": "​",
            "style": "IPY_MODEL_f3665a86662746c4ac7cb0796604781d",
            "value": " 20/? [01:27&lt;00:00,  6.45s/it]"
          }
        },
        "60168d85131d4afc99d55d61ab954ee6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a8537e37fc14fd9b16ca0ceee4fede6",
            "placeholder": "​",
            "style": "IPY_MODEL_1160a44dc18e47b0890f70c40eaa7eb0",
            "value": " 61/64 [00:02&lt;00:00, 23.36it/s]"
          }
        },
        "61b52ff459214129b8f7e6d67b192b78": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6864c81e2bcf459bbaf5acbb36bdfcbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10df31709059484c99f102453d780473",
            "placeholder": "​",
            "style": "IPY_MODEL_2508d229935744cbb5fc340222e2d660",
            "value": "Generating: 100%"
          }
        },
        "6eb8b2e3262c45248708a2082c366f0a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7710c7377cbc4c30b55b28b4bc99e88f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c2fda99d4204d85b1bf7ad354fd58d4",
            "placeholder": "​",
            "style": "IPY_MODEL_93cd4d35c5fd41f5904ca1d52d1f52a8",
            "value": "embedding nodes:  95%"
          }
        },
        "7cb241365f604419af454c1c28de197a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "7dce19ac55264f2b88a0e4730e55867b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ddffd834e09940a4bd3874c3f39b4e21",
            "placeholder": "​",
            "style": "IPY_MODEL_ef63c3b2d51e452da03cdae5d9b034be",
            "value": ""
          }
        },
        "849b5c95008541d49f1ceedf0a59ac60": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "890e0dd7fa524ceca1e805cb6253ee71": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8baf0ed3d0f743f294e07f2b5407e820": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93cd4d35c5fd41f5904ca1d52d1f52a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9cf586576ff44dba86ba2eb389593c61": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9edf898aeeab40dda9b9475395776521": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "a6d681eeafa44d18b933a4c5dec88382": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bf8dcc0895054529af356da401c513f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7dce19ac55264f2b88a0e4730e55867b",
              "IPY_MODEL_2a0755d4476543feb4a64538e3e37213",
              "IPY_MODEL_158212a630f04cbd884c937f2f60f5c8"
            ],
            "layout": "IPY_MODEL_11c7f66acc1d45be9517d0addf49331e"
          }
        },
        "c20b539cd70b4ba99601ad1d69fd9cec": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "ca791fc471e34b9da2f9070fc1053c0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23863bc37a8645029934b8c106622c51",
            "placeholder": "​",
            "style": "IPY_MODEL_5ab5f08afa5841709aedb2f78a52a11c",
            "value": " 20/20 [00:52&lt;00:00,  4.50s/it]"
          }
        },
        "d1d54ccd56494c4d831f71b416a1f880": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ddffd834e09940a4bd3874c3f39b4e21": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0c233ad01604540a6c873f4a731982d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b44cb0f8e34446c8dde668a75d3d8ad",
            "placeholder": "​",
            "style": "IPY_MODEL_edaac6587b2d4bd5be52b89bb097f99f",
            "value": ""
          }
        },
        "e9a01115c75b499884f7e0ef32e9e599": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7cb241365f604419af454c1c28de197a",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9cf586576ff44dba86ba2eb389593c61",
            "value": 1
          }
        },
        "edaac6587b2d4bd5be52b89bb097f99f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef25efa751304e4699910f1fbc14345f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef63c3b2d51e452da03cdae5d9b034be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f3665a86662746c4ac7cb0796604781d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
